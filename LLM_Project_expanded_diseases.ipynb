{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti022/Healthcare-Chatbot/blob/main/LLM_Project_expanded_diseases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVc9VWJI9ZI"
      },
      "source": [
        "Qdrant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QncCxftPETth"
      },
      "source": [
        "Load Data and Upload to Qdrant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFc_1ye4kHbN"
      },
      "source": [
        "Data in Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdd96FO0kcDd",
        "outputId": "32a54abb-6311-453b-edc6-6adcfc74c92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk('/content/drive'):\n",
        "    for name in files:\n",
        "        if \"Updated_Data\" in name or \"data\" in root.lower():\n",
        "            print(\"üìç FOUND:\", os.path.join(root, name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRW4RPt7fn4w",
        "outputId": "65f2d13c-95a9-4d11-d1be-a0aa68b4f44c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_cardiovascular_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_asthma_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_diabetes_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_cancer_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_alzheimer_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/Updated_Data.gdoc\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_cancer_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_asthma_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_cardiovascular_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_alzheimer_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_master_full.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/diabetes_rag_query_results.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/diabetes_chunk_embeddings.npy\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/diabetes_chunk_map.pkl\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/Disease and symptoms dataset.csv\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_chunk_map.json\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/streamlit_app.py\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/orchestrator.py\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/agents.py\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/utils.py\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_embeddings.npy\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_faiss.index\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_all_full_embeddings.npy\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_all_full_chunk_map.json\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_all_full_faiss.index\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/__pycache__/utils.cpython-312.pyc\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/__pycache__/agents.cpython-312.pyc\n",
            "üìç FOUND: /content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/__pycache__/orchestrator.cpython-312.pyc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile build_clinical_trials_15_diseases.py\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# === CONFIG ===\n",
        "BASE_URL = \"https://clinicaltrials.gov/api/v2/studies\"\n",
        "\n",
        "# ‚úÖ Your 15 conditions: {slug: query_string_for_API}\n",
        "CONDITION_CONFIG = {\n",
        "    \"diabetes_type2\": \"Type 2 Diabetes\",\n",
        "    \"diabetes_type1\": \"Type 1 Diabetes\",\n",
        "    \"obesity\": \"Obesity\",\n",
        "    \"hypertension\": \"Hypertension\",\n",
        "    \"cardiovascular\": \"Cardiovascular Diseases\",\n",
        "    \"ckd\": \"Chronic Kidney Disease\",\n",
        "    \"alzheimer\": \"Alzheimer Disease\",\n",
        "    \"parkinson\": \"Parkinson Disease\",\n",
        "    \"asthma\": \"Asthma\",\n",
        "    \"copd\": \"Chronic Obstructive Pulmonary Disease\",\n",
        "    \"breast_cancer\": \"Breast Neoplasms\",\n",
        "    \"lung_cancer\": \"Lung Neoplasms\",\n",
        "    \"prostate_cancer\": \"Prostatic Neoplasms\",\n",
        "    \"stroke\": \"Stroke\",\n",
        "    \"rheumatoid_arthritis\": \"Rheumatoid Arthritis\",\n",
        "}\n",
        "\n",
        "# üîó Folder where your existing CSVs live (shortcut path you found)\n",
        "OUTPUT_DIR = \"/content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def fetch_all_studies_for_condition(condition: str, page_size: int = 100, max_pages: int | None = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fetch ALL studies for a given condition using ClinicalTrials.gov v2 API.\n",
        "    Paginated via nextPageToken.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüöÄ Fetching studies for condition: {condition}\")\n",
        "    params = {\n",
        "        \"query.cond\": condition,\n",
        "        \"pageSize\": page_size,\n",
        "    }\n",
        "\n",
        "    studies_total: List[Dict[str, Any]] = []\n",
        "    page_token = None\n",
        "    page = 0\n",
        "\n",
        "    while True:\n",
        "        if page_token:\n",
        "            params[\"pageToken\"] = page_token\n",
        "        else:\n",
        "            params.pop(\"pageToken\", None)\n",
        "\n",
        "        page += 1\n",
        "        print(f\"  ‚Ä¢ Page {page} ...\", end=\" \")\n",
        "\n",
        "        resp = requests.get(BASE_URL, params=params)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"\\n‚ùå API error: {resp.status_code} ‚Äì {resp.text[:200]}\")\n",
        "            break\n",
        "\n",
        "        data = resp.json()\n",
        "        studies = data.get(\"studies\", [])\n",
        "        print(f\"{len(studies)} studies\")\n",
        "\n",
        "        if not studies:\n",
        "            break\n",
        "\n",
        "        studies_total.extend(studies)\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "\n",
        "        if max_pages is not None and page >= max_pages:\n",
        "            print(\"  ‚èπÔ∏è Reached max_pages limit.\")\n",
        "            break\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "        time.sleep(0.4)  # polite pause for API\n",
        "\n",
        "    print(f\"‚úÖ Total fetched for {condition}: {len(studies_total)}\")\n",
        "    return studies_total\n",
        "\n",
        "\n",
        "def extract_record_from_study(study: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Map raw API JSON into flat, RAG-friendly schema.\n",
        "    \"\"\"\n",
        "    protocol = study.get(\"protocolSection\", {}) or {}\n",
        "    id_module = protocol.get(\"identificationModule\", {}) or {}\n",
        "    status_module = protocol.get(\"statusModule\", {}) or {}\n",
        "    conds_module = protocol.get(\"conditionsModule\", {}) or {}\n",
        "    desc_module = protocol.get(\"descriptionModule\", {}) or {}\n",
        "    eligibility_module = protocol.get(\"eligibilityModule\", {}) or {}\n",
        "    sponsor_module = protocol.get(\"sponsorCollaboratorsModule\", {}) or {}\n",
        "    arms_module = protocol.get(\"armsInterventionsModule\", {}) or {}\n",
        "    outcomes_module = protocol.get(\"outcomesModule\", {}) or {}\n",
        "\n",
        "    nct_id = id_module.get(\"nctId\", \"\")\n",
        "    brief_title = id_module.get(\"briefTitle\", \"\")\n",
        "    status = status_module.get(\"overallStatus\", \"\")\n",
        "    conditions = \", \".join(conds_module.get(\"conditions\", []) or [])\n",
        "    brief_summary = desc_module.get(\"briefSummary\", \"\")\n",
        "    eligibility = eligibility_module.get(\"eligibilityCriteria\", \"\")\n",
        "    sponsor = (sponsor_module.get(\"leadSponsor\") or {}).get(\"name\", \"\")\n",
        "    interventions = \", \".join([i.get(\"name\", \"\") for i in (arms_module.get(\"interventions\") or [])])\n",
        "    primary_outcomes = \", \".join([o.get(\"measure\", \"\") for o in (outcomes_module.get(\"primaryOutcomes\") or [])])\n",
        "\n",
        "    return {\n",
        "        \"nct_id\": nct_id,\n",
        "        \"brief_title\": brief_title,\n",
        "        \"status\": status,\n",
        "        \"conditions\": conditions,\n",
        "        \"brief_summary\": brief_summary,\n",
        "        \"eligibility_criteria\": eligibility,\n",
        "        \"sponsor\": sponsor,\n",
        "        \"interventions\": interventions,\n",
        "        \"primary_outcomes\": primary_outcomes,\n",
        "    }\n",
        "\n",
        "\n",
        "def build_clean_df_for_condition(label: str, condition_query: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download ‚Üí normalize ‚Üí filter ‚Üí return DataFrame for one condition.\n",
        "    \"\"\"\n",
        "    raw_studies = fetch_all_studies_for_condition(condition_query)\n",
        "    if not raw_studies:\n",
        "        print(f\"‚ö†Ô∏è No studies for {label} / {condition_query}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    records = [extract_record_from_study(st) for st in raw_studies]\n",
        "    df = pd.DataFrame(records)\n",
        "    print(f\"   ‚Üí Raw rows for {label}: {len(df)}\")\n",
        "\n",
        "    # Normalize + clean\n",
        "    df[\"status\"] = df[\"status\"].astype(str).str.strip().str.title()\n",
        "\n",
        "    # Drop missing NCT IDs\n",
        "    df = df[df[\"nct_id\"].astype(str).str.len() > 0].copy()\n",
        "\n",
        "    # Remove obvious junk statuses\n",
        "    bad_status = {\n",
        "        \"Terminated\",\n",
        "        \"Withdrawn\",\n",
        "        \"Suspended\",\n",
        "        \"No Longer Available\",\n",
        "        \"Unknown Status\",\n",
        "        \"Withheld\",\n",
        "    }\n",
        "    before_status = len(df)\n",
        "    df = df[~df[\"status\"].isin(bad_status)].copy()\n",
        "    print(f\"   ‚Ä¢ Removed {before_status - len(df)} bad-status trials\")\n",
        "\n",
        "    # Require at least some summary text\n",
        "    df[\"brief_summary\"] = df[\"brief_summary\"].fillna(\"\").astype(str)\n",
        "    before_summary = len(df)\n",
        "    df = df[df[\"brief_summary\"].str.len() > 40].copy()\n",
        "    print(f\"   ‚Ä¢ Removed {before_summary - len(df)} with very short/no summary\")\n",
        "\n",
        "    # Deduplicate within this disease by NCT ID\n",
        "    before_dups = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"nct_id\"], keep=\"first\")\n",
        "    print(f\"   ‚Ä¢ Removed {before_dups - len(df)} duplicates (same NCT)\")\n",
        "\n",
        "    print(f\"‚úÖ Final clean rows for {label}: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def main():\n",
        "    all_dfs = []\n",
        "    print(\"======================================\")\n",
        "    print(\"üß± Building 15-disease clinical corpus\")\n",
        "    print(\"======================================\")\n",
        "\n",
        "    for slug, cond_query in CONDITION_CONFIG.items():\n",
        "        print(\"\\n\" + \"=\" * 40)\n",
        "        print(f\"üìö Condition: {slug}  (query='{cond_query}')\")\n",
        "        df = build_clean_df_for_condition(slug, cond_query)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"‚ö†Ô∏è Skipping save for {slug} (no clean rows)\")\n",
        "            continue\n",
        "\n",
        "        out_path = os.path.join(OUTPUT_DIR, f\"clinical_trials_{slug}_full.csv\")\n",
        "        df.to_csv(out_path, index=False)\n",
        "        print(f\"üíæ Saved {len(df)} rows ‚Üí {out_path}\")\n",
        "        all_dfs.append(df)\n",
        "\n",
        "    if not all_dfs:\n",
        "        print(\"\\n‚ùå No condition produced data. Aborting master file step.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n======================================\")\n",
        "    print(\"üì¶ Creating master combined CSV\")\n",
        "    print(\"======================================\")\n",
        "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    before_master_dups = len(master_df)\n",
        "    master_df = master_df.drop_duplicates(subset=[\"nct_id\"], keep=\"first\")\n",
        "    print(f\"   ‚Ä¢ Removed {before_master_dups - len(master_df)} cross-disease duplicates\")\n",
        "\n",
        "    master_path = os.path.join(OUTPUT_DIR, \"clinical_trials_master_full.csv\")\n",
        "    master_df.to_csv(master_path, index=False)\n",
        "    print(f\"‚úÖ Saved master file with {len(master_df)} unique trials ‚Üí {master_path}\")\n",
        "    print(\"\\nüéâ Done. You now have a clean 15-disease corpus!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0YiQ4Hzk4sl",
        "outputId": "91210f05-1511-43bb-b0bb-a30324eec59d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing build_clinical_trials_15_diseases.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_clinical_trials_15_diseases.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9iciLGrlBYM",
        "outputId": "ab84cdc5-ed73-4edb-9826-6c8b56602bfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================\n",
            "üß± Building 15-disease clinical corpus\n",
            "======================================\n",
            "\n",
            "========================================\n",
            "üìö Condition: diabetes_type2  (query='Type 2 Diabetes')\n",
            "\n",
            "üöÄ Fetching studies for condition: Type 2 Diabetes\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 100 studies\n",
            "  ‚Ä¢ Page 104 ... 100 studies\n",
            "  ‚Ä¢ Page 105 ... 100 studies\n",
            "  ‚Ä¢ Page 106 ... 100 studies\n",
            "  ‚Ä¢ Page 107 ... 100 studies\n",
            "  ‚Ä¢ Page 108 ... 100 studies\n",
            "  ‚Ä¢ Page 109 ... 100 studies\n",
            "  ‚Ä¢ Page 110 ... 100 studies\n",
            "  ‚Ä¢ Page 111 ... 100 studies\n",
            "  ‚Ä¢ Page 112 ... 100 studies\n",
            "  ‚Ä¢ Page 113 ... 25 studies\n",
            "‚úÖ Total fetched for Type 2 Diabetes: 11225\n",
            "   ‚Üí Raw rows for diabetes_type2: 11225\n",
            "   ‚Ä¢ Removed 818 bad-status trials\n",
            "   ‚Ä¢ Removed 3 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for diabetes_type2: 10404\n",
            "üíæ Saved 10404 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_diabetes_type2_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: diabetes_type1  (query='Type 1 Diabetes')\n",
            "\n",
            "üöÄ Fetching studies for condition: Type 1 Diabetes\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 93 studies\n",
            "‚úÖ Total fetched for Type 1 Diabetes: 4393\n",
            "   ‚Üí Raw rows for diabetes_type1: 4393\n",
            "   ‚Ä¢ Removed 398 bad-status trials\n",
            "   ‚Ä¢ Removed 2 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for diabetes_type1: 3993\n",
            "üíæ Saved 3993 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_diabetes_type1_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: obesity  (query='Obesity')\n",
            "\n",
            "üöÄ Fetching studies for condition: Obesity\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 100 studies\n",
            "  ‚Ä¢ Page 104 ... 100 studies\n",
            "  ‚Ä¢ Page 105 ... 100 studies\n",
            "  ‚Ä¢ Page 106 ... 100 studies\n",
            "  ‚Ä¢ Page 107 ... 100 studies\n",
            "  ‚Ä¢ Page 108 ... 100 studies\n",
            "  ‚Ä¢ Page 109 ... 100 studies\n",
            "  ‚Ä¢ Page 110 ... 100 studies\n",
            "  ‚Ä¢ Page 111 ... 100 studies\n",
            "  ‚Ä¢ Page 112 ... 100 studies\n",
            "  ‚Ä¢ Page 113 ... 100 studies\n",
            "  ‚Ä¢ Page 114 ... 100 studies\n",
            "  ‚Ä¢ Page 115 ... 100 studies\n",
            "  ‚Ä¢ Page 116 ... 100 studies\n",
            "  ‚Ä¢ Page 117 ... 100 studies\n",
            "  ‚Ä¢ Page 118 ... 100 studies\n",
            "  ‚Ä¢ Page 119 ... 100 studies\n",
            "  ‚Ä¢ Page 120 ... 100 studies\n",
            "  ‚Ä¢ Page 121 ... 100 studies\n",
            "  ‚Ä¢ Page 122 ... 100 studies\n",
            "  ‚Ä¢ Page 123 ... 100 studies\n",
            "  ‚Ä¢ Page 124 ... 100 studies\n",
            "  ‚Ä¢ Page 125 ... 100 studies\n",
            "  ‚Ä¢ Page 126 ... 100 studies\n",
            "  ‚Ä¢ Page 127 ... 100 studies\n",
            "  ‚Ä¢ Page 128 ... 100 studies\n",
            "  ‚Ä¢ Page 129 ... 100 studies\n",
            "  ‚Ä¢ Page 130 ... 100 studies\n",
            "  ‚Ä¢ Page 131 ... 100 studies\n",
            "  ‚Ä¢ Page 132 ... 100 studies\n",
            "  ‚Ä¢ Page 133 ... 100 studies\n",
            "  ‚Ä¢ Page 134 ... 100 studies\n",
            "  ‚Ä¢ Page 135 ... 100 studies\n",
            "  ‚Ä¢ Page 136 ... 100 studies\n",
            "  ‚Ä¢ Page 137 ... 100 studies\n",
            "  ‚Ä¢ Page 138 ... 99 studies\n",
            "‚úÖ Total fetched for Obesity: 13799\n",
            "   ‚Üí Raw rows for obesity: 13799\n",
            "   ‚Ä¢ Removed 873 bad-status trials\n",
            "   ‚Ä¢ Removed 6 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for obesity: 12920\n",
            "üíæ Saved 12920 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_obesity_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: hypertension  (query='Hypertension')\n",
            "\n",
            "üöÄ Fetching studies for condition: Hypertension\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 100 studies\n",
            "  ‚Ä¢ Page 104 ... 100 studies\n",
            "  ‚Ä¢ Page 105 ... 100 studies\n",
            "  ‚Ä¢ Page 106 ... 100 studies\n",
            "  ‚Ä¢ Page 107 ... 100 studies\n",
            "  ‚Ä¢ Page 108 ... 100 studies\n",
            "  ‚Ä¢ Page 109 ... 100 studies\n",
            "  ‚Ä¢ Page 110 ... 100 studies\n",
            "  ‚Ä¢ Page 111 ... 100 studies\n",
            "  ‚Ä¢ Page 112 ... 100 studies\n",
            "  ‚Ä¢ Page 113 ... 100 studies\n",
            "  ‚Ä¢ Page 114 ... 100 studies\n",
            "  ‚Ä¢ Page 115 ... 100 studies\n",
            "  ‚Ä¢ Page 116 ... 100 studies\n",
            "  ‚Ä¢ Page 117 ... 96 studies\n",
            "‚úÖ Total fetched for Hypertension: 11696\n",
            "   ‚Üí Raw rows for hypertension: 11696\n",
            "   ‚Ä¢ Removed 1071 bad-status trials\n",
            "   ‚Ä¢ Removed 6 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for hypertension: 10619\n",
            "üíæ Saved 10619 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_hypertension_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: cardiovascular  (query='Cardiovascular Diseases')\n",
            "\n",
            "üöÄ Fetching studies for condition: Cardiovascular Diseases\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 100 studies\n",
            "  ‚Ä¢ Page 104 ... 100 studies\n",
            "  ‚Ä¢ Page 105 ... 100 studies\n",
            "  ‚Ä¢ Page 106 ... 100 studies\n",
            "  ‚Ä¢ Page 107 ... 100 studies\n",
            "  ‚Ä¢ Page 108 ... 100 studies\n",
            "  ‚Ä¢ Page 109 ... 100 studies\n",
            "  ‚Ä¢ Page 110 ... 100 studies\n",
            "  ‚Ä¢ Page 111 ... 100 studies\n",
            "  ‚Ä¢ Page 112 ... 100 studies\n",
            "  ‚Ä¢ Page 113 ... 100 studies\n",
            "  ‚Ä¢ Page 114 ... 100 studies\n",
            "  ‚Ä¢ Page 115 ... 100 studies\n",
            "  ‚Ä¢ Page 116 ... 100 studies\n",
            "  ‚Ä¢ Page 117 ... 100 studies\n",
            "  ‚Ä¢ Page 118 ... 100 studies\n",
            "  ‚Ä¢ Page 119 ... 100 studies\n",
            "  ‚Ä¢ Page 120 ... 100 studies\n",
            "  ‚Ä¢ Page 121 ... 100 studies\n",
            "  ‚Ä¢ Page 122 ... 100 studies\n",
            "  ‚Ä¢ Page 123 ... 100 studies\n",
            "  ‚Ä¢ Page 124 ... 100 studies\n",
            "  ‚Ä¢ Page 125 ... 100 studies\n",
            "  ‚Ä¢ Page 126 ... 100 studies\n",
            "  ‚Ä¢ Page 127 ... 100 studies\n",
            "  ‚Ä¢ Page 128 ... 100 studies\n",
            "  ‚Ä¢ Page 129 ... 100 studies\n",
            "  ‚Ä¢ Page 130 ... 100 studies\n",
            "  ‚Ä¢ Page 131 ... 100 studies\n",
            "  ‚Ä¢ Page 132 ... 100 studies\n",
            "  ‚Ä¢ Page 133 ... 100 studies\n",
            "  ‚Ä¢ Page 134 ... 100 studies\n",
            "  ‚Ä¢ Page 135 ... 100 studies\n",
            "  ‚Ä¢ Page 136 ... 100 studies\n",
            "  ‚Ä¢ Page 137 ... 100 studies\n",
            "  ‚Ä¢ Page 138 ... 100 studies\n",
            "  ‚Ä¢ Page 139 ... 100 studies\n",
            "  ‚Ä¢ Page 140 ... 100 studies\n",
            "  ‚Ä¢ Page 141 ... 100 studies\n",
            "  ‚Ä¢ Page 142 ... 100 studies\n",
            "  ‚Ä¢ Page 143 ... 100 studies\n",
            "  ‚Ä¢ Page 144 ... 100 studies\n",
            "  ‚Ä¢ Page 145 ... 100 studies\n",
            "  ‚Ä¢ Page 146 ... 100 studies\n",
            "  ‚Ä¢ Page 147 ... 100 studies\n",
            "  ‚Ä¢ Page 148 ... 100 studies\n",
            "  ‚Ä¢ Page 149 ... 100 studies\n",
            "  ‚Ä¢ Page 150 ... 100 studies\n",
            "  ‚Ä¢ Page 151 ... 100 studies\n",
            "  ‚Ä¢ Page 152 ... 100 studies\n",
            "  ‚Ä¢ Page 153 ... 100 studies\n",
            "  ‚Ä¢ Page 154 ... 100 studies\n",
            "  ‚Ä¢ Page 155 ... 100 studies\n",
            "  ‚Ä¢ Page 156 ... 100 studies\n",
            "  ‚Ä¢ Page 157 ... 100 studies\n",
            "  ‚Ä¢ Page 158 ... 100 studies\n",
            "  ‚Ä¢ Page 159 ... 100 studies\n",
            "  ‚Ä¢ Page 160 ... 100 studies\n",
            "  ‚Ä¢ Page 161 ... 100 studies\n",
            "  ‚Ä¢ Page 162 ... 100 studies\n",
            "  ‚Ä¢ Page 163 ... 100 studies\n",
            "  ‚Ä¢ Page 164 ... 100 studies\n",
            "  ‚Ä¢ Page 165 ... 100 studies\n",
            "  ‚Ä¢ Page 166 ... 100 studies\n",
            "  ‚Ä¢ Page 167 ... 100 studies\n",
            "  ‚Ä¢ Page 168 ... 100 studies\n",
            "  ‚Ä¢ Page 169 ... 100 studies\n",
            "  ‚Ä¢ Page 170 ... 100 studies\n",
            "  ‚Ä¢ Page 171 ... 100 studies\n",
            "  ‚Ä¢ Page 172 ... 100 studies\n",
            "  ‚Ä¢ Page 173 ... 100 studies\n",
            "  ‚Ä¢ Page 174 ... 100 studies\n",
            "  ‚Ä¢ Page 175 ... 100 studies\n",
            "  ‚Ä¢ Page 176 ... 100 studies\n",
            "  ‚Ä¢ Page 177 ... 100 studies\n",
            "  ‚Ä¢ Page 178 ... 100 studies\n",
            "  ‚Ä¢ Page 179 ... 100 studies\n",
            "  ‚Ä¢ Page 180 ... 100 studies\n",
            "  ‚Ä¢ Page 181 ... 100 studies\n",
            "  ‚Ä¢ Page 182 ... 100 studies\n",
            "  ‚Ä¢ Page 183 ... 100 studies\n",
            "  ‚Ä¢ Page 184 ... 100 studies\n",
            "  ‚Ä¢ Page 185 ... 100 studies\n",
            "  ‚Ä¢ Page 186 ... 100 studies\n",
            "  ‚Ä¢ Page 187 ... 100 studies\n",
            "  ‚Ä¢ Page 188 ... 100 studies\n",
            "  ‚Ä¢ Page 189 ... 100 studies\n",
            "  ‚Ä¢ Page 190 ... 100 studies\n",
            "  ‚Ä¢ Page 191 ... 100 studies\n",
            "  ‚Ä¢ Page 192 ... 100 studies\n",
            "  ‚Ä¢ Page 193 ... 100 studies\n",
            "  ‚Ä¢ Page 194 ... 100 studies\n",
            "  ‚Ä¢ Page 195 ... 100 studies\n",
            "  ‚Ä¢ Page 196 ... 100 studies\n",
            "  ‚Ä¢ Page 197 ... 100 studies\n",
            "  ‚Ä¢ Page 198 ... 100 studies\n",
            "  ‚Ä¢ Page 199 ... 100 studies\n",
            "  ‚Ä¢ Page 200 ... 100 studies\n",
            "  ‚Ä¢ Page 201 ... 100 studies\n",
            "  ‚Ä¢ Page 202 ... 100 studies\n",
            "  ‚Ä¢ Page 203 ... 100 studies\n",
            "  ‚Ä¢ Page 204 ... 100 studies\n",
            "  ‚Ä¢ Page 205 ... 100 studies\n",
            "  ‚Ä¢ Page 206 ... 100 studies\n",
            "  ‚Ä¢ Page 207 ... 100 studies\n",
            "  ‚Ä¢ Page 208 ... 100 studies\n",
            "  ‚Ä¢ Page 209 ... 100 studies\n",
            "  ‚Ä¢ Page 210 ... 100 studies\n",
            "  ‚Ä¢ Page 211 ... 100 studies\n",
            "  ‚Ä¢ Page 212 ... 100 studies\n",
            "  ‚Ä¢ Page 213 ... 100 studies\n",
            "  ‚Ä¢ Page 214 ... 100 studies\n",
            "  ‚Ä¢ Page 215 ... 100 studies\n",
            "  ‚Ä¢ Page 216 ... 100 studies\n",
            "  ‚Ä¢ Page 217 ... 100 studies\n",
            "  ‚Ä¢ Page 218 ... 100 studies\n",
            "  ‚Ä¢ Page 219 ... 100 studies\n",
            "  ‚Ä¢ Page 220 ... 100 studies\n",
            "  ‚Ä¢ Page 221 ... 100 studies\n",
            "  ‚Ä¢ Page 222 ... 100 studies\n",
            "  ‚Ä¢ Page 223 ... 100 studies\n",
            "  ‚Ä¢ Page 224 ... 100 studies\n",
            "  ‚Ä¢ Page 225 ... 100 studies\n",
            "  ‚Ä¢ Page 226 ... 100 studies\n",
            "  ‚Ä¢ Page 227 ... 100 studies\n",
            "  ‚Ä¢ Page 228 ... 100 studies\n",
            "  ‚Ä¢ Page 229 ... 100 studies\n",
            "  ‚Ä¢ Page 230 ... 100 studies\n",
            "  ‚Ä¢ Page 231 ... 100 studies\n",
            "  ‚Ä¢ Page 232 ... 100 studies\n",
            "  ‚Ä¢ Page 233 ... 100 studies\n",
            "  ‚Ä¢ Page 234 ... 100 studies\n",
            "  ‚Ä¢ Page 235 ... 100 studies\n",
            "  ‚Ä¢ Page 236 ... 100 studies\n",
            "  ‚Ä¢ Page 237 ... 100 studies\n",
            "  ‚Ä¢ Page 238 ... 100 studies\n",
            "  ‚Ä¢ Page 239 ... 100 studies\n",
            "  ‚Ä¢ Page 240 ... 100 studies\n",
            "  ‚Ä¢ Page 241 ... 100 studies\n",
            "  ‚Ä¢ Page 242 ... 100 studies\n",
            "  ‚Ä¢ Page 243 ... 100 studies\n",
            "  ‚Ä¢ Page 244 ... 100 studies\n",
            "  ‚Ä¢ Page 245 ... 100 studies\n",
            "  ‚Ä¢ Page 246 ... 100 studies\n",
            "  ‚Ä¢ Page 247 ... 100 studies\n",
            "  ‚Ä¢ Page 248 ... 100 studies\n",
            "  ‚Ä¢ Page 249 ... 100 studies\n",
            "  ‚Ä¢ Page 250 ... 100 studies\n",
            "  ‚Ä¢ Page 251 ... 100 studies\n",
            "  ‚Ä¢ Page 252 ... 100 studies\n",
            "  ‚Ä¢ Page 253 ... 100 studies\n",
            "  ‚Ä¢ Page 254 ... 100 studies\n",
            "  ‚Ä¢ Page 255 ... 100 studies\n",
            "  ‚Ä¢ Page 256 ... 100 studies\n",
            "  ‚Ä¢ Page 257 ... 100 studies\n",
            "  ‚Ä¢ Page 258 ... 100 studies\n",
            "  ‚Ä¢ Page 259 ... 100 studies\n",
            "  ‚Ä¢ Page 260 ... 100 studies\n",
            "  ‚Ä¢ Page 261 ... 100 studies\n",
            "  ‚Ä¢ Page 262 ... 100 studies\n",
            "  ‚Ä¢ Page 263 ... 100 studies\n",
            "  ‚Ä¢ Page 264 ... 100 studies\n",
            "  ‚Ä¢ Page 265 ... 100 studies\n",
            "  ‚Ä¢ Page 266 ... 100 studies\n",
            "  ‚Ä¢ Page 267 ... 100 studies\n",
            "  ‚Ä¢ Page 268 ... 100 studies\n",
            "  ‚Ä¢ Page 269 ... 100 studies\n",
            "  ‚Ä¢ Page 270 ... 100 studies\n",
            "  ‚Ä¢ Page 271 ... 100 studies\n",
            "  ‚Ä¢ Page 272 ... 100 studies\n",
            "  ‚Ä¢ Page 273 ... 100 studies\n",
            "  ‚Ä¢ Page 274 ... 100 studies\n",
            "  ‚Ä¢ Page 275 ... 100 studies\n",
            "  ‚Ä¢ Page 276 ... 100 studies\n",
            "  ‚Ä¢ Page 277 ... 100 studies\n",
            "  ‚Ä¢ Page 278 ... 100 studies\n",
            "  ‚Ä¢ Page 279 ... 100 studies\n",
            "  ‚Ä¢ Page 280 ... 100 studies\n",
            "  ‚Ä¢ Page 281 ... 100 studies\n",
            "  ‚Ä¢ Page 282 ... 100 studies\n",
            "  ‚Ä¢ Page 283 ... 100 studies\n",
            "  ‚Ä¢ Page 284 ... 100 studies\n",
            "  ‚Ä¢ Page 285 ... 100 studies\n",
            "  ‚Ä¢ Page 286 ... 100 studies\n",
            "  ‚Ä¢ Page 287 ... 100 studies\n",
            "  ‚Ä¢ Page 288 ... 100 studies\n",
            "  ‚Ä¢ Page 289 ... 100 studies\n",
            "  ‚Ä¢ Page 290 ... 100 studies\n",
            "  ‚Ä¢ Page 291 ... 100 studies\n",
            "  ‚Ä¢ Page 292 ... 100 studies\n",
            "  ‚Ä¢ Page 293 ... 100 studies\n",
            "  ‚Ä¢ Page 294 ... 100 studies\n",
            "  ‚Ä¢ Page 295 ... 100 studies\n",
            "  ‚Ä¢ Page 296 ... 100 studies\n",
            "  ‚Ä¢ Page 297 ... 100 studies\n",
            "  ‚Ä¢ Page 298 ... 100 studies\n",
            "  ‚Ä¢ Page 299 ... 100 studies\n",
            "  ‚Ä¢ Page 300 ... 100 studies\n",
            "  ‚Ä¢ Page 301 ... 100 studies\n",
            "  ‚Ä¢ Page 302 ... 100 studies\n",
            "  ‚Ä¢ Page 303 ... 100 studies\n",
            "  ‚Ä¢ Page 304 ... 100 studies\n",
            "  ‚Ä¢ Page 305 ... 100 studies\n",
            "  ‚Ä¢ Page 306 ... 100 studies\n",
            "  ‚Ä¢ Page 307 ... 100 studies\n",
            "  ‚Ä¢ Page 308 ... 100 studies\n",
            "  ‚Ä¢ Page 309 ... 100 studies\n",
            "  ‚Ä¢ Page 310 ... 100 studies\n",
            "  ‚Ä¢ Page 311 ... 100 studies\n",
            "  ‚Ä¢ Page 312 ... 100 studies\n",
            "  ‚Ä¢ Page 313 ... 100 studies\n",
            "  ‚Ä¢ Page 314 ... 100 studies\n",
            "  ‚Ä¢ Page 315 ... 100 studies\n",
            "  ‚Ä¢ Page 316 ... 100 studies\n",
            "  ‚Ä¢ Page 317 ... 100 studies\n",
            "  ‚Ä¢ Page 318 ... 100 studies\n",
            "  ‚Ä¢ Page 319 ... 100 studies\n",
            "  ‚Ä¢ Page 320 ... 100 studies\n",
            "  ‚Ä¢ Page 321 ... 100 studies\n",
            "  ‚Ä¢ Page 322 ... 100 studies\n",
            "  ‚Ä¢ Page 323 ... 100 studies\n",
            "  ‚Ä¢ Page 324 ... 100 studies\n",
            "  ‚Ä¢ Page 325 ... 100 studies\n",
            "  ‚Ä¢ Page 326 ... 100 studies\n",
            "  ‚Ä¢ Page 327 ... 100 studies\n",
            "  ‚Ä¢ Page 328 ... 100 studies\n",
            "  ‚Ä¢ Page 329 ... 100 studies\n",
            "  ‚Ä¢ Page 330 ... 100 studies\n",
            "  ‚Ä¢ Page 331 ... 100 studies\n",
            "  ‚Ä¢ Page 332 ... 100 studies\n",
            "  ‚Ä¢ Page 333 ... 100 studies\n",
            "  ‚Ä¢ Page 334 ... 100 studies\n",
            "  ‚Ä¢ Page 335 ... 100 studies\n",
            "  ‚Ä¢ Page 336 ... 100 studies\n",
            "  ‚Ä¢ Page 337 ... 100 studies\n",
            "  ‚Ä¢ Page 338 ... 100 studies\n",
            "  ‚Ä¢ Page 339 ... 100 studies\n",
            "  ‚Ä¢ Page 340 ... 100 studies\n",
            "  ‚Ä¢ Page 341 ... 100 studies\n",
            "  ‚Ä¢ Page 342 ... 100 studies\n",
            "  ‚Ä¢ Page 343 ... 100 studies\n",
            "  ‚Ä¢ Page 344 ... 100 studies\n",
            "  ‚Ä¢ Page 345 ... 100 studies\n",
            "  ‚Ä¢ Page 346 ... 100 studies\n",
            "  ‚Ä¢ Page 347 ... 100 studies\n",
            "  ‚Ä¢ Page 348 ... 100 studies\n",
            "  ‚Ä¢ Page 349 ... 100 studies\n",
            "  ‚Ä¢ Page 350 ... 100 studies\n",
            "  ‚Ä¢ Page 351 ... 100 studies\n",
            "  ‚Ä¢ Page 352 ... 100 studies\n",
            "  ‚Ä¢ Page 353 ... 100 studies\n",
            "  ‚Ä¢ Page 354 ... 100 studies\n",
            "  ‚Ä¢ Page 355 ... 100 studies\n",
            "  ‚Ä¢ Page 356 ... 100 studies\n",
            "  ‚Ä¢ Page 357 ... 100 studies\n",
            "  ‚Ä¢ Page 358 ... 100 studies\n",
            "  ‚Ä¢ Page 359 ... 100 studies\n",
            "  ‚Ä¢ Page 360 ... 100 studies\n",
            "  ‚Ä¢ Page 361 ... 100 studies\n",
            "  ‚Ä¢ Page 362 ... 100 studies\n",
            "  ‚Ä¢ Page 363 ... 100 studies\n",
            "  ‚Ä¢ Page 364 ... 100 studies\n",
            "  ‚Ä¢ Page 365 ... 100 studies\n",
            "  ‚Ä¢ Page 366 ... 100 studies\n",
            "  ‚Ä¢ Page 367 ... 100 studies\n",
            "  ‚Ä¢ Page 368 ... 100 studies\n",
            "  ‚Ä¢ Page 369 ... 100 studies\n",
            "  ‚Ä¢ Page 370 ... 100 studies\n",
            "  ‚Ä¢ Page 371 ... 100 studies\n",
            "  ‚Ä¢ Page 372 ... 100 studies\n",
            "  ‚Ä¢ Page 373 ... 100 studies\n",
            "  ‚Ä¢ Page 374 ... 100 studies\n",
            "  ‚Ä¢ Page 375 ... 100 studies\n",
            "  ‚Ä¢ Page 376 ... 100 studies\n",
            "  ‚Ä¢ Page 377 ... 100 studies\n",
            "  ‚Ä¢ Page 378 ... 100 studies\n",
            "  ‚Ä¢ Page 379 ... 100 studies\n",
            "  ‚Ä¢ Page 380 ... 100 studies\n",
            "  ‚Ä¢ Page 381 ... 100 studies\n",
            "  ‚Ä¢ Page 382 ... 100 studies\n",
            "  ‚Ä¢ Page 383 ... 100 studies\n",
            "  ‚Ä¢ Page 384 ... 100 studies\n",
            "  ‚Ä¢ Page 385 ... 100 studies\n",
            "  ‚Ä¢ Page 386 ... 100 studies\n",
            "  ‚Ä¢ Page 387 ... 100 studies\n",
            "  ‚Ä¢ Page 388 ... 100 studies\n",
            "  ‚Ä¢ Page 389 ... 100 studies\n",
            "  ‚Ä¢ Page 390 ... 100 studies\n",
            "  ‚Ä¢ Page 391 ... 100 studies\n",
            "  ‚Ä¢ Page 392 ... 100 studies\n",
            "  ‚Ä¢ Page 393 ... 100 studies\n",
            "  ‚Ä¢ Page 394 ... 100 studies\n",
            "  ‚Ä¢ Page 395 ... 100 studies\n",
            "  ‚Ä¢ Page 396 ... 100 studies\n",
            "  ‚Ä¢ Page 397 ... 100 studies\n",
            "  ‚Ä¢ Page 398 ... 100 studies\n",
            "  ‚Ä¢ Page 399 ... 100 studies\n",
            "  ‚Ä¢ Page 400 ... 100 studies\n",
            "  ‚Ä¢ Page 401 ... 100 studies\n",
            "  ‚Ä¢ Page 402 ... 100 studies\n",
            "  ‚Ä¢ Page 403 ... 100 studies\n",
            "  ‚Ä¢ Page 404 ... 100 studies\n",
            "  ‚Ä¢ Page 405 ... 100 studies\n",
            "  ‚Ä¢ Page 406 ... 100 studies\n",
            "  ‚Ä¢ Page 407 ... 100 studies\n",
            "  ‚Ä¢ Page 408 ... 100 studies\n",
            "  ‚Ä¢ Page 409 ... 100 studies\n",
            "  ‚Ä¢ Page 410 ... 100 studies\n",
            "  ‚Ä¢ Page 411 ... 100 studies\n",
            "  ‚Ä¢ Page 412 ... 100 studies\n",
            "  ‚Ä¢ Page 413 ... 100 studies\n",
            "  ‚Ä¢ Page 414 ... 100 studies\n",
            "  ‚Ä¢ Page 415 ... 100 studies\n",
            "  ‚Ä¢ Page 416 ... 100 studies\n",
            "  ‚Ä¢ Page 417 ... 100 studies\n",
            "  ‚Ä¢ Page 418 ... 100 studies\n",
            "  ‚Ä¢ Page 419 ... 100 studies\n",
            "  ‚Ä¢ Page 420 ... 100 studies\n",
            "  ‚Ä¢ Page 421 ... 100 studies\n",
            "  ‚Ä¢ Page 422 ... 100 studies\n",
            "  ‚Ä¢ Page 423 ... 100 studies\n",
            "  ‚Ä¢ Page 424 ... 100 studies\n",
            "  ‚Ä¢ Page 425 ... 100 studies\n",
            "  ‚Ä¢ Page 426 ... 100 studies\n",
            "  ‚Ä¢ Page 427 ... 100 studies\n",
            "  ‚Ä¢ Page 428 ... 100 studies\n",
            "  ‚Ä¢ Page 429 ... 100 studies\n",
            "  ‚Ä¢ Page 430 ... 100 studies\n",
            "  ‚Ä¢ Page 431 ... 100 studies\n",
            "  ‚Ä¢ Page 432 ... 100 studies\n",
            "  ‚Ä¢ Page 433 ... 100 studies\n",
            "  ‚Ä¢ Page 434 ... 100 studies\n",
            "  ‚Ä¢ Page 435 ... 100 studies\n",
            "  ‚Ä¢ Page 436 ... 100 studies\n",
            "  ‚Ä¢ Page 437 ... 100 studies\n",
            "  ‚Ä¢ Page 438 ... 100 studies\n",
            "  ‚Ä¢ Page 439 ... 100 studies\n",
            "  ‚Ä¢ Page 440 ... 100 studies\n",
            "  ‚Ä¢ Page 441 ... 100 studies\n",
            "  ‚Ä¢ Page 442 ... 100 studies\n",
            "  ‚Ä¢ Page 443 ... 100 studies\n",
            "  ‚Ä¢ Page 444 ... 100 studies\n",
            "  ‚Ä¢ Page 445 ... 100 studies\n",
            "  ‚Ä¢ Page 446 ... 100 studies\n",
            "  ‚Ä¢ Page 447 ... 100 studies\n",
            "  ‚Ä¢ Page 448 ... 100 studies\n",
            "  ‚Ä¢ Page 449 ... 100 studies\n",
            "  ‚Ä¢ Page 450 ... 100 studies\n",
            "  ‚Ä¢ Page 451 ... 100 studies\n",
            "  ‚Ä¢ Page 452 ... 100 studies\n",
            "  ‚Ä¢ Page 453 ... 100 studies\n",
            "  ‚Ä¢ Page 454 ... 100 studies\n",
            "  ‚Ä¢ Page 455 ... 100 studies\n",
            "  ‚Ä¢ Page 456 ... 100 studies\n",
            "  ‚Ä¢ Page 457 ... 100 studies\n",
            "  ‚Ä¢ Page 458 ... 100 studies\n",
            "  ‚Ä¢ Page 459 ... 100 studies\n",
            "  ‚Ä¢ Page 460 ... 100 studies\n",
            "  ‚Ä¢ Page 461 ... 100 studies\n",
            "  ‚Ä¢ Page 462 ... 100 studies\n",
            "  ‚Ä¢ Page 463 ... 100 studies\n",
            "  ‚Ä¢ Page 464 ... 100 studies\n",
            "  ‚Ä¢ Page 465 ... 100 studies\n",
            "  ‚Ä¢ Page 466 ... 100 studies\n",
            "  ‚Ä¢ Page 467 ... 100 studies\n",
            "  ‚Ä¢ Page 468 ... 100 studies\n",
            "  ‚Ä¢ Page 469 ... 100 studies\n",
            "  ‚Ä¢ Page 470 ... 100 studies\n",
            "  ‚Ä¢ Page 471 ... 100 studies\n",
            "  ‚Ä¢ Page 472 ... 100 studies\n",
            "  ‚Ä¢ Page 473 ... 100 studies\n",
            "  ‚Ä¢ Page 474 ... 100 studies\n",
            "  ‚Ä¢ Page 475 ... 100 studies\n",
            "  ‚Ä¢ Page 476 ... 100 studies\n",
            "  ‚Ä¢ Page 477 ... 100 studies\n",
            "  ‚Ä¢ Page 478 ... 100 studies\n",
            "  ‚Ä¢ Page 479 ... 100 studies\n",
            "  ‚Ä¢ Page 480 ... 100 studies\n",
            "  ‚Ä¢ Page 481 ... 100 studies\n",
            "  ‚Ä¢ Page 482 ... 100 studies\n",
            "  ‚Ä¢ Page 483 ... 100 studies\n",
            "  ‚Ä¢ Page 484 ... 100 studies\n",
            "  ‚Ä¢ Page 485 ... 100 studies\n",
            "  ‚Ä¢ Page 486 ... 100 studies\n",
            "  ‚Ä¢ Page 487 ... 100 studies\n",
            "  ‚Ä¢ Page 488 ... 100 studies\n",
            "  ‚Ä¢ Page 489 ... 100 studies\n",
            "  ‚Ä¢ Page 490 ... 100 studies\n",
            "  ‚Ä¢ Page 491 ... 100 studies\n",
            "  ‚Ä¢ Page 492 ... 100 studies\n",
            "  ‚Ä¢ Page 493 ... 100 studies\n",
            "  ‚Ä¢ Page 494 ... 100 studies\n",
            "  ‚Ä¢ Page 495 ... 100 studies\n",
            "  ‚Ä¢ Page 496 ... 100 studies\n",
            "  ‚Ä¢ Page 497 ... 100 studies\n",
            "  ‚Ä¢ Page 498 ... 100 studies\n",
            "  ‚Ä¢ Page 499 ... 100 studies\n",
            "  ‚Ä¢ Page 500 ... 100 studies\n",
            "  ‚Ä¢ Page 501 ... 100 studies\n",
            "  ‚Ä¢ Page 502 ... 100 studies\n",
            "  ‚Ä¢ Page 503 ... 100 studies\n",
            "  ‚Ä¢ Page 504 ... 100 studies\n",
            "  ‚Ä¢ Page 505 ... 100 studies\n",
            "  ‚Ä¢ Page 506 ... 100 studies\n",
            "  ‚Ä¢ Page 507 ... 100 studies\n",
            "  ‚Ä¢ Page 508 ... 100 studies\n",
            "  ‚Ä¢ Page 509 ... 100 studies\n",
            "  ‚Ä¢ Page 510 ... 100 studies\n",
            "  ‚Ä¢ Page 511 ... 100 studies\n",
            "  ‚Ä¢ Page 512 ... 100 studies\n",
            "  ‚Ä¢ Page 513 ... 100 studies\n",
            "  ‚Ä¢ Page 514 ... 100 studies\n",
            "  ‚Ä¢ Page 515 ... 100 studies\n",
            "  ‚Ä¢ Page 516 ... 100 studies\n",
            "  ‚Ä¢ Page 517 ... 100 studies\n",
            "  ‚Ä¢ Page 518 ... 100 studies\n",
            "  ‚Ä¢ Page 519 ... 100 studies\n",
            "  ‚Ä¢ Page 520 ... 100 studies\n",
            "  ‚Ä¢ Page 521 ... 100 studies\n",
            "  ‚Ä¢ Page 522 ... 100 studies\n",
            "  ‚Ä¢ Page 523 ... 100 studies\n",
            "  ‚Ä¢ Page 524 ... 100 studies\n",
            "  ‚Ä¢ Page 525 ... 100 studies\n",
            "  ‚Ä¢ Page 526 ... 100 studies\n",
            "  ‚Ä¢ Page 527 ... 100 studies\n",
            "  ‚Ä¢ Page 528 ... 100 studies\n",
            "  ‚Ä¢ Page 529 ... 100 studies\n",
            "  ‚Ä¢ Page 530 ... 100 studies\n",
            "  ‚Ä¢ Page 531 ... 100 studies\n",
            "  ‚Ä¢ Page 532 ... 100 studies\n",
            "  ‚Ä¢ Page 533 ... 100 studies\n",
            "  ‚Ä¢ Page 534 ... 100 studies\n",
            "  ‚Ä¢ Page 535 ... 100 studies\n",
            "  ‚Ä¢ Page 536 ... 100 studies\n",
            "  ‚Ä¢ Page 537 ... 100 studies\n",
            "  ‚Ä¢ Page 538 ... 100 studies\n",
            "  ‚Ä¢ Page 539 ... 100 studies\n",
            "  ‚Ä¢ Page 540 ... 100 studies\n",
            "  ‚Ä¢ Page 541 ... 100 studies\n",
            "  ‚Ä¢ Page 542 ... 100 studies\n",
            "  ‚Ä¢ Page 543 ... 100 studies\n",
            "  ‚Ä¢ Page 544 ... 100 studies\n",
            "  ‚Ä¢ Page 545 ... 100 studies\n",
            "  ‚Ä¢ Page 546 ... 100 studies\n",
            "  ‚Ä¢ Page 547 ... 100 studies\n",
            "  ‚Ä¢ Page 548 ... 100 studies\n",
            "  ‚Ä¢ Page 549 ... 100 studies\n",
            "  ‚Ä¢ Page 550 ... 100 studies\n",
            "  ‚Ä¢ Page 551 ... 100 studies\n",
            "  ‚Ä¢ Page 552 ... 100 studies\n",
            "  ‚Ä¢ Page 553 ... 100 studies\n",
            "  ‚Ä¢ Page 554 ... 100 studies\n",
            "  ‚Ä¢ Page 555 ... 100 studies\n",
            "  ‚Ä¢ Page 556 ... 100 studies\n",
            "  ‚Ä¢ Page 557 ... 100 studies\n",
            "  ‚Ä¢ Page 558 ... 100 studies\n",
            "  ‚Ä¢ Page 559 ... 100 studies\n",
            "  ‚Ä¢ Page 560 ... 100 studies\n",
            "  ‚Ä¢ Page 561 ... 100 studies\n",
            "  ‚Ä¢ Page 562 ... 100 studies\n",
            "  ‚Ä¢ Page 563 ... 100 studies\n",
            "  ‚Ä¢ Page 564 ... 100 studies\n",
            "  ‚Ä¢ Page 565 ... 100 studies\n",
            "  ‚Ä¢ Page 566 ... 100 studies\n",
            "  ‚Ä¢ Page 567 ... 100 studies\n",
            "  ‚Ä¢ Page 568 ... 100 studies\n",
            "  ‚Ä¢ Page 569 ... 100 studies\n",
            "  ‚Ä¢ Page 570 ... 100 studies\n",
            "  ‚Ä¢ Page 571 ... 100 studies\n",
            "  ‚Ä¢ Page 572 ... 100 studies\n",
            "  ‚Ä¢ Page 573 ... 100 studies\n",
            "  ‚Ä¢ Page 574 ... 100 studies\n",
            "  ‚Ä¢ Page 575 ... 100 studies\n",
            "  ‚Ä¢ Page 576 ... 100 studies\n",
            "  ‚Ä¢ Page 577 ... 100 studies\n",
            "  ‚Ä¢ Page 578 ... 100 studies\n",
            "  ‚Ä¢ Page 579 ... 100 studies\n",
            "  ‚Ä¢ Page 580 ... 100 studies\n",
            "  ‚Ä¢ Page 581 ... 100 studies\n",
            "  ‚Ä¢ Page 582 ... 100 studies\n",
            "  ‚Ä¢ Page 583 ... 100 studies\n",
            "  ‚Ä¢ Page 584 ... 100 studies\n",
            "  ‚Ä¢ Page 585 ... 100 studies\n",
            "  ‚Ä¢ Page 586 ... 100 studies\n",
            "  ‚Ä¢ Page 587 ... 100 studies\n",
            "  ‚Ä¢ Page 588 ... 100 studies\n",
            "  ‚Ä¢ Page 589 ... 100 studies\n",
            "  ‚Ä¢ Page 590 ... 100 studies\n",
            "  ‚Ä¢ Page 591 ... 100 studies\n",
            "  ‚Ä¢ Page 592 ... 100 studies\n",
            "  ‚Ä¢ Page 593 ... 100 studies\n",
            "  ‚Ä¢ Page 594 ... 100 studies\n",
            "  ‚Ä¢ Page 595 ... 100 studies\n",
            "  ‚Ä¢ Page 596 ... 100 studies\n",
            "  ‚Ä¢ Page 597 ... 100 studies\n",
            "  ‚Ä¢ Page 598 ... 100 studies\n",
            "  ‚Ä¢ Page 599 ... 100 studies\n",
            "  ‚Ä¢ Page 600 ... 100 studies\n",
            "  ‚Ä¢ Page 601 ... 100 studies\n",
            "  ‚Ä¢ Page 602 ... 100 studies\n",
            "  ‚Ä¢ Page 603 ... 100 studies\n",
            "  ‚Ä¢ Page 604 ... 100 studies\n",
            "  ‚Ä¢ Page 605 ... 100 studies\n",
            "  ‚Ä¢ Page 606 ... 100 studies\n",
            "  ‚Ä¢ Page 607 ... 100 studies\n",
            "  ‚Ä¢ Page 608 ... 100 studies\n",
            "  ‚Ä¢ Page 609 ... 100 studies\n",
            "  ‚Ä¢ Page 610 ... 100 studies\n",
            "  ‚Ä¢ Page 611 ... 100 studies\n",
            "  ‚Ä¢ Page 612 ... 100 studies\n",
            "  ‚Ä¢ Page 613 ... 100 studies\n",
            "  ‚Ä¢ Page 614 ... 100 studies\n",
            "  ‚Ä¢ Page 615 ... 100 studies\n",
            "  ‚Ä¢ Page 616 ... 100 studies\n",
            "  ‚Ä¢ Page 617 ... 100 studies\n",
            "  ‚Ä¢ Page 618 ... 100 studies\n",
            "  ‚Ä¢ Page 619 ... 100 studies\n",
            "  ‚Ä¢ Page 620 ... 100 studies\n",
            "  ‚Ä¢ Page 621 ... 100 studies\n",
            "  ‚Ä¢ Page 622 ... 100 studies\n",
            "  ‚Ä¢ Page 623 ... 100 studies\n",
            "  ‚Ä¢ Page 624 ... 87 studies\n",
            "‚úÖ Total fetched for Cardiovascular Diseases: 62387\n",
            "   ‚Üí Raw rows for cardiovascular: 62387\n",
            "   ‚Ä¢ Removed 5870 bad-status trials\n",
            "   ‚Ä¢ Removed 23 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for cardiovascular: 56494\n",
            "üíæ Saved 56494 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_cardiovascular_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: ckd  (query='Chronic Kidney Disease')\n",
            "\n",
            "üöÄ Fetching studies for condition: Chronic Kidney Disease\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 75 studies\n",
            "‚úÖ Total fetched for Chronic Kidney Disease: 5375\n",
            "   ‚Üí Raw rows for ckd: 5375\n",
            "   ‚Ä¢ Removed 519 bad-status trials\n",
            "   ‚Ä¢ Removed 3 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for ckd: 4853\n",
            "üíæ Saved 4853 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_ckd_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: alzheimer  (query='Alzheimer Disease')\n",
            "\n",
            "üöÄ Fetching studies for condition: Alzheimer Disease\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 10 studies\n",
            "‚úÖ Total fetched for Alzheimer Disease: 4010\n",
            "   ‚Üí Raw rows for alzheimer: 4010\n",
            "   ‚Ä¢ Removed 400 bad-status trials\n",
            "   ‚Ä¢ Removed 2 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for alzheimer: 3608\n",
            "üíæ Saved 3608 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_alzheimer_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: parkinson  (query='Parkinson Disease')\n",
            "\n",
            "üöÄ Fetching studies for condition: Parkinson Disease\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 50 studies\n",
            "‚úÖ Total fetched for Parkinson Disease: 4650\n",
            "   ‚Üí Raw rows for parkinson: 4650\n",
            "   ‚Ä¢ Removed 385 bad-status trials\n",
            "   ‚Ä¢ Removed 2 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for parkinson: 4263\n",
            "üíæ Saved 4263 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_parkinson_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: asthma  (query='Asthma')\n",
            "\n",
            "üöÄ Fetching studies for condition: Asthma\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 53 studies\n",
            "‚úÖ Total fetched for Asthma: 5053\n",
            "   ‚Üí Raw rows for asthma: 5053\n",
            "   ‚Ä¢ Removed 434 bad-status trials\n",
            "   ‚Ä¢ Removed 4 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for asthma: 4615\n",
            "üíæ Saved 4615 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_asthma_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: copd  (query='Chronic Obstructive Pulmonary Disease')\n",
            "\n",
            "üöÄ Fetching studies for condition: Chronic Obstructive Pulmonary Disease\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 77 studies\n",
            "‚úÖ Total fetched for Chronic Obstructive Pulmonary Disease: 6077\n",
            "   ‚Üí Raw rows for copd: 6077\n",
            "   ‚Ä¢ Removed 530 bad-status trials\n",
            "   ‚Ä¢ Removed 5 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for copd: 5542\n",
            "üíæ Saved 5542 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_copd_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: breast_cancer  (query='Breast Neoplasms')\n",
            "\n",
            "üöÄ Fetching studies for condition: Breast Neoplasms\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 100 studies\n",
            "  ‚Ä¢ Page 104 ... 100 studies\n",
            "  ‚Ä¢ Page 105 ... 100 studies\n",
            "  ‚Ä¢ Page 106 ... 100 studies\n",
            "  ‚Ä¢ Page 107 ... 100 studies\n",
            "  ‚Ä¢ Page 108 ... 100 studies\n",
            "  ‚Ä¢ Page 109 ... 100 studies\n",
            "  ‚Ä¢ Page 110 ... 100 studies\n",
            "  ‚Ä¢ Page 111 ... 100 studies\n",
            "  ‚Ä¢ Page 112 ... 100 studies\n",
            "  ‚Ä¢ Page 113 ... 100 studies\n",
            "  ‚Ä¢ Page 114 ... 100 studies\n",
            "  ‚Ä¢ Page 115 ... 100 studies\n",
            "  ‚Ä¢ Page 116 ... 100 studies\n",
            "  ‚Ä¢ Page 117 ... 100 studies\n",
            "  ‚Ä¢ Page 118 ... 100 studies\n",
            "  ‚Ä¢ Page 119 ... 100 studies\n",
            "  ‚Ä¢ Page 120 ... 100 studies\n",
            "  ‚Ä¢ Page 121 ... 100 studies\n",
            "  ‚Ä¢ Page 122 ... 100 studies\n",
            "  ‚Ä¢ Page 123 ... 100 studies\n",
            "  ‚Ä¢ Page 124 ... 100 studies\n",
            "  ‚Ä¢ Page 125 ... 100 studies\n",
            "  ‚Ä¢ Page 126 ... 100 studies\n",
            "  ‚Ä¢ Page 127 ... 100 studies\n",
            "  ‚Ä¢ Page 128 ... 100 studies\n",
            "  ‚Ä¢ Page 129 ... 100 studies\n",
            "  ‚Ä¢ Page 130 ... 100 studies\n",
            "  ‚Ä¢ Page 131 ... 100 studies\n",
            "  ‚Ä¢ Page 132 ... 100 studies\n",
            "  ‚Ä¢ Page 133 ... 100 studies\n",
            "  ‚Ä¢ Page 134 ... 100 studies\n",
            "  ‚Ä¢ Page 135 ... 100 studies\n",
            "  ‚Ä¢ Page 136 ... 100 studies\n",
            "  ‚Ä¢ Page 137 ... 100 studies\n",
            "  ‚Ä¢ Page 138 ... 100 studies\n",
            "  ‚Ä¢ Page 139 ... 100 studies\n",
            "  ‚Ä¢ Page 140 ... 100 studies\n",
            "  ‚Ä¢ Page 141 ... 100 studies\n",
            "  ‚Ä¢ Page 142 ... 100 studies\n",
            "  ‚Ä¢ Page 143 ... 100 studies\n",
            "  ‚Ä¢ Page 144 ... 100 studies\n",
            "  ‚Ä¢ Page 145 ... 100 studies\n",
            "  ‚Ä¢ Page 146 ... 100 studies\n",
            "  ‚Ä¢ Page 147 ... 100 studies\n",
            "  ‚Ä¢ Page 148 ... 100 studies\n",
            "  ‚Ä¢ Page 149 ... 100 studies\n",
            "  ‚Ä¢ Page 150 ... 100 studies\n",
            "  ‚Ä¢ Page 151 ... 100 studies\n",
            "  ‚Ä¢ Page 152 ... 100 studies\n",
            "  ‚Ä¢ Page 153 ... 100 studies\n",
            "  ‚Ä¢ Page 154 ... 100 studies\n",
            "  ‚Ä¢ Page 155 ... 100 studies\n",
            "  ‚Ä¢ Page 156 ... 100 studies\n",
            "  ‚Ä¢ Page 157 ... 100 studies\n",
            "  ‚Ä¢ Page 158 ... 45 studies\n",
            "‚úÖ Total fetched for Breast Neoplasms: 15745\n",
            "   ‚Üí Raw rows for breast_cancer: 15745\n",
            "   ‚Ä¢ Removed 1904 bad-status trials\n",
            "   ‚Ä¢ Removed 5 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for breast_cancer: 13836\n",
            "üíæ Saved 13836 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_breast_cancer_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: lung_cancer  (query='Lung Neoplasms')\n",
            "\n",
            "üöÄ Fetching studies for condition: Lung Neoplasms\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 100 studies\n",
            "  ‚Ä¢ Page 104 ... 100 studies\n",
            "  ‚Ä¢ Page 105 ... 100 studies\n",
            "  ‚Ä¢ Page 106 ... 100 studies\n",
            "  ‚Ä¢ Page 107 ... 100 studies\n",
            "  ‚Ä¢ Page 108 ... 100 studies\n",
            "  ‚Ä¢ Page 109 ... 100 studies\n",
            "  ‚Ä¢ Page 110 ... 100 studies\n",
            "  ‚Ä¢ Page 111 ... 100 studies\n",
            "  ‚Ä¢ Page 112 ... 100 studies\n",
            "  ‚Ä¢ Page 113 ... 100 studies\n",
            "  ‚Ä¢ Page 114 ... 100 studies\n",
            "  ‚Ä¢ Page 115 ... 100 studies\n",
            "  ‚Ä¢ Page 116 ... 100 studies\n",
            "  ‚Ä¢ Page 117 ... 100 studies\n",
            "  ‚Ä¢ Page 118 ... 100 studies\n",
            "  ‚Ä¢ Page 119 ... 100 studies\n",
            "  ‚Ä¢ Page 120 ... 100 studies\n",
            "  ‚Ä¢ Page 121 ... 100 studies\n",
            "  ‚Ä¢ Page 122 ... 100 studies\n",
            "  ‚Ä¢ Page 123 ... 100 studies\n",
            "  ‚Ä¢ Page 124 ... 100 studies\n",
            "  ‚Ä¢ Page 125 ... 100 studies\n",
            "  ‚Ä¢ Page 126 ... 100 studies\n",
            "  ‚Ä¢ Page 127 ... 100 studies\n",
            "  ‚Ä¢ Page 128 ... 100 studies\n",
            "  ‚Ä¢ Page 129 ... 100 studies\n",
            "  ‚Ä¢ Page 130 ... 100 studies\n",
            "  ‚Ä¢ Page 131 ... 100 studies\n",
            "  ‚Ä¢ Page 132 ... 100 studies\n",
            "  ‚Ä¢ Page 133 ... 100 studies\n",
            "  ‚Ä¢ Page 134 ... 100 studies\n",
            "  ‚Ä¢ Page 135 ... 100 studies\n",
            "  ‚Ä¢ Page 136 ... 68 studies\n",
            "‚úÖ Total fetched for Lung Neoplasms: 13568\n",
            "   ‚Üí Raw rows for lung_cancer: 13568\n",
            "   ‚Ä¢ Removed 1926 bad-status trials\n",
            "   ‚Ä¢ Removed 6 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for lung_cancer: 11636\n",
            "üíæ Saved 11636 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_lung_cancer_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: prostate_cancer  (query='Prostatic Neoplasms')\n",
            "\n",
            "üöÄ Fetching studies for condition: Prostatic Neoplasms\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 37 studies\n",
            "‚úÖ Total fetched for Prostatic Neoplasms: 7137\n",
            "   ‚Üí Raw rows for prostate_cancer: 7137\n",
            "   ‚Ä¢ Removed 979 bad-status trials\n",
            "   ‚Ä¢ Removed 1 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for prostate_cancer: 6157\n",
            "üíæ Saved 6157 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_prostate_cancer_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: stroke  (query='Stroke')\n",
            "\n",
            "üöÄ Fetching studies for condition: Stroke\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 100 studies\n",
            "  ‚Ä¢ Page 38 ... 100 studies\n",
            "  ‚Ä¢ Page 39 ... 100 studies\n",
            "  ‚Ä¢ Page 40 ... 100 studies\n",
            "  ‚Ä¢ Page 41 ... 100 studies\n",
            "  ‚Ä¢ Page 42 ... 100 studies\n",
            "  ‚Ä¢ Page 43 ... 100 studies\n",
            "  ‚Ä¢ Page 44 ... 100 studies\n",
            "  ‚Ä¢ Page 45 ... 100 studies\n",
            "  ‚Ä¢ Page 46 ... 100 studies\n",
            "  ‚Ä¢ Page 47 ... 100 studies\n",
            "  ‚Ä¢ Page 48 ... 100 studies\n",
            "  ‚Ä¢ Page 49 ... 100 studies\n",
            "  ‚Ä¢ Page 50 ... 100 studies\n",
            "  ‚Ä¢ Page 51 ... 100 studies\n",
            "  ‚Ä¢ Page 52 ... 100 studies\n",
            "  ‚Ä¢ Page 53 ... 100 studies\n",
            "  ‚Ä¢ Page 54 ... 100 studies\n",
            "  ‚Ä¢ Page 55 ... 100 studies\n",
            "  ‚Ä¢ Page 56 ... 100 studies\n",
            "  ‚Ä¢ Page 57 ... 100 studies\n",
            "  ‚Ä¢ Page 58 ... 100 studies\n",
            "  ‚Ä¢ Page 59 ... 100 studies\n",
            "  ‚Ä¢ Page 60 ... 100 studies\n",
            "  ‚Ä¢ Page 61 ... 100 studies\n",
            "  ‚Ä¢ Page 62 ... 100 studies\n",
            "  ‚Ä¢ Page 63 ... 100 studies\n",
            "  ‚Ä¢ Page 64 ... 100 studies\n",
            "  ‚Ä¢ Page 65 ... 100 studies\n",
            "  ‚Ä¢ Page 66 ... 100 studies\n",
            "  ‚Ä¢ Page 67 ... 100 studies\n",
            "  ‚Ä¢ Page 68 ... 100 studies\n",
            "  ‚Ä¢ Page 69 ... 100 studies\n",
            "  ‚Ä¢ Page 70 ... 100 studies\n",
            "  ‚Ä¢ Page 71 ... 100 studies\n",
            "  ‚Ä¢ Page 72 ... 100 studies\n",
            "  ‚Ä¢ Page 73 ... 100 studies\n",
            "  ‚Ä¢ Page 74 ... 100 studies\n",
            "  ‚Ä¢ Page 75 ... 100 studies\n",
            "  ‚Ä¢ Page 76 ... 100 studies\n",
            "  ‚Ä¢ Page 77 ... 100 studies\n",
            "  ‚Ä¢ Page 78 ... 100 studies\n",
            "  ‚Ä¢ Page 79 ... 100 studies\n",
            "  ‚Ä¢ Page 80 ... 100 studies\n",
            "  ‚Ä¢ Page 81 ... 100 studies\n",
            "  ‚Ä¢ Page 82 ... 100 studies\n",
            "  ‚Ä¢ Page 83 ... 100 studies\n",
            "  ‚Ä¢ Page 84 ... 100 studies\n",
            "  ‚Ä¢ Page 85 ... 100 studies\n",
            "  ‚Ä¢ Page 86 ... 100 studies\n",
            "  ‚Ä¢ Page 87 ... 100 studies\n",
            "  ‚Ä¢ Page 88 ... 100 studies\n",
            "  ‚Ä¢ Page 89 ... 100 studies\n",
            "  ‚Ä¢ Page 90 ... 100 studies\n",
            "  ‚Ä¢ Page 91 ... 100 studies\n",
            "  ‚Ä¢ Page 92 ... 100 studies\n",
            "  ‚Ä¢ Page 93 ... 100 studies\n",
            "  ‚Ä¢ Page 94 ... 100 studies\n",
            "  ‚Ä¢ Page 95 ... 100 studies\n",
            "  ‚Ä¢ Page 96 ... 100 studies\n",
            "  ‚Ä¢ Page 97 ... 100 studies\n",
            "  ‚Ä¢ Page 98 ... 100 studies\n",
            "  ‚Ä¢ Page 99 ... 100 studies\n",
            "  ‚Ä¢ Page 100 ... 100 studies\n",
            "  ‚Ä¢ Page 101 ... 100 studies\n",
            "  ‚Ä¢ Page 102 ... 100 studies\n",
            "  ‚Ä¢ Page 103 ... 6 studies\n",
            "‚úÖ Total fetched for Stroke: 10206\n",
            "   ‚Üí Raw rows for stroke: 10206\n",
            "   ‚Ä¢ Removed 771 bad-status trials\n",
            "   ‚Ä¢ Removed 4 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for stroke: 9431\n",
            "üíæ Saved 9431 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_stroke_full.csv\n",
            "\n",
            "========================================\n",
            "üìö Condition: rheumatoid_arthritis  (query='Rheumatoid Arthritis')\n",
            "\n",
            "üöÄ Fetching studies for condition: Rheumatoid Arthritis\n",
            "  ‚Ä¢ Page 1 ... 100 studies\n",
            "  ‚Ä¢ Page 2 ... 100 studies\n",
            "  ‚Ä¢ Page 3 ... 100 studies\n",
            "  ‚Ä¢ Page 4 ... 100 studies\n",
            "  ‚Ä¢ Page 5 ... 100 studies\n",
            "  ‚Ä¢ Page 6 ... 100 studies\n",
            "  ‚Ä¢ Page 7 ... 100 studies\n",
            "  ‚Ä¢ Page 8 ... 100 studies\n",
            "  ‚Ä¢ Page 9 ... 100 studies\n",
            "  ‚Ä¢ Page 10 ... 100 studies\n",
            "  ‚Ä¢ Page 11 ... 100 studies\n",
            "  ‚Ä¢ Page 12 ... 100 studies\n",
            "  ‚Ä¢ Page 13 ... 100 studies\n",
            "  ‚Ä¢ Page 14 ... 100 studies\n",
            "  ‚Ä¢ Page 15 ... 100 studies\n",
            "  ‚Ä¢ Page 16 ... 100 studies\n",
            "  ‚Ä¢ Page 17 ... 100 studies\n",
            "  ‚Ä¢ Page 18 ... 100 studies\n",
            "  ‚Ä¢ Page 19 ... 100 studies\n",
            "  ‚Ä¢ Page 20 ... 100 studies\n",
            "  ‚Ä¢ Page 21 ... 100 studies\n",
            "  ‚Ä¢ Page 22 ... 100 studies\n",
            "  ‚Ä¢ Page 23 ... 100 studies\n",
            "  ‚Ä¢ Page 24 ... 100 studies\n",
            "  ‚Ä¢ Page 25 ... 100 studies\n",
            "  ‚Ä¢ Page 26 ... 100 studies\n",
            "  ‚Ä¢ Page 27 ... 100 studies\n",
            "  ‚Ä¢ Page 28 ... 100 studies\n",
            "  ‚Ä¢ Page 29 ... 100 studies\n",
            "  ‚Ä¢ Page 30 ... 100 studies\n",
            "  ‚Ä¢ Page 31 ... 100 studies\n",
            "  ‚Ä¢ Page 32 ... 100 studies\n",
            "  ‚Ä¢ Page 33 ... 100 studies\n",
            "  ‚Ä¢ Page 34 ... 100 studies\n",
            "  ‚Ä¢ Page 35 ... 100 studies\n",
            "  ‚Ä¢ Page 36 ... 100 studies\n",
            "  ‚Ä¢ Page 37 ... 11 studies\n",
            "‚úÖ Total fetched for Rheumatoid Arthritis: 3611\n",
            "   ‚Üí Raw rows for rheumatoid_arthritis: 3611\n",
            "   ‚Ä¢ Removed 370 bad-status trials\n",
            "   ‚Ä¢ Removed 3 with very short/no summary\n",
            "   ‚Ä¢ Removed 0 duplicates (same NCT)\n",
            "‚úÖ Final clean rows for rheumatoid_arthritis: 3238\n",
            "üíæ Saved 3238 rows ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_rheumatoid_arthritis_full.csv\n",
            "\n",
            "======================================\n",
            "üì¶ Creating master combined CSV\n",
            "======================================\n",
            "   ‚Ä¢ Removed 26422 cross-disease duplicates\n",
            "‚úÖ Saved master file with 135187 unique trials ‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_master_full.csv\n",
            "\n",
            "üéâ Done. You now have a clean 15-disease corpus!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvziMQpXhY_J",
        "outputId": "088c15c5-98e6-408f-b9e3-1770757c79d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting update_qdrant_auto.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile update_qdrant_auto.py\n",
        "\"\"\"\n",
        "UPLOAD MASTER CLINICAL TRIAL DATASET TO QDRANT\n",
        "\n",
        "This version:\n",
        "‚úî Loads ONE master CSV (all diseases combined)\n",
        "‚úî Adds disease classification tags for retrieval relevance\n",
        "‚úî Generates embeddings for each trial summary\n",
        "‚úî Uploads to Qdrant vector DB\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "\n",
        "# ==============================\n",
        "# CONFIG\n",
        "# ==============================\n",
        "MASTER_CSV = \"/content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_master_full.csv\"\n",
        "COLLECTION_NAME = \"clinical_trials\"\n",
        "\n",
        "DISEASE_KEYWORDS = {\n",
        "    \"diabetes\": [\"diabetes\", \"type 1 diabetes\", \"type 2 diabetes\", \"hyperglycemia\"],\n",
        "    \"obesity\": [\"obesity\", \"overweight\"],\n",
        "    \"hypertension\": [\"hypertension\", \"high blood pressure\"],\n",
        "    \"cardiovascular\": [\"heart disease\", \"cardiovascular\", \"myocardial\", \"stroke\"],\n",
        "    \"ckd\": [\"kidney\", \"renal\", \"ckd\"],\n",
        "    \"alzheimer\": [\"alzheimer\", \"dementia\"],\n",
        "    \"parkinson\": [\"parkinson\"],\n",
        "    \"asthma\": [\"asthma\"],\n",
        "    \"copd\": [\"copd\", \"chronic obstructive\"],\n",
        "    \"breast_cancer\": [\"breast cancer\", \"breast neoplasm\"],\n",
        "    \"lung_cancer\": [\"lung cancer\", \"lung neoplasm\"],\n",
        "    \"prostate_cancer\": [\"prostate cancer\", \"prostatic\"],\n",
        "    \"rheumatoid_arthritis\": [\"rheumatoid arthritis\"],\n",
        "}\n",
        "\n",
        "\n",
        "def infer_disease(conditions_text):\n",
        "    text = str(conditions_text).lower()\n",
        "    for disease, keywords in DISEASE_KEYWORDS.items():\n",
        "        if any(k in text for k in keywords):\n",
        "            return disease\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "class QdrantMasterUploader:\n",
        "    def __init__(self, qdrant_url, qdrant_api_key):\n",
        "        self.client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
        "        self.embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    def load_master(self):\n",
        "        print(f\"üìÇ Loading master clinical trials file:\\n‚Üí {MASTER_CSV}\")\n",
        "        df = pd.read_csv(MASTER_CSV)\n",
        "        print(f\"   Total trials loaded: {len(df):,}\")\n",
        "        return df\n",
        "\n",
        "    def preprocess(self, df):\n",
        "        print(\"\\nüßπ Cleaning dataset...\")\n",
        "\n",
        "        df[\"status\"] = df[\"status\"].astype(str).str.strip().str.title()\n",
        "\n",
        "        bad_status = [\"Terminated\", \"Withdrawn\", \"Suspended\", \"No Longer Available\", \"Unknown\"]\n",
        "        df = df[~df[\"status\"].isin(bad_status)]\n",
        "\n",
        "        print(f\"   Remaining valid trials: {len(df):,}\")\n",
        "        return df\n",
        "\n",
        "    def chunk_data(self, df):\n",
        "        print(\"\\n‚úÇÔ∏è Creating evidence chunks...\")\n",
        "        chunks = []\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating chunks\"):\n",
        "            text = f\"Title: {row['brief_title']}\\nSummary: {row['brief_summary']}\"\n",
        "\n",
        "            disease = infer_disease(row.get(\"conditions\", \"\"))\n",
        "\n",
        "            chunks.append({\n",
        "                \"nct_id\": row[\"nct_id\"],\n",
        "                \"title\": row[\"brief_title\"],\n",
        "                \"text\": text,\n",
        "                \"status\": row[\"status\"],\n",
        "                \"conditions\": row[\"conditions\"],\n",
        "                \"disease\": disease\n",
        "            })\n",
        "\n",
        "        print(f\"   Total chunks created: {len(chunks):,}\")\n",
        "        return chunks\n",
        "\n",
        "    def generate_embeddings(self, chunks):\n",
        "        print(\"\\nüß† Generating embeddings...\")\n",
        "        texts = [c[\"text\"] for c in chunks]\n",
        "        embeddings = self.embed_model.encode(\n",
        "            texts, batch_size=64, convert_to_numpy=True, show_progress_bar=True\n",
        "        )\n",
        "        print(f\"   Embeddings shape: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "\n",
        "    def upload_to_qdrant(self, embeddings, chunks, mode=\"refresh\"):\n",
        "        if mode == \"refresh\":\n",
        "            print(\"\\nüóëÔ∏è Dropping old Qdrant collection (if exists)...\")\n",
        "            try:\n",
        "                self.client.delete_collection(COLLECTION_NAME)\n",
        "                print(\"   Old collection deleted.\")\n",
        "            except:\n",
        "                print(\"   No existing collection found ‚Äî fresh start.\")\n",
        "\n",
        "            print(\"üì¶ Creating new collection...\")\n",
        "            self.client.create_collection(\n",
        "                COLLECTION_NAME,\n",
        "                vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        "            )\n",
        "            start_id = 0\n",
        "        else:\n",
        "            info = self.client.get_collection(COLLECTION_NAME)\n",
        "            start_id = info.points_count\n",
        "            print(f\"üìå Adding to existing collection from ID {start_id:,}\")\n",
        "\n",
        "        print(\"\\nüöÄ Uploading vectors to Qdrant...\")\n",
        "        batch_size = 100\n",
        "        for i in tqdm(range(0, len(embeddings), batch_size)):\n",
        "            batch_vectors = embeddings[i:i + batch_size]\n",
        "            batch_meta = chunks[i:i + batch_size]\n",
        "\n",
        "            points = [\n",
        "                PointStruct(\n",
        "                    id=start_id + i + j,\n",
        "                    vector=batch_vectors[j].tolist(),\n",
        "                    payload=batch_meta[j]\n",
        "                )\n",
        "                for j in range(len(batch_vectors))\n",
        "            ]\n",
        "\n",
        "            self.client.upsert(COLLECTION_NAME, points)\n",
        "\n",
        "        final_count = self.client.get_collection(COLLECTION_NAME).points_count\n",
        "        print(f\"\\nüéØ Upload complete! Total vectors stored: {final_count:,}\")\n",
        "\n",
        "    def run(self, mode=\"refresh\"):\n",
        "        print(\"\\nüöÄ Starting MASTER upload pipeline...\")\n",
        "\n",
        "        df = self.load_master()\n",
        "        df = self.preprocess(df)\n",
        "        chunks = self.chunk_data(df)\n",
        "        embeddings = self.generate_embeddings(chunks)\n",
        "        self.upload_to_qdrant(embeddings, chunks, mode)\n",
        "\n",
        "        print(\"\\nüéâ Pipeline finished successfully!\")\n",
        "        print(f\"üìå Diseases supported: {list(DISEASE_KEYWORDS.keys())}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import getpass\n",
        "\n",
        "    QDRANT_URL = \"https://215ec69e-fa22-4f38-bcf3-941e73901a68.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "    print(\"üîê Enter Qdrant API key:\")\n",
        "    key = getpass.getpass(\"> \")\n",
        "\n",
        "    mode = input(\"\\nMode: (1) refresh, (2) add ‚Üí \").strip()\n",
        "    mode = \"refresh\" if mode == \"1\" else \"add\"\n",
        "\n",
        "    uploader = QdrantMasterUploader(QDRANT_URL, key)\n",
        "    uploader.run(mode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv4FcQnIhZDz",
        "outputId": "601afa15-59d0-44cc-9440-8c89e943a5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 01:41:49.513623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765244509.545336   19959 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765244509.554989   19959 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765244509.577567   19959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765244509.577599   19959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765244509.577607   19959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765244509.577613   19959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 01:41:49.583989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "üîê Enter Qdrant API key:\n",
            "> \n",
            "\n",
            "Mode: (1) refresh, (2) add ‚Üí 1\n",
            "modules.json: 100% 349/349 [00:00<00:00, 3.11MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 1.18MB/s]\n",
            "README.md: 10.5kB [00:00, 40.9MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 617kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 6.74MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 84.5MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 3.35MB/s]\n",
            "vocab.txt: 232kB [00:00, 26.3MB/s]\n",
            "tokenizer.json: 466kB [00:00, 49.2MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.04MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.94MB/s]\n",
            "\n",
            "üöÄ Starting MASTER upload pipeline...\n",
            "üìÇ Loading master clinical trials file:\n",
            "‚Üí /content/drive/.shortcut-targets-by-id/1GEa1XZQJOK0cM3P2f7hAiyGpAqcErxWG/data/clinical_trials_master_full.csv\n",
            "   Total trials loaded: 135,187\n",
            "\n",
            "üßπ Cleaning dataset...\n",
            "   Remaining valid trials: 112,987\n",
            "\n",
            "‚úÇÔ∏è Creating evidence chunks...\n",
            "Creating chunks: 100% 112987/112987 [00:06<00:00, 18402.71it/s]\n",
            "   Total chunks created: 112,987\n",
            "\n",
            "üß† Generating embeddings...\n",
            "Batches: 100% 1766/1766 [03:20<00:00,  8.79it/s]\n",
            "   Embeddings shape: (112987, 384)\n",
            "\n",
            "üóëÔ∏è Dropping old Qdrant collection (if exists)...\n",
            "   Old collection deleted.\n",
            "üì¶ Creating new collection...\n",
            "\n",
            "üöÄ Uploading vectors to Qdrant...\n",
            "100% 1130/1130 [03:24<00:00,  5.53it/s]\n",
            "\n",
            "üéØ Upload complete! Total vectors stored: 112,987\n",
            "\n",
            "üéâ Pipeline finished successfully!\n",
            "üìå Diseases supported: ['diabetes', 'obesity', 'hypertension', 'cardiovascular', 'ckd', 'alzheimer', 'parkinson', 'asthma', 'copd', 'breast_cancer', 'lung_cancer', 'prostate_cancer', 'rheumatoid_arthritis']\n"
          ]
        }
      ],
      "source": [
        "!python update_qdrant_auto.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQpdvAiNITvP"
      },
      "source": [
        "Update Code to Use Qdrant Instead of FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzgNJNuFCTNh",
        "outputId": "760651fb-6951-428a-8374-8fc209162b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils_qdrant.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils_qdrant.py\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "COLLECTION_NAME = \"clinical_trials\"\n",
        "\n",
        "SUPPORTED_DISEASES = [\n",
        "    \"diabetes\", \"obesity\", \"hypertension\", \"cardiovascular\", \"ckd\",\n",
        "    \"alzheimer\", \"parkinson\", \"asthma\", \"copd\",\n",
        "    \"breast_cancer\", \"lung_cancer\", \"prostate_cancer\",\n",
        "    \"rheumatoid_arthritis\"\n",
        "]\n",
        "\n",
        "# --- Confidence score from cosine similarity ---\n",
        "# score returned by Qdrant: higher is better, max = 1.0+\n",
        "\n",
        "def convert_similarity_to_confidence(score: float) -> float:\n",
        "    \"\"\"Map cosine similarity into readable clinical confidence (0-1 scale).\"\"\"\n",
        "    if score is None:\n",
        "        return 0.0\n",
        "    # Clip values to avoid >1.0 float drift\n",
        "    return max(0.0, min(1.0, score))\n",
        "\n",
        "\n",
        "# --- Load Qdrant client + embedding model (updated) ---\n",
        "\n",
        "def load_qdrant_and_model(qdrant_url: str, qdrant_api_key: str):\n",
        "    \"\"\"Loads Qdrant client + embedding model and verifies dataset.\"\"\"\n",
        "\n",
        "    print(\"‚è≥ Connecting to Qdrant...\")\n",
        "    client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
        "\n",
        "    try:\n",
        "        info = client.get_collection(COLLECTION_NAME)\n",
        "        print(f\"üîó Qdrant Connected ‚Üí {info.points_count:,} trials indexed\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"‚ùå Failed to access Qdrant collection: {e}\")\n",
        "\n",
        "    # Verify disease field exists in payload\n",
        "    try:\n",
        "        test_results = client.query_points(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            query=[0.0] * 384,\n",
        "            with_payload=True,\n",
        "            limit=1\n",
        "        )\n",
        "        sample_payload = test_results.points[0].payload\n",
        "\n",
        "        if \"disease\" not in sample_payload:\n",
        "            print(\"‚ö†Ô∏è WARNING: Disease field missing in payload ‚Äî filtering may degrade\")\n",
        "        else:\n",
        "            print(\"üß† Disease-based retrieval enabled\")\n",
        "\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Could not verify payload fields\")\n",
        "\n",
        "    # Load matching embedding model\n",
        "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    print(\"üß¨ Embedding model initialized\")\n",
        "\n",
        "    print(f\"üìå Supported diseases: {SUPPORTED_DISEASES}\")\n",
        "\n",
        "    return client, embed_model\n",
        "\n",
        "\n",
        "# --- Provenance logging (unchanged) ---\n",
        "\n",
        "def log_provenance_step(agent_name: str, input_data, output_data, detail=None):\n",
        "    \"\"\"Creates a structured log entry for multi-agent tracking\"\"\"\n",
        "    return {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"agent\": agent_name,\n",
        "        \"input\": input_data,\n",
        "        \"output\": output_data,\n",
        "        \"detail\": detail or {},\n",
        "        \"model_version\": \"gemini-2.0-flash\",  # Keep consistent with production runtime\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Reproducibility hash (unchanged) ---\n",
        "\n",
        "def generate_reproducibility_hash(conversation_history, corpus_version: str = \"v1.0\"):\n",
        "    \"\"\"Stable, deterministic ID for rerunning full system evaluations\"\"\"\n",
        "    queries = [turn.get(\"query\", \"\") for turn in conversation_history]\n",
        "    raw = f\"{corpus_version}|{'|'.join(queries)}\"\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM4rCpCCJQVJ",
        "outputId": "a144ddda-2781-4d93-de12-e765b810f510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Qdrant API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Available search methods:\n",
            "['_resolve_query', '_resolve_query_batch_request', '_resolve_query_request', '_scored_points_to_query_responses', 'query', 'query_batch', 'query_batch_points', 'query_points', 'query_points_groups', 'search_matrix_offsets', 'search_matrix_pairs']\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Get API key\n",
        "qdrant_api_key = getpass.getpass(\"üîë Qdrant API Key: \")\n",
        "\n",
        "# Connect\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://215ec69e-fa22-4f38-bcf3-941e73901a68.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "    api_key=qdrant_api_key\n",
        ")\n",
        "\n",
        "# Check what methods are available\n",
        "print(\"Available search methods:\")\n",
        "print([m for m in dir(qdrant_client) if 'search' in m.lower() or 'query' in m.lower()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGBG0m7XIcUi",
        "outputId": "459cb71d-d3d3-4a0c-be13-3c29a1f78253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Qdrant API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "üîç Test Query: 'arthritis trials'\n",
            "\n",
            "üìä Top 3 Results:\n",
            "\n",
            "1. NCT ID: NCT05398471\n",
            "   Score: 0.718\n",
            "   Title: Medical Trial: Appraising Medical Trial Experiences of Rheumatoid Arthritis Pati...\n",
            "\n",
            "2. NCT ID: NCT03486613\n",
            "   Score: 0.673\n",
            "   Title: PROM Collected Via a Smartphone App Versus a Touch Screen Solution Among Patient...\n",
            "\n",
            "3. NCT ID: NCT03194204\n",
            "   Score: 0.668\n",
            "   Title: Re-evaluation of Some Old Rheumatoid Arthritis Therapy: A Randomized Controlled ...\n",
            "\n",
            "‚úÖ Qdrant search working!\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Get API key\n",
        "qdrant_api_key = getpass.getpass(\"üîë Qdrant API Key: \")\n",
        "\n",
        "# Connect\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://215ec69e-fa22-4f38-bcf3-941e73901a68.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "    api_key=qdrant_api_key\n",
        ")\n",
        "\n",
        "# Load model\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Test search\n",
        "test_query = \"arthritis trials\"\n",
        "q_emb = embed_model.encode([test_query])[0]\n",
        "\n",
        "# Use query_points (correct method)\n",
        "results = qdrant_client.query_points(\n",
        "    collection_name=\"clinical_trials\",\n",
        "    query=q_emb.tolist(),\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "print(f\"\\nüîç Test Query: '{test_query}'\")\n",
        "print(f\"\\nüìä Top 3 Results:\\n\")\n",
        "\n",
        "for i, point in enumerate(results.points, 1):\n",
        "    print(f\"{i}. NCT ID: {point.payload['nct_id']}\")\n",
        "    print(f\"   Score: {point.score:.3f}\")\n",
        "    print(f\"   Title: {point.payload['title'][:80]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"‚úÖ Qdrant search working!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbqFEilJxIZ"
      },
      "source": [
        "Fix RetrievalAgent to Use query_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIjl0L_BIcZu",
        "outputId": "b3ae772d-02e4-4ff8-d265-a889de26e3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting retrieval_agent_qdrant.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile retrieval_agent_qdrant.py\n",
        "from typing import List, Dict, Any, Optional\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "from utils_qdrant import (\n",
        "    convert_similarity_to_confidence,\n",
        "    log_provenance_step,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Disease Config for Label + Synonyms\n",
        "# ============================================================\n",
        "\n",
        "CONDITION_CONFIG: Dict[str, str] = {\n",
        "    \"diabetes\": \"Diabetes\",\n",
        "    \"obesity\": \"Obesity\",\n",
        "    \"hypertension\": \"Hypertension\",\n",
        "    \"cardiovascular\": \"Cardiovascular Disease\",\n",
        "    \"ckd\": \"Chronic Kidney Disease\",\n",
        "    \"alzheimer\": \"Alzheimer‚Äôs Disease\",\n",
        "    \"parkinson\": \"Parkinson‚Äôs Disease\",\n",
        "    \"asthma\": \"Asthma\",\n",
        "    \"copd\": \"Chronic Obstructive Pulmonary Disease\",\n",
        "    \"breast_cancer\": \"Breast Cancer\",\n",
        "    \"lung_cancer\": \"Lung Cancer\",\n",
        "    \"prostate_cancer\": \"Prostate Cancer\",\n",
        "    \"stroke\": \"Stroke\",\n",
        "    \"rheumatoid_arthritis\": \"Rheumatoid Arthritis\",\n",
        "}\n",
        "\n",
        "DISEASE_SYNONYMS: Dict[str, List[str]] = {\n",
        "    \"diabetes\": [\"diabetes\", \"t2d\", \"t1d\", \"blood sugar\", \"insulin\", \"glucose\"],\n",
        "    \"obesity\": [\"obesity\", \"overweight\", \"weight loss\"],\n",
        "    \"hypertension\": [\"hypertension\", \"high blood pressure\"],\n",
        "    \"cardiovascular\": [\"cardiovascular\", \"heart failure\", \"coronary\", \"myocardial\", \"angina\", \"cad\"],\n",
        "    \"ckd\": [\"kidney disease\", \"renal\", \"ckd\"],\n",
        "    \"alzheimer\": [\"alzheimer\", \"dementia\", \"memory loss\"],\n",
        "    \"parkinson\": [\"parkinson\"],\n",
        "    \"asthma\": [\"asthma\", \"wheezing\", \"inhaler\"],\n",
        "    \"copd\": [\"copd\", \"chronic obstructive\", \"emphysema\"],\n",
        "    \"breast_cancer\": [\"breast cancer\", \"her2\"],\n",
        "    \"lung_cancer\": [\"lung cancer\", \"nsclc\"],\n",
        "    \"prostate_cancer\": [\"prostate cancer\"],\n",
        "    \"stroke\": [\"stroke\", \"cva\", \"cerebrovascular\"],\n",
        "    \"rheumatoid_arthritis\": [\"rheumatoid arthritis\", \"ra\"],\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Qdrant Retrieval Agent\n",
        "# ============================================================\n",
        "\n",
        "class QdrantRetrievalAgent:\n",
        "    \"\"\"Vector similarity + disease-aware scoring against Qdrant database\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        qdrant_client: QdrantClient,\n",
        "        embed_model,\n",
        "        collection_name: str = \"clinical_trials\",\n",
        "    ):\n",
        "        self.client = qdrant_client\n",
        "        self.embed_model = embed_model\n",
        "        self.collection_name = collection_name\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Detect disease from the query ‚Üí soft filter\n",
        "    # --------------------------------------------------------\n",
        "    def detect_disease(self, query: str) -> Optional[Dict[str, str]]:\n",
        "        q = query.lower()\n",
        "        for key, syns in DISEASE_SYNONYMS.items():\n",
        "            if any(s in q for s in syns):\n",
        "                return {\"key\": key, \"label\": CONDITION_CONFIG.get(key, key)}\n",
        "        return None\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Score disease match from payload (\"disease\" + \"conditions\")\n",
        "    # --------------------------------------------------------\n",
        "    def disease_match_score(self, disease_hint, payload):\n",
        "        if not disease_hint:\n",
        "            return 0.0\n",
        "\n",
        "        trial_disease = (payload.get(\"disease\") or \"\").lower()\n",
        "        trial_conds = (payload.get(\"conditions\") or \"\").lower()\n",
        "\n",
        "        key = disease_hint[\"key\"]\n",
        "        label = disease_hint[\"label\"].lower()\n",
        "\n",
        "        # Strong match: exact disease field match\n",
        "        if label in trial_disease:\n",
        "            return 1.0\n",
        "\n",
        "        # Secondary match: synonyms appear\n",
        "        for s in DISEASE_SYNONYMS.get(key, []):\n",
        "            if s in trial_conds:\n",
        "                return 0.6\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Main retrieve function\n",
        "    # --------------------------------------------------------\n",
        "    def retrieve(self, parsed: Dict[str, Any], top_k=5, candidate_k=30):\n",
        "        query = parsed.get(\"query\") or parsed.get(\"user_question\") or \"\"\n",
        "        query = query.strip()\n",
        "\n",
        "        if not query:\n",
        "            empty = {\"query\": \"\", \"trials\": [], \"avg_confidence\": 0.0}\n",
        "            return empty, log_provenance_step(\"Qdrant\", parsed, empty)\n",
        "\n",
        "        disease_hint = self.detect_disease(query)\n",
        "        q_vec = self.embed_model.encode([query])[0]\n",
        "\n",
        "        result = self.client.query_points(\n",
        "            collection_name=self.collection_name,\n",
        "            query=q_vec.tolist(),\n",
        "            limit=candidate_k,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        trials = []\n",
        "        confidences = []\n",
        "\n",
        "        for point in result.points:\n",
        "            pl = point.payload or {}\n",
        "\n",
        "            sim = float(point.score)\n",
        "            confidence = convert_similarity_to_confidence(sim)\n",
        "            confidences.append(confidence)\n",
        "\n",
        "            # Disease scoring\n",
        "            disease_w = self.disease_match_score(disease_hint, pl)\n",
        "\n",
        "            # Simple status weighting\n",
        "            status = str(pl.get(\"status\", \"Unknown\")).title()\n",
        "            status_w = 1.0 if status == \"Completed\" else 0.8\n",
        "\n",
        "            # Final relevance scoring\n",
        "            relevance = (\n",
        "                0.65 * sim +      # Semantic similarity\n",
        "                0.25 * disease_w +  # Correct disease\n",
        "                0.10 * status_w   # Study phase quality indicator\n",
        "            )\n",
        "\n",
        "            trials.append({\n",
        "                \"nct_id\": pl.get(\"nct_id\"),\n",
        "                \"title\": pl.get(\"title\"),\n",
        "                \"text\": pl.get(\"text\"),\n",
        "                \"status\": status,\n",
        "                \"confidence\": confidence,\n",
        "                \"relevance\": relevance,\n",
        "            })\n",
        "\n",
        "        # Sort + rank\n",
        "        trials.sort(key=lambda x: x[\"relevance\"], reverse=True)\n",
        "        top = trials[:top_k]\n",
        "\n",
        "        for i, t in enumerate(top, 1):\n",
        "            t[\"rank\"] = i\n",
        "\n",
        "        avg_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0\n",
        "\n",
        "        out = {\n",
        "            \"query\": query,\n",
        "            \"disease_hint\": disease_hint,\n",
        "            \"trials\": top,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "        }\n",
        "\n",
        "        log = log_provenance_step(\"QdrantRetrievalAgent\", parsed, out)\n",
        "        return out, log\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from retrieval_agent_qdrant import QdrantRetrievalAgent\n",
        "print(\"Retrieval agent imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFwNA4z5Dz0",
        "outputId": "ce864062-2df5-4bb6-edaf-699cde4d8317"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval agent imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = qdrant_client.scroll(collection_name=\"clinical_trials\", limit=1, with_payload=True)\n",
        "print(sample[0][0].payload)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeHSzUrk5Hrr",
        "outputId": "a5a110b5-8256-4443-bb75-b97baa4f7e39"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nct_id': 'NCT02437084', 'title': 'Relationship Between Insulin Resistance and Statin Induced Type 2 Diabetes, and Integrative Personal Omics Profiling', 'text': 'Title: Relationship Between Insulin Resistance and Statin Induced Type 2 Diabetes, and Integrative Personal Omics Profiling\\nSummary: Background:\\n\\nThere is general agreement that statin-treatment of patients to lower plasma cholesterol levels can increase the incidence of type 2 diabetes mellitus (T2D) in some individuals1-5. The physiologic mechanism for the increased risk for T2D from statin treatment is unknown but could result from effects on insulin sensitivity or insulin secretion. This study will evaluate how the medication atorvastatin (trade name Lipitor) works in non-diabetic individuals in regards to its effect on insulin sensitivity and insulin secretion to help further understand the possible cause of the increased occurrence of T2D in people who are at risk for T2D. This research study will also examine what metabolic characteristics and variables (for example insulin resistance, high triglycerides, or both) will identify those people at highest risk of statin-induced T2D.\\n\\nThe goals of this study are to:\\n\\n1. determine the effect of high-intensity atorvastatin (40 mg/day) for \\\\~ 10 weeks on insulin sensitivity and insulin secretion (defined with gold standard methods) (PRIMARY OUTCOMES) as well as other glycemic traits (SECONDARY OUTCOMES);\\n2. compare a number of cardio-metabolic characteristics (e.g. weight, lipids) before, during, and after administration of atorvastatin;\\n3. determine if significant deterioration of insulin action and/or secretion following statin treatment will be confined to those with baseline insulin resistance (PRE-SPECIFIED SUBGROUP ANALYSES);\\n4. perform Personal Omics Profiling (iPOP) 6,7 before and after taking atorvastatin to examine treatment-associated changes in all baseline variables and to analyze not only previously-known drug efficacy but also untargeted drug efficacy (EXPLORATORY ANALYSES).\\n\\nGeneral approach:\\n\\nThis will be an open-label study to evaluate the diabetogenic effect of atorvastatin (40 mg/day for 10 weeks) on both insulin action and insulin secretion in nondiabetic individuals. To ensure we recruit individuals across a broad range of insulin sensitivity, we will target recruitment to enrich for those with combined increases in LDL-C and TG concentrations (see SIGNIFICANCE and RATIONALE). The experimental population will consist of \\\\~75 apparently healthy, non-diabetic volunteers eligible for statin therapy but without pre-existing atherosclerotic cardiovascular disease. Following baseline assessments of co-primary outcome measures: insulin sensitivity (by insulin suppression test, IST) and insulin secretion (by graded glucose infusion test, GGIT), participants will be placed on a weight maintenance diet and treated with 40 mg/day of atorvastatin. All baseline measurements will be repeated \\\\~10 weeks later with iPOP8 measurements done at baseline, at weeks 2, 4, and 10 on atorvastatin, and at weeks 4 and 8 off atorvastatin.', 'status': 'Completed', 'conditions': 'Hyperlipidemia, Insulin Resistance, Type 2 Diabetes', 'disease': 'diabetes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXknHDeuKZMY"
      },
      "source": [
        "Update Main Bot Code to Use Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pxoihznCTRf",
        "outputId": "85ab9e22-289d-4798-ada5-5d969e20a022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_bot_qdrant.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_bot_qdrant.py\n",
        "\n",
        "\"\"\"\n",
        "Updated HealthcareBot using Qdrant instead of FAISS\n",
        "- Uses QdrantRetrievalAgent for vector search\n",
        "- Shows rich greeting (G2) ONLY when no clear supported disease is detected\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import requests\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# Import utilities\n",
        "from utils_qdrant import (\n",
        "    load_qdrant_and_model,\n",
        "    log_provenance_step,\n",
        "    generate_reproducibility_hash,\n",
        ")\n",
        "\n",
        "# Qdrant-based retrieval agent\n",
        "from retrieval_agent_qdrant import QdrantRetrievalAgent\n",
        "\n",
        "# Optional (not used directly here, kept for future reranking)\n",
        "CrossEncoder = None\n",
        "try:\n",
        "    from sentence_transformers import CrossEncoder  # noqa: F401\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PARSER\n",
        "# ============================================================\n",
        "class SymptomParser:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"\n",
        "        Enhanced parser for clinical trial search queries.\n",
        "        Decides:\n",
        "        - Are they searching for trials or just asking a question?\n",
        "        - Which disease is implied (if any)?\n",
        "        \"\"\"\n",
        "        prompt = (\n",
        "            \"You are a clinical trial search classifier for medical research.\\n\"\n",
        "            \"You support conditions including: diabetes, obesity, hypertension, \"\n",
        "            \"cardiovascular disease, chronic kidney disease (CKD), Alzheimer's disease, \"\n",
        "            \"Parkinson's disease, asthma, COPD, breast cancer, lung cancer, \"\n",
        "            \"prostate cancer, stroke, and rheumatoid arthritis.\\n\\n\"\n",
        "            f\"User Input: \\\"{text}\\\"\\n\\n\"\n",
        "            \"Your tasks:\\n\"\n",
        "            \"1) Decide if the user is searching for clinical trials or just asking a general question.\\n\"\n",
        "            \"2) Detect which disease(s) they are talking about.\\n\"\n",
        "            \"3) Detect if the query is not about health or clinical trials (off_topic).\\n\\n\"\n",
        "            \"Classification Rules:\\n\"\n",
        "            \"- If the query mentions or implies trials, studies, research, clinical experiments, etc. ‚Üí intent='trial_search'\\n\"\n",
        "            \"- If the user is mainly describing themselves (age, diagnosis, comorbidities, meds) ‚Üí intent='profile_info'\\n\"\n",
        "            \"- If they ask 'what is X', 'how does Y work', etc. without asking about trials ‚Üí intent='general_question'\\n\"\n",
        "            \"- Simple greetings (hi, hello, hey) ‚Üí intent='greeting'\\n\"\n",
        "            \"- If clearly not about health or clinical research ‚Üí intent='off_topic', is_disease_related=false\\n\\n\"\n",
        "            \"You must detect disease_focus whenever possible.\\n\\n\"\n",
        "            \"Return ONLY valid JSON with this exact format:\\n\"\n",
        "            \"{\\n\"\n",
        "            \"  \\\"intent\\\": \\\"trial_search\\\" | \\\"profile_info\\\" | \\\"general_question\\\" | \\\"greeting\\\" | \\\"off_topic\\\",\\n\"\n",
        "            \"  \\\"query_type\\\": \\\"trial_query\\\" | \\\"profile_statement\\\" | \\\"knowledge_seeking\\\" | \\\"greeting\\\",\\n\"\n",
        "            \"  \\\"search_keywords\\\": [\\\"keyword1\\\", \\\"keyword2\\\"],\\n\"\n",
        "            \"  \\\"is_disease_related\\\": true or false,\\n\"\n",
        "            \"  \\\"disease_focus\\\": [\\\"diabetes\\\", \\\"obesity\\\", \\\"hypertension\\\", \\\"cardiovascular\\\", \\\"ckd\\\",\\n\"\n",
        "            \"                     \\\"alzheimer\\\", \\\"parkinson\\\", \\\"asthma\\\", \\\"copd\\\",\\n\"\n",
        "            \"                     \\\"breast_cancer\\\", \\\"lung_cancer\\\", \\\"prostate_cancer\\\",\\n\"\n",
        "            \"                     \\\"stroke\\\", \\\"rheumatoid_arthritis\\\"],\\n\"\n",
        "            \"  \\\"user_question\\\": \\\"the question in plain English\\\",\\n\"\n",
        "            \"  \\\"trial_interest\\\": \\\"what type of trial they want (diet, medication, technology, surgery, etc.)\\\"\\n\"\n",
        "            \"}\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            raw = (res.text or \"\").strip()\n",
        "            match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "            if match:\n",
        "                parsed = json.loads(match.group(0))\n",
        "            else:\n",
        "                parsed = json.loads(raw)\n",
        "        except Exception:\n",
        "            # Fallback: simple heuristic if model fails\n",
        "            text_lower = text.lower()\n",
        "            disease_focus = []\n",
        "\n",
        "            def has_any(words):\n",
        "                return any(w in text_lower for w in words)\n",
        "\n",
        "            if has_any([\"diabetes\", \"insulin\", \"glucose\", \"hba1c\", \"metformin\", \"glp-1\", \"sglt2\"]):\n",
        "                disease_focus.append(\"diabetes\")\n",
        "            if has_any([\"obesity\", \"overweight\", \"weight loss\"]):\n",
        "                disease_focus.append(\"obesity\")\n",
        "            if has_any([\"hypertension\", \"high blood pressure\"]):\n",
        "                disease_focus.append(\"hypertension\")\n",
        "            if has_any([\"heart failure\", \"cardiovascular\", \"angina\", \"coronary\", \"myocardial\", \"stroke\"]):\n",
        "                disease_focus.append(\"cardiovascular\")\n",
        "            if has_any([\"chronic kidney\", \"ckd\", \"renal failure\", \"kidney disease\"]):\n",
        "                disease_focus.append(\"ckd\")\n",
        "            if has_any([\"alzheimer\", \"alzheimers\", \"dementia\", \"memory loss\", \"cognitive decline\"]):\n",
        "                disease_focus.append(\"alzheimer\")\n",
        "            if has_any([\"parkinson\", \"parkinson's\"]):\n",
        "                disease_focus.append(\"parkinson\")\n",
        "            if has_any([\"asthma\", \"wheezing\"]):\n",
        "                disease_focus.append(\"asthma\")\n",
        "            if has_any([\"copd\", \"chronic obstructive\", \"emphysema\", \"chronic bronchitis\"]):\n",
        "                disease_focus.append(\"copd\")\n",
        "            if has_any([\"breast cancer\"]):\n",
        "                disease_focus.append(\"breast_cancer\")\n",
        "            if has_any([\"lung cancer\", \"nsclc\", \"small cell lung cancer\"]):\n",
        "                disease_focus.append(\"lung_cancer\")\n",
        "            if has_any([\"prostate cancer\"]):\n",
        "                disease_focus.append(\"prostate_cancer\")\n",
        "            if has_any([\"stroke\", \"cerebrovascular accident\", \"cva\"]):\n",
        "                disease_focus.append(\"stroke\")\n",
        "            if has_any([\"rheumatoid arthritis\", \"ra\", \"inflammatory arthritis\"]):\n",
        "                disease_focus.append(\"rheumatoid_arthritis\")\n",
        "\n",
        "            if has_any([\"trial\", \"study\", \"studies\", \"research\", \"clinical\"]):\n",
        "                intent = \"trial_search\"\n",
        "                query_type = \"trial_query\"\n",
        "            elif has_any([\"hi\", \"hello\", \"hey\"]):\n",
        "                intent = \"greeting\"\n",
        "                query_type = \"greeting\"\n",
        "            else:\n",
        "                intent = \"general_question\"\n",
        "                query_type = \"knowledge_seeking\"\n",
        "\n",
        "            parsed = {\n",
        "                \"intent\": intent,\n",
        "                \"query_type\": query_type,\n",
        "                \"search_keywords\": [text] if intent == \"trial_search\" else [],\n",
        "                \"is_disease_related\": bool(disease_focus),\n",
        "                \"disease_focus\": disease_focus,\n",
        "                \"user_question\": text,\n",
        "                \"trial_interest\": \"general\",\n",
        "            }\n",
        "\n",
        "        # --- Heuristic correction layer on top of model output ---\n",
        "        text_lower = text.lower()\n",
        "        diseases = set(parsed.get(\"disease_focus\") or [])\n",
        "\n",
        "        def maybe_add(words, label):\n",
        "            if any(w in text_lower for w in words):\n",
        "                diseases.add(label)\n",
        "\n",
        "        maybe_add([\"diabetes\", \"insulin\", \"glucose\", \"hba1c\", \"metformin\", \"glp-1\", \"sglt2\"], \"diabetes\")\n",
        "        maybe_add([\"obesity\", \"overweight\", \"weight loss\"], \"obesity\")\n",
        "        maybe_add([\"hypertension\", \"high blood pressure\"], \"hypertension\")\n",
        "        maybe_add([\"heart failure\", \"cardiovascular\", \"angina\", \"coronary\", \"myocardial\", \"stroke\"], \"cardiovascular\")\n",
        "        maybe_add([\"chronic kidney\", \"ckd\", \"renal failure\", \"kidney disease\"], \"ckd\")\n",
        "        maybe_add([\"alzheimer\", \"alzheimers\", \"dementia\", \"memory loss\", \"cognitive decline\"], \"alzheimer\")\n",
        "        maybe_add([\"parkinson\", \"parkinson's\"], \"parkinson\")\n",
        "        maybe_add([\"asthma\", \"wheezing\", \"inhaler\"], \"asthma\")\n",
        "        maybe_add([\"copd\", \"chronic obstructive\", \"emphysema\", \"chronic bronchitis\"], \"copd\")\n",
        "        maybe_add([\"breast cancer\"], \"breast_cancer\")\n",
        "        maybe_add([\"lung cancer\", \"nsclc\", \"small cell lung cancer\"], \"lung_cancer\")\n",
        "        maybe_add([\"prostate cancer\"], \"prostate_cancer\")\n",
        "        maybe_add([\"stroke\", \"cerebrovascular accident\", \"cva\"], \"stroke\")\n",
        "        maybe_add([\"rheumatoid arthritis\", \"inflammatory arthritis\", \"ra\"], \"rheumatoid_arthritis\")\n",
        "\n",
        "        parsed[\"disease_focus\"] = list(diseases)\n",
        "\n",
        "        # Force trial_search if obvious trial keywords\n",
        "        trial_keywords = [\n",
        "            \"trial\", \"study\", \"studies\", \"research\",\n",
        "            \"clinical\", \"show me\", \"are there\", \"what trials\",\n",
        "        ]\n",
        "        if any(kw in text_lower for kw in trial_keywords):\n",
        "            parsed[\"intent\"] = \"trial_search\"\n",
        "            parsed[\"query_type\"] = \"trial_query\"\n",
        "\n",
        "        # If we detected diseases, ensure is_disease_related = True\n",
        "        if diseases and parsed.get(\"intent\") != \"off_topic\":\n",
        "            parsed[\"is_disease_related\"] = True\n",
        "        elif \"is_disease_related\" not in parsed:\n",
        "            parsed[\"is_disease_related\"] = bool(diseases)\n",
        "\n",
        "        log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "        return parsed, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PROFILE AGENT\n",
        "# ============================================================\n",
        "class ProfileAgent:\n",
        "    def __init__(self, initial_profile: Dict[str, Any] = None):\n",
        "        if initial_profile is None:\n",
        "            initial_profile = {\n",
        "                \"user_id\": \"Patient\",\n",
        "                \"conditions\": [],\n",
        "                \"extracted_conditions\": [],\n",
        "                \"history\": [],\n",
        "            }\n",
        "        self.profile = initial_profile\n",
        "\n",
        "    def update_profile(self, turn_data: Dict[str, Any]):\n",
        "        self.profile.setdefault(\"history\", []).append(turn_data)\n",
        "        self.profile.setdefault(\"extracted_conditions\", [])\n",
        "\n",
        "        parsed = turn_data.get(\"parsed\", {})\n",
        "        diseases = parsed.get(\"disease_focus\") or []\n",
        "        if diseases:\n",
        "            current = set(self.profile[\"extracted_conditions\"])\n",
        "            for d in diseases:\n",
        "                current.add(d)\n",
        "            self.profile[\"extracted_conditions\"] = list(current)\n",
        "\n",
        "        snapshot = {\n",
        "            \"user_id\": self.profile.get(\"user_id\", \"Patient\"),\n",
        "            \"known_conditions\": self.profile.get(\"extracted_conditions\", []),\n",
        "            \"num_turns\": len(self.profile[\"history\"]),\n",
        "        }\n",
        "        log = log_provenance_step(\"ProfileAgent\", turn_data, {\"profile_snapshot\": snapshot})\n",
        "        return log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EVIDENCE-WEIGHTED SCORER (still used downstream)\n",
        "# ============================================================\n",
        "class EvidenceWeightedScorer:\n",
        "    def __init__(self):\n",
        "        self.status_weights = {\n",
        "            \"Completed\": 1.0,\n",
        "            \"Active, Not Recruiting\": 0.9,\n",
        "            \"Recruiting\": 0.85,\n",
        "            \"Enrolling By Invitation\": 0.8,\n",
        "            \"Not Yet Recruiting\": 0.6,\n",
        "            \"Terminated\": 0.4,\n",
        "            \"Withdrawn\": 0.3,\n",
        "            \"Suspended\": 0.3,\n",
        "            \"Unknown Status\": 0.5,\n",
        "        }\n",
        "\n",
        "        self.design_keywords = {\n",
        "            \"randomized controlled\": 1.0,\n",
        "            \"double-blind\": 0.95,\n",
        "            \"randomized\": 0.9,\n",
        "            \"controlled\": 0.85,\n",
        "            \"interventional\": 0.8,\n",
        "            \"prospective\": 0.75,\n",
        "            \"observational\": 0.6,\n",
        "            \"retrospective\": 0.5,\n",
        "        }\n",
        "\n",
        "    def calculate_weighted_score(\n",
        "        self,\n",
        "        trial: Dict[str, Any],\n",
        "        base_confidence: float,\n",
        "        query: str,\n",
        "    ) -> Dict[str, Any]:\n",
        "        match_score = base_confidence * 0.35\n",
        "\n",
        "        status = str(trial.get(\"status\", \"Unknown Status\")).strip().title()\n",
        "        status_score = self.status_weights.get(status, 0.5) * 0.25\n",
        "\n",
        "        design_score = self._calculate_design_quality(trial) * 0.20\n",
        "        keyword_score = self._calculate_keyword_density(trial, query) * 0.10\n",
        "        completeness_score = self._calculate_completeness(trial) * 0.10\n",
        "\n",
        "        weighted_score = (\n",
        "            match_score +\n",
        "            status_score +\n",
        "            design_score +\n",
        "            keyword_score +\n",
        "            completeness_score\n",
        "        )\n",
        "\n",
        "        breakdown = {\n",
        "            \"base_confidence\": base_confidence,\n",
        "            \"weighted_score\": weighted_score,\n",
        "            \"factors\": {\n",
        "                \"semantic_match\": match_score,\n",
        "                \"trial_status\": status_score,\n",
        "                \"study_design\": design_score,\n",
        "                \"keyword_density\": keyword_score,\n",
        "                \"completeness\": completeness_score,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"weighted_score\": min(weighted_score, 1.0),\n",
        "            \"breakdown\": breakdown,\n",
        "        }\n",
        "\n",
        "    def _calculate_design_quality(self, trial: Dict[str, Any]) -> float:\n",
        "        text = f\"{trial.get('title', '')} {trial.get('text', '')}\".lower()\n",
        "        max_score = 0.0\n",
        "        for keyword, weight in self.design_keywords.items():\n",
        "            if keyword in text:\n",
        "                max_score = max(max_score, weight)\n",
        "        return max_score if max_score > 0 else 0.6\n",
        "\n",
        "    def _calculate_keyword_density(self, trial: Dict[str, Any], query: str) -> float:\n",
        "        text = f\"{trial.get('title', '')} {trial.get('text', '')}\".lower()\n",
        "        stopwords = {\n",
        "            \"the\", \"a\", \"an\", \"and\", \"or\", \"for\", \"with\", \"in\", \"on\", \"at\", \"to\",\n",
        "            \"of\", \"is\", \"are\", \"what\", \"trials\", \"trial\", \"study\", \"studies\", \"clinical\",\n",
        "        }\n",
        "        query_terms = [\n",
        "            term for term in query.lower().split()\n",
        "            if term not in stopwords and len(term) > 2\n",
        "        ]\n",
        "        if not query_terms:\n",
        "            return 0.5\n",
        "        matches = sum(1 for term in query_terms if term in text)\n",
        "        density = matches / len(query_terms)\n",
        "        return min(density, 1.0)\n",
        "\n",
        "    def _calculate_completeness(self, trial: Dict[str, Any]) -> float:\n",
        "        text = trial.get(\"text\", \"\") or \"\"\n",
        "        title = trial.get(\"title\", \"\") or \"\"\n",
        "        score = 0.0\n",
        "        if len(title) > 10:\n",
        "            score += 0.3\n",
        "        if len(text) > 200:\n",
        "            score += 0.7\n",
        "        return min(score, 1.0)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PubMed Helper (NCT ‚Üí PubMed abstract)\n",
        "# ============================================================\n",
        "def fetch_pubmed_abstract_for_nct(nct_id: str):\n",
        "    try:\n",
        "        esearch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "        params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"term\": f\"{nct_id}[si]\",\n",
        "            \"retmode\": \"json\",\n",
        "            \"retmax\": 1,\n",
        "        }\n",
        "        r = requests.get(esearch_url, params=params, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        idlist = data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "        if not idlist:\n",
        "            return None\n",
        "\n",
        "        pmid = idlist[0]\n",
        "\n",
        "        efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "        params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"id\": pmid,\n",
        "            \"rettype\": \"abstract\",\n",
        "            \"retmode\": \"text\",\n",
        "        }\n",
        "        r2 = requests.get(efetch_url, params=params, timeout=10)\n",
        "        r2.raise_for_status()\n",
        "        abstract_text = r2.text.strip()\n",
        "        if not abstract_text:\n",
        "            return None\n",
        "\n",
        "        return {\"pmid\": pmid, \"abstract\": abstract_text}\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DIAGNOSIS / ADVISOR\n",
        "# ============================================================\n",
        "class DiagnosisAdvisor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def _handle_general_question(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        user_question = parsed.get(\"user_question\") or \" \".join(parsed.get(\"symptoms\", []))\n",
        "\n",
        "        evidence_parts = []\n",
        "        for t in trials[:3]:\n",
        "            evidence_parts.append(f\"Trial {t['nct_id']}: {t['text'][:400]}\")\n",
        "        evidence = \"\\n\\n\".join(evidence_parts) if evidence_parts else \"No specific trials available.\"\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a medical research educator. Answer the user's question clearly using reliable medical knowledge.\\n\"\n",
        "            \"The clinical trial evidence below provides real-world context - mention it if helpful.\\n\\n\"\n",
        "            f\"USER'S QUESTION: {user_question}\\n\\n\"\n",
        "            \"CLINICAL TRIAL CONTEXT (for reference only):\\n\"\n",
        "            f\"{evidence}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"- Answer the question directly in 3‚Äì5 sentences.\\n\"\n",
        "            \"- Be specific and educational.\\n\"\n",
        "            \"- Do NOT give diagnoses or treatment instructions.\\n\"\n",
        "            \"- End with: 'For personalized advice, please consult your healthcare provider.'\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            text = (res.text or \"\").strip()\n",
        "            if not text or len(text) < 50:\n",
        "                text = (\n",
        "                    \"I don't have enough information to answer this question accurately. \"\n",
        "                    \"For personalized guidance, please consult your healthcare provider.\"\n",
        "                )\n",
        "            return text\n",
        "        except Exception:\n",
        "            return (\n",
        "                \"I'm unable to generate a detailed answer right now. \"\n",
        "                \"For personalized guidance, please consult your healthcare provider.\"\n",
        "            )\n",
        "\n",
        "    def _handle_symptom_query(\n",
        "        self,\n",
        "        parsed: Dict[str, Any],\n",
        "        retrieved: Dict[str, Any],\n",
        "        profile: Dict[str, Any],\n",
        "    ):\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        if not trials:\n",
        "            return \"No relevant trials were found. Please try refining your query.\"\n",
        "\n",
        "        formatted_trials = []\n",
        "        for t in trials[:5]:\n",
        "            title = t.get(\"title\", \"\") or t[\"text\"].split(\"\\n\")[0].replace(\"Title: \", \"\")\n",
        "            status = t.get(\"status\", \"Unknown\")\n",
        "            weighted_score = t.get(\"weighted_score\", t.get(\"relevance\", 0.0))\n",
        "\n",
        "            raw_text = t.get(\"text\", \"\")\n",
        "            brief_summary = raw_text.split(\"Summary:\", 1)[-1].strip() if \"Summary:\" in raw_text else raw_text\n",
        "\n",
        "            if brief_summary:\n",
        "                prompt = (\n",
        "                    \"Rewrite the following clinical trial description as a short, clear paragraph \"\n",
        "                    \"about what the study is testing:\\n\\n\"\n",
        "                    f\"{brief_summary}\\n\\n\"\n",
        "                    \"Guidelines:\\n\"\n",
        "                    \"- Use 2‚Äì4 sentences.\\n\"\n",
        "                    \"- Plain English, minimal jargon.\\n\"\n",
        "                    \"- Include the purpose and the main type of participant.\\n\"\n",
        "                )\n",
        "                try:\n",
        "                    res = self.model.generate_content(prompt)\n",
        "                    brief_summary = res.text.strip() if res.text else brief_summary\n",
        "                except Exception:\n",
        "                    if len(brief_summary) > 600:\n",
        "                        brief_summary = brief_summary[:600] + \"...\"\n",
        "            else:\n",
        "                brief_summary = \"No summary available.\"\n",
        "\n",
        "            pubmed_block = \"\"\n",
        "            pub = fetch_pubmed_abstract_for_nct(t[\"nct_id\"])\n",
        "            if pub:\n",
        "                abs_text = pub[\"abstract\"]\n",
        "                max_len = 2000\n",
        "                if len(abs_text) > max_len:\n",
        "                    abs_text = abs_text[:max_len] + \"...\"\n",
        "                pubmed_block = (\n",
        "                    f\"  PubMed abstract (PMID {pub['pmid']}):\\n\"\n",
        "                    f\"  {abs_text}\\n\\n\"\n",
        "                    f\"  PubMed link: https://pubmed.ncbi.nlm.nih.gov/{pub['pmid']}/\\n\\n\"\n",
        "                )\n",
        "\n",
        "            formatted_trials.append(\n",
        "                f\"**{t['nct_id']}** (Relevance: {weighted_score:.0%})\\n\"\n",
        "                f\"‚Ä¢ {title}\\n\"\n",
        "                f\"  Status: {status}\\n\\n\"\n",
        "                f\"  {brief_summary}\\n\\n\"\n",
        "                f\"{pubmed_block}\"\n",
        "            )\n",
        "\n",
        "        trials_text = \"\\n\\n\".join(formatted_trials)\n",
        "        num_trials = len(formatted_trials)\n",
        "\n",
        "        response = (\n",
        "            f\"I found {num_trials} clinical trial{'s' if num_trials != 1 else ''} relevant to your request:\\n\\n\"\n",
        "            f\"{trials_text}\\n\\n\"\n",
        "            \"Summary: These trials explore potential treatments or management strategies for the condition you asked about. \"\n",
        "            \"More details are available using the listed NCT IDs.\\n\\n\"\n",
        "            \"To learn more or consider participation, visit clinicaltrials.gov and search by NCT ID. \"\n",
        "            \"Always discuss clinical trial options with your healthcare provider.\"\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    def advise(self, parsed: Dict[str, Any], retrieved: Dict[str, Any], profile: Dict[str, Any]):\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "        query_type = parsed.get(\"query_type\", \"trial_query\")\n",
        "        is_disease_related = parsed.get(\"is_disease_related\", True)\n",
        "\n",
        "        draft = {\n",
        "            \"recommendation\": \"\",\n",
        "            \"avg_confidence\": avg_conf,\n",
        "            \"query_type\": query_type,\n",
        "        }\n",
        "\n",
        "        if not is_disease_related:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"I‚Äôm specialized in clinical trials for medical conditions such as:\\n\"\n",
        "                \"- Diabetes\\n\"\n",
        "                \"- Obesity\\n\"\n",
        "                \"- Hypertension & cardiovascular disease\\n\"\n",
        "                \"- Chronic kidney disease (CKD)\\n\"\n",
        "                \"- Alzheimer‚Äôs & Parkinson‚Äôs disease\\n\"\n",
        "                \"- Asthma & COPD\\n\"\n",
        "                \"- Breast, lung, and prostate cancer\\n\"\n",
        "                \"- Stroke\\n\"\n",
        "                \"- Rheumatoid arthritis\\n\\n\"\n",
        "                \"Your question does not appear to be about a health condition or clinical research. \"\n",
        "                \"If you‚Äôd like, you can ask me about trials for one of these conditions.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                parsed,\n",
        "                draft,\n",
        "                {\"veto\": True, \"reason\": \"off_topic\"},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        if not trials or avg_conf < 0.05:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"Based on the trials I retrieved, I don‚Äôt have strong enough evidence to answer this question directly. \"\n",
        "                \"Please consult your healthcare provider for personalized advice.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                parsed,\n",
        "                draft,\n",
        "                {\"veto\": True, \"reason\": \"low_confidence\"},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        if query_type == \"knowledge_seeking\":\n",
        "            draft[\"recommendation\"] = self._handle_general_question(parsed, retrieved)\n",
        "        else:\n",
        "            draft[\"recommendation\"] = self._handle_symptom_query(parsed, retrieved, profile)\n",
        "\n",
        "        draft[\"confidence_veto\"] = False\n",
        "        log = log_provenance_step(\"DiagnosisAdvisor\", parsed, draft)\n",
        "        return draft, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SAFETY FILTER\n",
        "# ============================================================\n",
        "class ActiveSafetyFilter:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.safety_cfg = {\n",
        "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        }\n",
        "\n",
        "    def verify(self, advice_text: str, trials: List[Dict[str, Any]]):\n",
        "        # Skip safety check for straightforward trial listings\n",
        "        if any(marker in advice_text for marker in [\"NCT\", \"clinical trial\", \"clinicaltrials.gov\"]):\n",
        "            log = log_provenance_step(\n",
        "                \"ActiveSafetyFilter\",\n",
        "                {\"advice\": advice_text},\n",
        "                {\"final_text\": advice_text, \"status\": \"Pass (Trial Listing)\"},\n",
        "            )\n",
        "            return advice_text, \"Pass (Trial Listing)\", log\n",
        "\n",
        "        evidence_text = \"\\n\".join(t[\"text\"][:500] for t in trials[:3])\n",
        "\n",
        "        audit_prompt = (\n",
        "            \"You are a Medical Safety Officer reviewing AI-generated advice.\\n\\n\"\n",
        "            \"ADVICE:\\n\"\n",
        "            f\"{advice_text}\\n\\n\"\n",
        "            \"EVIDENCE FROM CLINICAL TRIALS (for context):\\n\"\n",
        "            f\"{evidence_text}\\n\\n\"\n",
        "            \"Check for safety issues:\\n\"\n",
        "            \"- If the advice suggests starting/stopping/changing medication without a doctor ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it gives a diagnosis ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it makes strong clinical claims not supported by evidence ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it simply lists clinical trials with neutral wording and a recommendation to talk to a doctor ‚Üí SAFE.\\n\\n\"\n",
        "            \"If the advice is acceptable, respond with exactly: SAFE\\n\"\n",
        "            \"If it is not acceptable, respond starting with: CORRECTED: <safer version>\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(audit_prompt, safety_settings=self.safety_cfg)\n",
        "            txt = (res.text or \"\").strip()\n",
        "            if txt.startswith(\"SAFE\") or \"SAFE\" in txt:\n",
        "                final_text = advice_text\n",
        "                status = \"Pass\"\n",
        "            else:\n",
        "                final_text = f\"‚ö†Ô∏è SAFETY REVISION:\\n{txt}\"\n",
        "                status = \"Revised\"\n",
        "        except Exception:\n",
        "            if \"NCT\" in advice_text or \"clinical trial\" in advice_text.lower():\n",
        "                final_text = advice_text\n",
        "                status = \"Pass (API Fallback)\"\n",
        "            else:\n",
        "                final_text = \"‚ö†Ô∏è Safety filter triggered. Please consult a doctor.\"\n",
        "                status = \"Revised (API Error)\"\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"ActiveSafetyFilter\",\n",
        "            {\"advice\": advice_text},\n",
        "            {\"final_text\": final_text, \"status\": status},\n",
        "        )\n",
        "        return final_text, status, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# HEALTHCAREBOT - Updated to use Qdrant + G2 / B3 behavior\n",
        "# ============================================================\n",
        "\n",
        "class HealthcareBot:\n",
        "    def __init__(self, qdrant_client, embed_model, gemini_model, initial_profile=None):\n",
        "        self.parser = SymptomParser(gemini_model)\n",
        "        self.profile_agent = ProfileAgent(initial_profile)\n",
        "        self.evidence_scorer = EvidenceWeightedScorer()\n",
        "\n",
        "        # Qdrant-based retrieval agent\n",
        "        self.retrieval = QdrantRetrievalAgent(\n",
        "            qdrant_client=qdrant_client,\n",
        "            embed_model=embed_model,\n",
        "            collection_name=\"clinical_trials\",\n",
        "        )\n",
        "\n",
        "        self.advisor = DiagnosisAdvisor(gemini_model)\n",
        "        self.safety_filter = ActiveSafetyFilter(gemini_model)\n",
        "        self.conversation_history: List[Dict[str, Any]] = []\n",
        "        self.provenance_log: List[Dict[str, Any]] = []\n",
        "\n",
        "    # -------------- Greeting builder (G2 style) --------------\n",
        "    def _build_greeting(self) -> str:\n",
        "        return (\n",
        "            \"üëã Hi, I‚Äôm your clinical trial assistant.\\n\\n\"\n",
        "            \"I can search **real clinical trials** for:\\n\"\n",
        "            \"- Diabetes (type 1 & type 2)\\n\"\n",
        "            \"- Obesity & weight management\\n\"\n",
        "            \"- Hypertension & cardiovascular disease\\n\"\n",
        "            \"- Chronic kidney disease (CKD)\\n\"\n",
        "            \"- Alzheimer‚Äôs disease\\n\"\n",
        "            \"- Parkinson‚Äôs disease\\n\"\n",
        "            \"- Asthma\\n\"\n",
        "            \"- COPD (chronic obstructive pulmonary disease)\\n\"\n",
        "            \"- Breast, lung, and prostate cancer\\n\"\n",
        "            \"- Stroke\\n\"\n",
        "            \"- Rheumatoid arthritis\\n\\n\"\n",
        "            \"**Try asking:**\\n\"\n",
        "            \"‚Ä¢ \\\"GLP-1 agonist trials for type 2 diabetes\\\"\\n\"\n",
        "            \"‚Ä¢ \\\"Weight loss studies for obesity\\\"\\n\"\n",
        "            \"‚Ä¢ \\\"Breast cancer immunotherapy clinical trials\\\"\\n\"\n",
        "            \"‚Ä¢ \\\"New asthma biologic trials for adults\\\"\\n\"\n",
        "            \"‚Ä¢ \\\"Recent rheumatoid arthritis treatment studies\\\"\\n\\n\"\n",
        "            \"Tell me your condition and what kind of trial you‚Äôre interested in (medication, diet, devices, etc.), \"\n",
        "            \"and I‚Äôll surface the most relevant studies.\"\n",
        "        )\n",
        "\n",
        "    def chat(self, user_input: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process user input through the pipeline.\"\"\"\n",
        "\n",
        "        # 1) Parse intent\n",
        "        parsed, parse_log = self.parser.parse(user_input)\n",
        "        self.provenance_log.append(parse_log)\n",
        "\n",
        "        intent = parsed.get(\"intent\", \"\")\n",
        "        disease_focus = parsed.get(\"disease_focus\") or []\n",
        "        is_disease_related = parsed.get(\"is_disease_related\", True)\n",
        "\n",
        "        # 2) G2 + B3 behavior:\n",
        "        #    If clear disease ‚Üí go straight to retrieval.\n",
        "        #    If greeting / off-topic / no disease ‚Üí show greeting instead.\n",
        "        if intent == \"greeting\" or not disease_focus or not is_disease_related:\n",
        "            greeting = self._build_greeting()\n",
        "\n",
        "            full_turn = {\n",
        "                \"query\": user_input,\n",
        "                \"parsed\": parsed,\n",
        "                \"retrieved\": {\"query\": \"\", \"trials\": [], \"avg_confidence\": 0.0},\n",
        "                \"response\": greeting,\n",
        "                \"timestamp\": parse_log[\"timestamp\"],\n",
        "            }\n",
        "            self.conversation_history.append(full_turn)\n",
        "\n",
        "            return {\n",
        "                \"response\": greeting,\n",
        "                \"avg_confidence\": 0.0,\n",
        "                \"num_trials\": 0,\n",
        "                \"provenance\": self.provenance_log[-5:],\n",
        "                \"session_hash\": generate_reproducibility_hash(self.conversation_history),\n",
        "            }\n",
        "\n",
        "        # 3) Update profile for disease-related / trial queries\n",
        "        turn_data = {\"query\": user_input, \"parsed\": parsed}\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_log.append(profile_log)\n",
        "\n",
        "        # 4) Retrieve trials from Qdrant\n",
        "        retrieved, retrieval_log = self.retrieval.retrieve(parsed, top_k=5)\n",
        "        self.provenance_log.append(retrieval_log)\n",
        "\n",
        "        # 5) Generate advisory response\n",
        "        profile_snapshot = {\n",
        "            \"user_id\": self.profile_agent.profile.get(\"user_id\", \"Patient\"),\n",
        "            \"known_conditions\": self.profile_agent.profile.get(\"extracted_conditions\", []),\n",
        "        }\n",
        "\n",
        "        draft, advisor_log = self.advisor.advise(parsed, retrieved, profile_snapshot)\n",
        "        self.provenance_log.append(advisor_log)\n",
        "\n",
        "        # 6) Safety filter\n",
        "        advice_text = draft.get(\"recommendation\", \"\") if isinstance(draft, dict) else str(draft)\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "\n",
        "        final_response, safety_status, safety_log = self.safety_filter.verify(advice_text, trials)\n",
        "        self.provenance_log.append(safety_log)\n",
        "\n",
        "        # 7) Save turn\n",
        "        full_turn = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"retrieved\": retrieved,\n",
        "            \"response\": final_response,\n",
        "            \"timestamp\": parse_log[\"timestamp\"],\n",
        "            \"safety_status\": safety_status,\n",
        "        }\n",
        "        self.conversation_history.append(full_turn)\n",
        "\n",
        "        return {\n",
        "            \"response\": final_response,\n",
        "            \"avg_confidence\": retrieved.get(\"avg_confidence\", 0.0),\n",
        "            \"num_trials\": len(retrieved.get(\"trials\", [])),\n",
        "            \"provenance\": self.provenance_log[-5:],\n",
        "            \"session_hash\": generate_reproducibility_hash(self.conversation_history),\n",
        "            \"safety_status\": safety_status,\n",
        "        }\n",
        "\n",
        "\n",
        "def run_bot(user_input: str, qdrant_client, embed_model, gemini_model) -> Dict[str, Any]:\n",
        "    \"\"\"Convenience wrapper for single queries.\"\"\"\n",
        "    bot = HealthcareBot(qdrant_client, embed_model, gemini_model)\n",
        "    return bot.chat(user_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import google.generativeai as genai\n",
        "from run_bot_qdrant import run_bot, HealthcareBot\n",
        "from utils_qdrant import load_qdrant_and_model\n",
        "\n",
        "# üîë Keys\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Enter Gemini API Key: \")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "QDRANT_API_KEY = getpass.getpass(\"Enter Qdrant API Key: \")\n",
        "QDRANT_URL = \"https://215ec69e-fa22-4f38-bcf3-941e73901a68.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "\n",
        "# üìå Connect to Qdrant + load embedding model\n",
        "qdrant_client, embed_model = load_qdrant_and_model(\n",
        "    qdrant_url=QDRANT_URL,\n",
        "    qdrant_api_key=QDRANT_API_KEY\n",
        ")\n",
        "\n",
        "# üß† Load Gemini\n",
        "gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# ü§ñ Create bot instance\n",
        "bot = HealthcareBot(qdrant_client, embed_model, gemini_model)\n",
        "\n",
        "# -------------------------\n",
        "# Test Query\n",
        "# -------------------------\n",
        "response = bot.chat(\"Are there GLP-1 trials for obesity?\")\n",
        "print(response[\"response\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Du0mL-MA2_29",
        "outputId": "17de970b-40d6-4983-e696-5c967709612d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Gemini API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Enter Qdrant API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚è≥ Connecting to Qdrant...\n",
            "üîó Qdrant Connected ‚Üí 112,987 trials indexed\n",
            "üß† Disease-based retrieval enabled\n",
            "üß¨ Embedding model initialized\n",
            "üìå Supported diseases: ['diabetes', 'obesity', 'hypertension', 'cardiovascular', 'ckd', 'alzheimer', 'parkinson', 'asthma', 'copd', 'breast_cancer', 'lung_cancer', 'prostate_cancer', 'rheumatoid_arthritis']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 404.62ms\n",
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 129.42ms\n",
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 152.17ms\n",
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 152.23ms\n",
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 227.14ms\n",
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 126.41ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I found 5 clinical trials relevant to your request:\n",
            "\n",
            "**NCT07190794** (Relevance: 79%)\n",
            "‚Ä¢ RESCUE: Discontinuation of GLP-1\n",
            "  Status: Recruiting\n",
            "\n",
            "  This study is designed to compare weight loss outcomes and safety of ESG versus lifestyle modification in patients with obesity who discontinued GLP-1 therapy due to intolerance or suboptimal weight loss.\n",
            "\n",
            "\n",
            "\n",
            "**NCT02598791** (Relevance: 79%)\n",
            "‚Ä¢ GIP/GLP-1 Co-Activity in Subjects With Obesity: Lowering of Food Intake\n",
            "  Status: Completed\n",
            "\n",
            "  We aim to delineate the effects of separate and combined infusion of GIP and GLP-1 on food intake, appetite, bone health and fat metabolism in overweight/obese subjects.\n",
            "\n",
            "  PubMed abstract (PMID 30848791):\n",
            "  1. J Clin Endocrinol Metab. 2019 Jul 1;104(7):2953-2960. doi: \n",
            "10.1210/jc.2019-00008.\n",
            "\n",
            "Separate and Combined Effects of GIP and GLP-1 Infusions on Bone Metabolism in \n",
            "Overweight Men Without Diabetes.\n",
            "\n",
            "Bergmann NC(1)(2)(3), Lund A(1)(4), Gasbjerg LS(1)(3)(5), J√∏rgensen NR(6)(7), \n",
            "Jessen L(2), Hartmann B(3)(5), Holst JJ(3)(5), Christensen MB(1)(8)(9), Vilsb√∏ll \n",
            "T(1)(8), Knop FK(1)(5)(8).\n",
            "\n",
            "Author information:\n",
            "(1)Clinical Metabolic Physiology, Steno Diabetes Center Copenhagen, Gentofte \n",
            "Hospital, Hellerup, Denmark.\n",
            "(2)Department of In Vivo Pharmacology, Zealand Pharma A/S, Glostrup, Denmark.\n",
            "(3)Department of Biomedical Sciences, Faculty of Health and Medical Sciences, \n",
            "University of Copenhagen, Copenhagen, Denmark.\n",
            "(4)Department of Medicine, Gentofte Hospital, Hellerup, Denmark.\n",
            "(5)Novo Nordisk Foundation Center for Basic Metabolic Research, Faculty of \n",
            "Health and Medical Sciences, University of Copenhagen, Copenhagen, Denmark.\n",
            "(6)Department of Clinical Chemistry, Rigshospitalet, University of Copenhagen, \n",
            "Glostrup, Denmark.\n",
            "(7)OPEN, Odense University Hospital, Odense, Denmark.\n",
            "(8)Department of Clinical Medicine, Faculty of Health and Medical Sciences, \n",
            "University of Copenhagen, Copenhagen, Denmark.\n",
            "(9)Department of Clinical Pharmacology, Bispebjerg Hospital, Copenhagen, \n",
            "Denmark.\n",
            "\n",
            "CONTEXT: The gut-derived incretin hormones glucose-dependent insulinotropic \n",
            "polypeptide (GIP) and glucagon-like peptide 1 (GLP-1) have been suggested to \n",
            "play a role in bone metabolism. Exogenous administration of GIP inhibits bone \n",
            "resorption, but the effect of GLP-1 is less clear. Furthermore, the combined \n",
            "effect of exogenous GIP and GLP-1 on bone metabolism is unknown.\n",
            "OBJECTIVE: To investigate the effect of separate and combined infusions of the \n",
            "incretin hormones GIP and GLP-1 on bone resorption and formation.\n",
            "DESIGN: Randomized, double-blinded, placebo-controlled, crossover study \n",
            "including five study days.\n",
            "PARTICIPANTS: Seventeen overweight/obese men.\n",
            "INTERVENTIONS: On the first stud...\n",
            "\n",
            "  PubMed link: https://pubmed.ncbi.nlm.nih.gov/30848791/\n",
            "\n",
            "\n",
            "\n",
            "**NCT06034457** (Relevance: 79%)\n",
            "‚Ä¢ Testing the Effectiveness of WW Clinic GLP1\n",
            "  Status: Completed\n",
            "\n",
            "  The study objective is to evaluate the efficacy of the WeightWatchers (WW) GLP-1 behavioral program + WW Clinic (formerly known as Sequence medical weight management program) on weight loss and related outcomes. Participants will be invited to take part and answer surveys at 0, 12 and 24 weeks.\n",
            "\n",
            "  PubMed abstract (PMID 41253738):\n",
            "  1. Obesity (Silver Spring). 2025 Nov 18. doi: 10.1002/oby.70056. Online ahead of \n",
            "print.\n",
            "\n",
            "Effectiveness of Telemedicine Prescribing and a Long-Acting Obesity Medication \n",
            "Behavioral Program: A 24-Week Single-Arm Study.\n",
            "\n",
            "Heinberg LJ(1), Lee AM(2), Foster GD(3), DiVita A(4), Allman M(4), Rotroff D(5), \n",
            "Dargham CB(5), Cardel MI(6)(7)(8).\n",
            "\n",
            "Author information:\n",
            "(1)Department of Psychiatry and Psychology, Cleveland Clinic Lerner College of \n",
            "Medicine, Cleveland, Ohio, USA.\n",
            "(2)WW International Inc., New York City, New York, USA.\n",
            "(3)Weight and Eating Disorders Program, Perelman School of Medicine, University \n",
            "of Pennsylvania, Philadelphia, Pennsylvania, USA.\n",
            "(4)Department of Psychiatry and Psychology, Cleveland Clinic, Cleveland, Ohio, \n",
            "USA.\n",
            "(5)Department of Quantitative Health Sciences and Center for Quantitative \n",
            "Metabolic Research, Cleveland Clinic Research, Cleveland, Ohio, USA.\n",
            "(6)Department of Health Outcomes & Biomedical Informatics, University of Florida \n",
            "College of Medicine, Gainesville, Florida, USA.\n",
            "(7)Department of Pharmaceutical Outcomes & Policy, University of Florida College \n",
            "of Pharmacy, Gainesville, Florida, USA.\n",
            "(8)Center for Integrative Cardiovascular & Metabolic Disease, University of \n",
            "Florida, Gainesville, Florida, USA.\n",
            "\n",
            "OBJECTIVE: Limited data exist on telemedicine prescribing of semaglutide and \n",
            "tirzepatide. The effects of a telehealth obesity management program combining \n",
            "the provision of semaglutide and tirzepatide with a behavioral program tailored \n",
            "for long-acting antiobesity medications (AOMs) were evaluated.\n",
            "METHODS: In this single-arm study, 180 participants recruited from a telehealth \n",
            "medical obesity program received a virtual behavioral program tailored for \n",
            "long-acting AOMs, Bluetooth wireless scales, and a blood pressure (BP) cuff. \n",
            "Participants' (mean age‚Äâ=‚Äâ44.1; 91% female; 81% White; mean weight‚Äâ=‚Äâ102.8‚Äâkg) \n",
            "weight, BP, eating habits, dietary quality, physical activity, and side effects \n",
            "were assessed at baseline and at 12 and 24 weeks. Wi...\n",
            "\n",
            "  PubMed link: https://pubmed.ncbi.nlm.nih.gov/41253738/\n",
            "\n",
            "\n",
            "\n",
            "**NCT06411210** (Relevance: 79%)\n",
            "‚Ä¢ Obesity Complicating Type 1 Diabetes: GLP-1 Analogue Anti-obesity Treatment\n",
            "  Status: Recruiting\n",
            "\n",
            "  More than 40% of young adults with type 1 diabetes (T1D) also have overweight or obesity. Each of these diagnoses increase the risk of adverse cardiovascular events. GLP-1 analogues are anti-obesity medications that are cardioprotective in adults with type 2 diabetes, however evaluation of these agents in people with T1D has been limited to glycemic outcomes. Investigators aim to study the impact of GLP-1 analogue obesity treatment on markers of cardiometabolic risk in young adults with T1D and obesity.\n",
            "\n",
            "\n",
            "\n",
            "**NCT02082496** (Relevance: 78%)\n",
            "‚Ä¢ GLP-1 Response and Effect in Individuals With Obesity Causing Genetic Mutations\n",
            "  Status: Completed\n",
            "\n",
            "  The obesity epidemic is attributable to dietary and behavioral trends acting on a person's genetic makeup to determine body mass and susceptibility to obesity-related diseases. Furthermore, common forms of obesity have a strong hereditary component and many genetic pathways that contribute to obesity have already ben identified.\n",
            "\n",
            "Glucagon-like peptide-1 (GLP-1) is an incretin hormone that potentiates glucose-stimulated insulin secretion. However, GLP-1 also acts as an appetite-inhibiting hormone affecting the appetite center in the hypothalamus. Today, GLP-1 receptor agonists are available for...\n",
            "\n",
            "\n",
            "\n",
            "Summary: These trials explore potential treatments or management strategies for the condition you asked about. More details are available using the listed NCT IDs.\n",
            "\n",
            "To learn more or consider participation, visit clinicaltrials.gov and search by NCT ID. Always discuss clinical trial options with your healthcare provider.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilWN6ecnKaR8",
        "outputId": "a0f16414-17d7-45d3-95c2-6f48f348e32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(self, advice_text: str, trials: List[Dict[str, Any]])\n"
          ]
        }
      ],
      "source": [
        "# # Quick check - what does verify expect?\n",
        "# import inspect\n",
        "# from run_bot_qdrant import ActiveSafetyFilter\n",
        "\n",
        "# print(inspect.signature(ActiveSafetyFilter.verify))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKelNxetKaW6",
        "outputId": "fd424dfc-682d-472d-e79e-0671338d21e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        # Safety filter - FIXED: verify() returns (text, status, log)\n",
            "        advice_text = draft.get(\"recommendation\", \"\") if isinstance(draft, dict) else str(draft)\n",
            "        trials = retrieved.get(\"trials\", [])\n",
            "\n",
            "        final_response, safety_status, safety_log = self.safety_filter.verify(advice_text, trials)\n",
            "        self.provenance_log.append(safety_log)\n",
            "\n",
            "        # Save turn\n"
          ]
        }
      ],
      "source": [
        "# !sed -n '805,812p' /content/run_bot_qdrant.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "w4AX_Q9xKach",
        "outputId": "f4f4a77b-9205-447c-9709-52966151a7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Loading Cross-Encoder reranker...\n",
            "‚úÖ Reranker loaded\n"
          ]
        }
      ],
      "source": [
        "# # Force reload the module\n",
        "# import sys\n",
        "# if 'run_bot_qdrant' in sys.modules:\n",
        "#     del sys.modules['run_bot_qdrant']\n",
        "\n",
        "# # Now import fresh\n",
        "# from run_bot_qdrant import run_bot\n",
        "\n",
        "# # Test again\n",
        "# result = run_bot(\n",
        "#     \"What trials are studying insulin therapy for diabetes?\",\n",
        "#     qdrant_client,\n",
        "#     embed_model,\n",
        "#     gemini_model\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLR9-TISTbCa",
        "outputId": "ae443e13-f84c-4ca5-e6de-93d465b72c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def verify(self, advice_text: str, trials: List[Dict[str, Any]]):\n",
            "        # Skip safety check for list-type responses about trials\n",
            "        if any(marker in advice_text for marker in [\"NCT\", \"clinical trial\", \"clinicaltrials.gov\"]):\n",
            "            log = log_provenance_step(\n",
            "                \"ActiveSafetyFilter\",\n",
            "                {\"advice\": advice_text},\n",
            "                {\"final_text\": advice_text, \"status\": \"Pass (Trial Listing)\"},\n",
            "            )\n",
            "            return advice_text, \"Pass (Trial Listing)\", log\n",
            "\n",
            "        evidence_text = \"\\n\".join(t[\"text\"][:500] for t in trials[:3])\n",
            "\n",
            "        audit_prompt = (\n",
            "            \"You are a Medical Safety Officer reviewing AI-generated advice.\\n\\n\"\n",
            "            \"ADVICE:\\n\"\n",
            "            f\"{advice_text}\\n\\n\"\n",
            "            \"EVIDENCE FROM CLINICAL TRIALS (for context):\\n\"\n",
            "            f\"{evidence_text}\\n\\n\"\n",
            "            \"Check for safety issues:\\n\"\n",
            "            \"- If the advice suggests starting/stopping/changing medication without a doctor ‚Üí UNSAFE.\\n\"\n",
            "            \"- If it gives a diagnosis ‚Üí UNSAFE.\\n\"\n",
            "            \"- If it makes strong clinical claims not supported by evidence ‚Üí UNSAFE.\\n\"\n",
            "            \"- If it simply lists clinical trials with neutral wording and a recommendation to talk to a doctor ‚Üí SAFE.\\n\\n\"\n",
            "            \"If the advice is acceptable, respond with exactly: SAFE\\n\"\n",
            "            \"If it is not acceptable, respond starting with: CORRECTED: <safer version>\\n\"\n",
            "        )\n",
            "\n",
            "        try:\n",
            "            res = self.model.generate_content(audit_prompt, safety_settings=self.safety_cfg)\n",
            "            txt = (res.text or \"\").strip()\n",
            "            if txt.startswith(\"SAFE\") or \"SAFE\" in txt:\n",
            "                final_text = advice_text\n",
            "                status = \"Pass\"\n",
            "            else:\n",
            "                final_text = f\"‚ö†Ô∏è SAFETY REVISION:\\n{txt}\"\n",
            "                status = \"Revised\"\n",
            "        except Exception:\n",
            "            if \"NCT\" in advice_text or \"clinical trial\" in advice_text.lower():\n",
            "                final_text = advice_text\n",
            "                status = \"Pass (API Fallback)\"\n",
            "            else:\n",
            "                final_text = \"‚ö†Ô∏è Safety filter triggered. Please consult a doctor.\"\n",
            "                status = \"Revised (API Error)\"\n",
            "\n",
            "        log = log_provenance_step(\n",
            "            \"ActiveSafetyFilter\",\n",
            "            {\"advice\": advice_text},\n",
            "            {\"final_text\": final_text, \"status\": status},\n",
            "        )\n",
            "        return final_text, status, log\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import inspect\n",
        "# from run_bot_qdrant import ActiveSafetyFilter\n",
        "\n",
        "# # Show the FULL source code of verify\n",
        "# print(inspect.getsource(ActiveSafetyFilter.verify))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcktXAO0TbFZ",
        "outputId": "6c587226-96ba-4c8f-c1c6-068853ef91d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modules cleared\n"
          ]
        }
      ],
      "source": [
        "# # Kill all cached modules\n",
        "# import sys\n",
        "# for key in list(sys.modules.keys()):\n",
        "#     if any(x in key for x in ['run_bot', 'utils_qdrant', 'retrieval_agent']):\n",
        "#         del sys.modules[key]\n",
        "\n",
        "# print(\"‚úÖ Modules cleared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUcIBKF0ZHer"
      },
      "source": [
        "Update Streamlit App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jAd8XzdZeXV",
        "outputId": "e3dddce2-927f-451b-ddaa-877087316088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Packages installed!\n"
          ]
        }
      ],
      "source": [
        "# Install Streamlit and pyngrok\n",
        "!pip install -q streamlit pyngrok\n",
        "\n",
        "print(\"‚úÖ Packages installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkcOXJy8Kajv",
        "outputId": "019446a9-a729-45a9-e75e-476c3ac6dbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\"\"\"\n",
        "Streamlit UI for HealthcareBot with Qdrant backend\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Import Qdrant utilities and bot\n",
        "from utils_qdrant import load_qdrant_and_model\n",
        "from run_bot_qdrant import HealthcareBot\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Clinical Trials Search Assistant\",\n",
        "    page_icon=\"üè•\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Title + description (updated list of diseases)\n",
        "SUPPORTED_DISEASES = [\n",
        "    \"Diabetes\", \"Obesity\", \"Hypertension\", \"Cardiovascular disease\",\n",
        "    \"Chronic Kidney Disease\", \"Alzheimer‚Äôs disease\", \"Parkinson‚Äôs disease\",\n",
        "    \"Asthma\", \"COPD\", \"Breast cancer\", \"Lung cancer\", \"Prostate cancer\",\n",
        "    \"Rheumatoid Arthritis\"\n",
        "]\n",
        "\n",
        "st.title(\"üè• Clinical Trials Search Assistant\")\n",
        "st.markdown(\"**Powered by Qdrant + Gemini 2.0 Flash**\")\n",
        "st.markdown(\n",
        "    f\"Search across 260,000+ clinical trials for:\\n\"\n",
        "    f\"- {', '.join(SUPPORTED_DISEASES)}\"\n",
        ")\n",
        "\n",
        "# Sidebar for API keys + config\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configuration\")\n",
        "\n",
        "    gemini_key = st.text_input(\n",
        "        \"Gemini API Key\",\n",
        "        type=\"password\",\n",
        "        help=\"Enter your Gemini API key\"\n",
        "    )\n",
        "    qdrant_key = st.text_input(\n",
        "        \"Qdrant API Key\",\n",
        "        type=\"password\",\n",
        "        help=\"Enter your Qdrant API key\"\n",
        "    )\n",
        "\n",
        "    qdrant_url = st.text_input(\n",
        "        \"Qdrant Cluster URL\",\n",
        "        value=\"https://215ec69e-fa22-4f38-bcf3-941e73901a68.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "        help=\"Your Qdrant cluster URL\"\n",
        "    )\n",
        "\n",
        "    st.divider()\n",
        "    st.markdown(\"### üìä System Status\")\n",
        "    if gemini_key and qdrant_key:\n",
        "        st.success(\"‚úì Keys configured\")\n",
        "    else:\n",
        "        st.warning(\"Enter API keys to start\")\n",
        "\n",
        "# Initialize state\n",
        "for key in [\"messages\", \"bot\", \"qdrant_client\", \"embed_model\"]:\n",
        "    if key not in st.session_state:\n",
        "        st.session_state[key] = None if key == \"bot\" else []\n",
        "\n",
        "# Initialize bot\n",
        "if gemini_key and qdrant_key and st.session_state.bot is None:\n",
        "    with st.spinner(\"Initializing system...\"):\n",
        "        try:\n",
        "            os.environ[\"GEMINI_API_KEY\"] = gemini_key\n",
        "            genai.configure(api_key=gemini_key)\n",
        "            gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
        "\n",
        "            qdrant_client, embed_model = load_qdrant_and_model(qdrant_url, qdrant_key)\n",
        "\n",
        "            st.session_state.qdrant_client = qdrant_client\n",
        "            st.session_state.embed_model = embed_model\n",
        "            st.session_state.bot = HealthcareBot(\n",
        "                qdrant_client,\n",
        "                embed_model,\n",
        "                gemini_model\n",
        "            )\n",
        "            st.success(\"System ready!\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Initialization failed: {e}\")\n",
        "\n",
        "# Display chat history\n",
        "if st.session_state.messages:\n",
        "    for m in st.session_state.messages:\n",
        "        with st.chat_message(m[\"role\"]):\n",
        "            st.markdown(m[\"content\"])\n",
        "            if m[\"role\"] == \"assistant\" and \"metadata\" in m:\n",
        "                with st.expander(\"üìä Details\"):\n",
        "                    col1, col2 = st.columns(2)\n",
        "                    col1.metric(\"Trials Found\", m[\"metadata\"][\"num_trials\"])\n",
        "                    col2.metric(\"Confidence\", f\"{m['metadata']['avg_confidence']:.0%}\")\n",
        "\n",
        "# Input bar\n",
        "if prompt := st.chat_input(\"Ask about clinical trials...\"):\n",
        "    if st.session_state.bot is None:\n",
        "        st.error(\"Please enter API keys first!\")\n",
        "    else:\n",
        "        # Store and show user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        # Process üîç\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Searching clinical trials...\"):\n",
        "                result = st.session_state.bot.chat(prompt)\n",
        "\n",
        "                response = result[\"response\"]\n",
        "                st.markdown(response)\n",
        "\n",
        "                metadata = {\n",
        "                    \"num_trials\": result[\"num_trials\"],\n",
        "                    \"avg_confidence\": result[\"avg_confidence\"]\n",
        "                }\n",
        "\n",
        "                with st.expander(\"üìä Details\"):\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "                    col1.metric(\"Trials Found\", metadata[\"num_trials\"])\n",
        "                    col2.metric(\"Confidence\", f\"{metadata['avg_confidence']:.0%}\")\n",
        "                    col3.metric(\"Session Hash\", result[\"session_hash\"][:8])\n",
        "\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response,\n",
        "                    \"metadata\": metadata\n",
        "                })\n",
        "\n",
        "# Sidebar Example Queries ‚Äî fixed to trigger the bot\n",
        "with st.sidebar:\n",
        "    st.divider()\n",
        "    st.markdown(\"### üí° Example Queries\")\n",
        "\n",
        "    examples = [\n",
        "        \"Trials for insulin therapy in diabetes\",\n",
        "        \"Breast cancer immunotherapy trials\",\n",
        "        \"Alzheimer‚Äôs disease new medication studies\",\n",
        "        \"COPD clinical trials recruiting now\",\n",
        "        \"Prostate cancer hormone therapy trials\",\n",
        "        \"Rheumatoid arthritis biologic trials\",\n",
        "        \"Obesity and GLP-1 weight loss trials\",\n",
        "        \"Heart failure new treatment studies\",\n",
        "        \"Parkinson‚Äôs disease clinical studies\"\n",
        "    ]\n",
        "\n",
        "    for example in examples:\n",
        "        if st.button(example):\n",
        "            if st.session_state.bot:\n",
        "                # Act like user input was typed\n",
        "                st.session_state.messages.append({\"role\": \"user\", \"content\": example})\n",
        "                result = st.session_state.bot.chat(example)\n",
        "\n",
        "                metadata = {\n",
        "                    \"num_trials\": result[\"num_trials\"],\n",
        "                    \"avg_confidence\": result[\"avg_confidence\"]\n",
        "                }\n",
        "\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": result[\"response\"],\n",
        "                    \"metadata\": metadata\n",
        "                })\n",
        "\n",
        "                st.rerun()\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"\"\"\n",
        "<div style='text-align: center; color: gray; font-size: 0.9em;'>\n",
        "üî¨ Powered by Qdrant Vector Database + Gemini 2.0 Flash<br>\n",
        "üìä Searching clinical trials across 13 disease areas\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5h7FMHpyZGtT"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 cloudflared\n",
        "!chmod +x cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NMIxuy2ZGxo",
        "outputId": "09f8acc4-90d5-41a3-e264-7d4ebd0d664b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-12-09T02:58:04Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-12-09T02:58:04Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m |  https://immigration-managed-spare-scotland.trycloudflare.com                              |\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: b775e7fe-7c61-4ede-a29f-d7aa6e08ad9a\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "2025/12/09 02:58:07 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-12-09T02:58:07Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m9ebc2793-b8a1-48fa-9358-e61d5f95acd1 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13 \u001b[36mlocation=\u001b[0msea09 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-12-09T03:00:06Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ],
      "source": [
        "#AI LLM\n",
        "!streamlit run app.py &>/dev/null&\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTN6OuTUZG5E",
        "outputId": "1c68d6f6-6c21-4317-90c8-93133b099c77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "eKEi7YkmcMKu",
        "outputId": "b67b2fa3-5b00-4891-a144-1af3772b678b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading app.py...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6864081d-c0d2-44c3-9c2a-f9a0f09f7e19\", \"app.py\", 6024)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading run_bot_qdrant.py...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b01aabc0-947a-463e-925b-8892e7d6d05d\", \"run_bot_qdrant.py\", 31080)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading utils_qdrant.py...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_99b9079c-bb78-44b1-8a1e-17ce432e957e\", \"utils_qdrant.py\", 2990)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading retrieval_agent_qdrant.py...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f98edd71-74e2-4c35-a686-dd9b33d73667\", \"retrieval_agent_qdrant.py\", 5928)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All files downloaded!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create a list of files to download\n",
        "files_to_download = [\n",
        "    'app.py',\n",
        "    'run_bot_qdrant.py',\n",
        "    'utils_qdrant.py',\n",
        "    'retrieval_agent_qdrant.py'\n",
        "]\n",
        "\n",
        "# Download each file\n",
        "for file in files_to_download:\n",
        "    if os.path.exists(f'/content/{file}'):\n",
        "        print(f\"üì• Downloading {file}...\")\n",
        "        files.download(f'/content/{file}')\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {file} not found!\")\n",
        "\n",
        "print(\"\\n‚úÖ All files downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4m1qboex6ub",
        "outputId": "ab2c6fa6-0b8a-45e6-b377-b24e6e277e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit==1.31.0\n",
        "pandas==2.1.4\n",
        "numpy==1.26.3\n",
        "sentence-transformers==2.3.1\n",
        "qdrant-client==1.7.0\n",
        "google-generativeai==0.3.2\n",
        "requests==2.31.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "e9b3g-mAylRm",
        "outputId": "50d1f643-c56f-4282-87f8-1c0762c5afa5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3ceae9d9-25b9-4172-9ffb-ef70db888478\", \"requirements.txt\", 140)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "files.download('/content/requirements.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcrfLYnpynPx",
        "outputId": "e1bc6834-6051-4aba-f81a-7ea3693b63bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .dockerignore\n"
          ]
        }
      ],
      "source": [
        "%%writefile .dockerignore\n",
        "__pycache__/\n",
        "*.pyc\n",
        "*.pyo\n",
        "*.pyd\n",
        ".Python\n",
        "*.egg-info/\n",
        ".env\n",
        ".venv\n",
        "venv/\n",
        "*.log\n",
        ".DS_Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iL1WqVqeyxRW",
        "outputId": "50613aba-1cb9-41b8-94ba-d4bb582488cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c2b8e214-bc56-4eec-80a7-e5735328d959\", \".dockerignore\", 84)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "files.download('/content/.dockerignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI1N07ayyyyj",
        "outputId": "5398dca5-5fc6-4330-ecef-37ec5103eeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "# Use Python 3.11 slim image\n",
        "FROM python:3.11-slim\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Install system dependencies\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    build-essential \\\n",
        "    curl \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Copy requirements first (for caching)\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install Python dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy application files\n",
        "COPY . .\n",
        "\n",
        "# Expose port 8080 (Cloud Run requirement)\n",
        "EXPOSE 8080\n",
        "\n",
        "# Health check\n",
        "HEALTHCHECK CMD curl --fail http://localhost:8080/_stcore/health || exit 1\n",
        "\n",
        "# Run Streamlit\n",
        "CMD streamlit run app.py \\\n",
        "    --server.port=8080 \\\n",
        "    --server.address=0.0.0.0 \\\n",
        "    --server.headless=true \\\n",
        "    --server.enableCORS=false \\\n",
        "    --server.enableXsrfProtection=false\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fMg-OIo6zFlC",
        "outputId": "5dd7974e-80da-45c0-8b38-75255f9fd252"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9dbe4f21-1203-438a-b568-b6bf5be692b1\", \"Dockerfile\", 761)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "files.download('/content/Dockerfile')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}