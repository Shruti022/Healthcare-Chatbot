{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti022/Healthcare-Chatbot/blob/main/Copy_of_LLM_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVc9VWJI9ZI"
      },
      "source": [
        "Project Phase 1: Stepwise API Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUmbb9KsIaRL"
      },
      "source": [
        "Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2IWghs9QZVy",
        "outputId": "31cbc02a-2382-4a9e-cea8-fd73a340155d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas streamlit pyngrok faiss-cpu sentence-transformers numpy\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUskaUMVEYsn",
        "outputId": "eda34f59-a076-4352-b4e2-bd5a18648403"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile build_embeddings.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# === REAL PATH (from readlink) ===\n",
        "BASE = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Load Data\n",
        "# ---------------------------------------------\n",
        "df = pd.read_csv(f\"{BASE}/clinical_trials_diabetes_full.csv\")\n",
        "\n",
        "df[\"status\"] = df[\"status\"].astype(str).str.strip().str.title()\n",
        "bad_status = [\"Terminated\", \"Withdrawn\", \"Suspended\", \"No Longer Available\", \"Unknown\"]\n",
        "df_clean = df[~df[\"status\"].isin(bad_status)].copy()\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Chunking\n",
        "# ---------------------------------------------\n",
        "chunks = []\n",
        "chunk_map = []\n",
        "\n",
        "for idx, row in df_clean.iterrows():\n",
        "    title = str(row.get(\"brief_title\", \"\")).strip()\n",
        "    summary = str(row.get(\"brief_summary\", \"\")).strip()\n",
        "\n",
        "    if len(summary) < 20:\n",
        "        continue\n",
        "\n",
        "    text = f\"Title: {title}\\nSummary: {summary}\"\n",
        "    chunks.append(text)\n",
        "\n",
        "    chunk_map.append({\n",
        "        \"nct_id\": row[\"nct_id\"],\n",
        "        \"title\": title,\n",
        "        \"text\": text,\n",
        "        \"status\": row[\"status\"]\n",
        "    })\n",
        "\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Embeddings\n",
        "# ---------------------------------------------\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embed_model.encode(chunks, batch_size=64, show_progress_bar=True)\n",
        "\n",
        "np.save(f\"{BASE}/clinical_trials_diabetes_full_embeddings.npy\", embeddings)\n",
        "print(\"Saved clinical_trials_diabetes_full_embeddings.npy\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Save chunk map\n",
        "# ---------------------------------------------\n",
        "with open(f\"{BASE}/clinical_trials_diabetes_full_chunk_map.json\", \"w\") as f:\n",
        "    json.dump(chunk_map, f)\n",
        "\n",
        "print(\"Saved clinical_trials_diabetes_full_chunk_map.json\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Build & Save FAISS\n",
        "# ---------------------------------------------\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings).astype(\"float32\"))\n",
        "faiss.write_index(index, f\"{BASE}/clinical_trials_diabetes_full_faiss.index\")\n",
        "\n",
        "print(\"Saved clinical_trials_diabetes_full_faiss.index\")\n",
        "print(\"✅ Embedding build COMPLETE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mwEwzAoPHlB",
        "outputId": "1e3315d9-701d-47d7-c955-51816199eb43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting build_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_embeddings.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32A5yZO0V4VT",
        "outputId": "99b4bff2-cd8e-412e-9928-5567bbdc5a04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-24 02:35:27.909514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763951727.941252   17168 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763951727.953380   17168 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763951727.980759   17168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763951727.980793   17168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763951727.980802   17168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763951727.980809   17168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Created 18063 chunks.\n",
            "Batches: 100% 283/283 [00:31<00:00,  8.85it/s]\n",
            "Saved clinical_trials_diabetes_full_embeddings.npy\n",
            "Saved clinical_trials_diabetes_full_chunk_map.json\n",
            "Saved clinical_trials_diabetes_full_faiss.index\n",
            "✅ Embedding build COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Confidence score from distance ---\n",
        "\n",
        "def calculate_confidence_score(distance: float, normalization_factor: float = 1.0) -> float:\n",
        "    \"\"\"Inverse L2 distance score in (0,1]; closer = higher confidence.\"\"\"\n",
        "    return normalization_factor / (normalization_factor + float(distance))\n",
        "\n",
        "\n",
        "# --- Load pre-built index + chunk map ---\n",
        "\n",
        "def load_data_and_index(chunk_map_path: str, faiss_path: str):\n",
        "    \"\"\"Loads pre-built chunks and FAISS index for quick startup.\"\"\"\n",
        "    print(\"⏳ Loading pre-built RAG index...\")\n",
        "\n",
        "    with open(chunk_map_path, \"r\") as f:\n",
        "        chunk_map = json.load(f)\n",
        "\n",
        "    index = faiss.read_index(faiss_path)\n",
        "\n",
        "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    print(f\"✅ RAG Index Ready: {index.ntotal} vectors loaded.\")\n",
        "    return embed_model, index, chunk_map\n",
        "\n",
        "\n",
        "# --- Provenance logging ---\n",
        "\n",
        "def log_provenance_step(agent_name: str, input_data, output_data, detail=None):\n",
        "    \"\"\"\n",
        "    Creates a detailed log entry for a single agent step.\n",
        "    \"\"\"\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"agent\": agent_name,\n",
        "        \"input\": input_data,\n",
        "        \"output\": output_data,\n",
        "        \"detail\": detail or {},\n",
        "        \"model_version\": \"gemini-2.0-flash\",\n",
        "    }\n",
        "    return log_entry\n",
        "\n",
        "\n",
        "# --- Reproducibility hash ---\n",
        "\n",
        "def generate_reproducibility_hash(conversation_history, corpus_version: str = \"v1.0\"):\n",
        "    \"\"\"\n",
        "    Generates a deterministic session hash based on the conversation history.\n",
        "    \"\"\"\n",
        "    queries = [turn.get(\"query\", \"\") for turn in conversation_history]\n",
        "    raw = f\"{corpus_version}|{'|'.join(queries)}\"\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PXqvXLkXaX7",
        "outputId": "a109894a-0f9c-439e-f33d-4d02601ecad2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_bot.py\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "from utils import (\n",
        "    load_data_and_index,\n",
        "    log_provenance_step,\n",
        "    generate_reproducibility_hash,\n",
        "    calculate_confidence_score,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG / PATHS\n",
        "# ============================================================\n",
        "# ⚠️ FOR GITHUB: keep this as \"***\" and DO NOT commit your real key.\n",
        "API_KEY = \"AIzaSyBrz8bfaDeuz_SVcBDaX2tsaF3vhBoUl4w\"  # <-- replace in Colab with your real key before running\n",
        "\n",
        "if API_KEY == \"***\":\n",
        "    print(\"⚠️ WARNING: You must set API_KEY in run_bot.py before running.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
        "\n",
        "#BASE = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data\"\n",
        "#CHUNK_PATH = f\"{BASE}/clinical_trials_diabetes_full_chunk_map.json\"\n",
        "#FAISS_PATH = f\"{BASE}/clinical_trials_diabetes_full_faiss.index\"\n",
        "\n",
        "CHUNK_PATH = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_chunk_map.json\"\n",
        "FAISS_PATH = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_faiss.index\"\n",
        "\n",
        "# Load embedding model, FAISS index, and chunk metadata\n",
        "embed_model, faiss_index, chunk_map = load_data_and_index(CHUNK_PATH, FAISS_PATH)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 1 — Symptom Parser\n",
        "# ============================================================\n",
        "\n",
        "# class SymptomParser:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def parse(self, text: str):\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#           parsed: dict with symptoms, duration, context, intent\n",
        "#           log: provenance entry\n",
        "#         \"\"\"\n",
        "#         prompt = (\n",
        "#             \"You are a medical NLP parser.\\n\"\n",
        "#             \"Extract structured info and detect whether this is a greeting or a symptom query.\\n\\n\"\n",
        "#             f'Input: \"{text}\"\\n\\n'\n",
        "#             \"Return ONLY valid JSON with this format:\\n\"\n",
        "#             \"{\\n\"\n",
        "#             '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "#             '  \"duration\": \"text or null\",\\n'\n",
        "#             '  \"context\": \"extra free-text context\",\\n'\n",
        "#             '  \"intent\": \"greeting\" or \"symptom_query\" or \"other\"\\n'\n",
        "#             \"}\\n\"\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             res = self.model.generate_content(prompt)\n",
        "#             raw = (res.text or \"\").strip()\n",
        "#             match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "#             if match:\n",
        "#                 parsed = json.loads(match.group(0))\n",
        "#             else:\n",
        "#                 parsed = json.loads(raw)\n",
        "#         except Exception:\n",
        "#             # Fallback\n",
        "#             parsed = {\n",
        "#                 \"symptoms\": [text],\n",
        "#                 \"duration\": None,\n",
        "#                 \"context\": \"\",\n",
        "#                 \"intent\": \"symptom_query\",\n",
        "#             }\n",
        "\n",
        "#         log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "#         return parsed, log\n",
        "\n",
        "\n",
        "\n",
        "# class SymptomParser:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def parse(self, text: str):\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#           parsed: dict with symptoms, duration, context, intent, relevance_to_diabetes\n",
        "#           log: provenance entry\n",
        "#         \"\"\"\n",
        "#         prompt = (\n",
        "#             \"You are a medical NLP parser for a diabetes clinical trial chatbot.\\n\"\n",
        "#             \"Extract structured info and classify the query type.\\n\\n\"\n",
        "#             f'Input: \"{text}\"\\n\\n'\n",
        "#             \"Return ONLY valid JSON with this format:\\n\"\n",
        "#             \"{\\n\"\n",
        "#             '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "#             '  \"duration\": \"text or null\",\\n'\n",
        "#             '  \"context\": \"extra free-text context\",\\n'\n",
        "#             '  \"intent\": \"greeting\" or \"symptom_query\" or \"general_question\" or \"off_topic\",\\n'\n",
        "#             '  \"is_diabetes_related\": true or false,\\n'\n",
        "#             '  \"query_type\": \"knowledge_seeking\" or \"symptom_matching\" or \"greeting\"\\n'\n",
        "#             \"}\\n\\n\"\n",
        "#             \"Intent rules:\\n\"\n",
        "#             \"- 'greeting': hi, hello, hey, etc.\\n\"\n",
        "#             \"- 'general_question': asking about diabetes info (symptoms, treatment, etc.)\\n\"\n",
        "#             \"- 'symptom_query': describing personal symptoms\\n\"\n",
        "#             \"- 'off_topic': not related to diabetes at all\\n\\n\"\n",
        "#             \"is_diabetes_related:\\n\"\n",
        "#             \"- true if query mentions diabetes, blood sugar, insulin, HbA1c, or diabetes complications\\n\"\n",
        "#             \"- false if symptoms/conditions are unrelated (e.g., headache, stomach upset alone)\\n\"\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             res = self.model.generate_content(prompt)\n",
        "#             raw = (res.text or \"\").strip()\n",
        "#             match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "#             if match:\n",
        "#                 parsed = json.loads(match.group(0))\n",
        "#             else:\n",
        "#                 parsed = json.loads(raw)\n",
        "#         except Exception:\n",
        "#             # Fallback\n",
        "#             parsed = {\n",
        "#                 \"symptoms\": [text],\n",
        "#                 \"duration\": None,\n",
        "#                 \"context\": \"\",\n",
        "#                 \"intent\": \"symptom_query\",\n",
        "#                 \"is_diabetes_related\": True,\n",
        "#                 \"query_type\": \"symptom_matching\",\n",
        "#             }\n",
        "\n",
        "#         log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "#         return parsed, log\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SymptomParser:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          parsed: dict with symptoms, duration, context, intent, relevance_to_diabetes\n",
        "          log: provenance entry\n",
        "        \"\"\"\n",
        "        prompt = (\n",
        "            \"You are a medical NLP parser for a diabetes clinical trial chatbot.\\n\"\n",
        "            \"Extract structured info and classify the query type.\\n\\n\"\n",
        "            f'Input: \"{text}\"\\n\\n'\n",
        "            \"Return ONLY valid JSON with this format:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "            '  \"duration\": \"text or null\",\\n'\n",
        "            '  \"context\": \"extra free-text context\",\\n'\n",
        "            '  \"intent\": \"greeting\" or \"symptom_query\" or \"general_question\" or \"off_topic\",\\n'\n",
        "            '  \"is_diabetes_related\": true or false,\\n'\n",
        "            '  \"query_type\": \"knowledge_seeking\" or \"symptom_matching\" or \"greeting\",\\n'\n",
        "            '  \"user_question\": \"the actual question being asked in plain English\"\\n'\n",
        "            \"}\\n\\n\"\n",
        "            \"Classification rules:\\n\"\n",
        "            \"- intent='greeting' → query_type='greeting' (hi, hello, hey)\\n\"\n",
        "            \"- intent='general_question' → query_type='knowledge_seeking' (asking ABOUT diabetes, not describing symptoms)\\n\"\n",
        "            \"  Examples: 'What are symptoms of diabetes?', 'How is diabetes treated?', 'What is HbA1c?'\\n\"\n",
        "            \"- intent='symptom_query' → query_type='symptom_matching' (user describing THEIR symptoms)\\n\"\n",
        "            \"  Examples: 'I have high blood sugar', 'I feel tired and thirsty'\\n\"\n",
        "            \"- intent='off_topic' → not diabetes-related at all\\n\\n\"\n",
        "            \"is_diabetes_related:\\n\"\n",
        "            \"- true if about diabetes, blood sugar, insulin, HbA1c, or diabetes complications\\n\"\n",
        "            \"- false if unrelated (headache alone, cold, flu, etc.)\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            raw = (res.text or \"\").strip()\n",
        "            match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "            if match:\n",
        "                parsed = json.loads(match.group(0))\n",
        "            else:\n",
        "                parsed = json.loads(raw)\n",
        "\n",
        "            # Validation: ensure query_type matches intent\n",
        "            intent = parsed.get(\"intent\", \"symptom_query\")\n",
        "            if intent == \"general_question\":\n",
        "                parsed[\"query_type\"] = \"knowledge_seeking\"\n",
        "            elif intent == \"greeting\":\n",
        "                parsed[\"query_type\"] = \"greeting\"\n",
        "            elif intent == \"symptom_query\":\n",
        "                parsed[\"query_type\"] = \"symptom_matching\"\n",
        "\n",
        "        except Exception:\n",
        "            # Fallback\n",
        "            parsed = {\n",
        "                \"symptoms\": [text],\n",
        "                \"duration\": None,\n",
        "                \"context\": \"\",\n",
        "                \"intent\": \"symptom_query\",\n",
        "                \"is_diabetes_related\": True,\n",
        "                \"query_type\": \"symptom_matching\",\n",
        "                \"user_question\": text,\n",
        "            }\n",
        "\n",
        "        log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "        return parsed, log\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 2 — ProfileAgent (simple memory)\n",
        "# ============================================================\n",
        "\n",
        "class ProfileAgent:\n",
        "    def __init__(self, initial_profile: Dict[str, Any] = None):\n",
        "        if initial_profile is None:\n",
        "            initial_profile = {\n",
        "                \"user_id\": \"Patient\",\n",
        "                \"conditions\": [\"diabetes\"],\n",
        "                \"history\": [],\n",
        "            }\n",
        "        self.profile = initial_profile\n",
        "\n",
        "    def update_profile(self, turn_data: Dict[str, Any]):\n",
        "        self.profile.setdefault(\"history\", []).append(turn_data)\n",
        "        snapshot = {\n",
        "            \"user_id\": self.profile.get(\"user_id\", \"Patient\"),\n",
        "            \"conditions\": self.profile.get(\"conditions\", []),\n",
        "            \"num_turns\": len(self.profile[\"history\"]),\n",
        "        }\n",
        "        log = log_provenance_step(\"ProfileAgent\", turn_data, {\"profile_snapshot\": snapshot})\n",
        "        return log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 3 — RetrievalAgent (FAISS + confidence)\n",
        "# ============================================================\n",
        "\n",
        "class RetrievalAgent:\n",
        "    def __init__(self, embed_model, faiss_index, chunk_map, profile_agent: ProfileAgent = None):\n",
        "        self.embed_model = embed_model\n",
        "        self.index = faiss_index\n",
        "        self.chunk_map = chunk_map\n",
        "        self.profile_agent = profile_agent\n",
        "\n",
        "    def retrieve(self, parsed: Dict[str, Any], top_k: int = 5):\n",
        "        symptoms = parsed.get(\"symptoms\") or []\n",
        "        context = parsed.get(\"context\") or \"\"\n",
        "        query = (\" \".join(symptoms) + \" \" + context).strip()\n",
        "\n",
        "        if not query:\n",
        "            retrieval = {\"query\": \"\", \"trials\": [], \"avg_confidence\": 0.0}\n",
        "            log = log_provenance_step(\"RetrievalAgent\", parsed, retrieval, {\"reason\": \"empty_query\"})\n",
        "            return retrieval, log\n",
        "\n",
        "        q_emb = self.embed_model.encode([query])\n",
        "        distances, indices = self.index.search(q_emb.astype(\"float32\"), top_k)\n",
        "\n",
        "        trials = []\n",
        "        confs = []\n",
        "\n",
        "        for rank, idx in enumerate(indices[0]):\n",
        "            item = self.chunk_map[idx]\n",
        "            dist = float(distances[0][rank])\n",
        "            conf = calculate_confidence_score(dist)\n",
        "            confs.append(conf)\n",
        "\n",
        "            trials.append({\n",
        "                \"nct_id\": item[\"nct_id\"],\n",
        "                \"title\": item[\"title\"],\n",
        "                \"text\": item[\"text\"],\n",
        "                \"status\": item[\"status\"],\n",
        "                \"distance\": dist,\n",
        "                \"confidence\": conf,\n",
        "                \"rank\": rank + 1,\n",
        "            })\n",
        "\n",
        "        avg_conf = float(np.mean(confs)) if confs else 0.0\n",
        "\n",
        "        retrieval = {\n",
        "            \"query\": query,\n",
        "            \"trials\": trials,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "        }\n",
        "\n",
        "        detail = {\n",
        "            \"top_k\": top_k,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "            \"num_trials\": len(trials),\n",
        "        }\n",
        "\n",
        "        log = log_provenance_step(\"RetrievalAgent\", parsed, retrieval, detail)\n",
        "        return retrieval, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 4 — DiagnosisAdvisor (evidence-only)\n",
        "# ============================================================\n",
        "\n",
        "# class DiagnosisAdvisor:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def advise(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "#         trials = retrieved.get(\"trials\", [])\n",
        "#         avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "\n",
        "#         # If retrieval is very low confidence, veto early\n",
        "#         draft = {\n",
        "#             \"recommendation\": \"\",\n",
        "#             \"avg_confidence\": avg_conf,\n",
        "#         }\n",
        "\n",
        "#         if not trials or avg_conf < 0.15:\n",
        "#             draft[\"recommendation\"] = (\n",
        "#                 \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY based on the \"\n",
        "#                 \"retrieved clinical trials. Please consult your healthcare provider.\"\n",
        "#             )\n",
        "#             draft[\"confidence_veto\"] = True\n",
        "#             log = log_provenance_step(\n",
        "#                 \"DiagnosisAdvisor\",\n",
        "#                 {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "#                 draft,\n",
        "#                 {\"veto\": True},\n",
        "#             )\n",
        "#             return draft, log\n",
        "\n",
        "#         evidence_parts = []\n",
        "#         for t in trials:\n",
        "#             evidence_parts.append(\n",
        "#                 f\"Trial {t['nct_id']} (rank {t['rank']}, confidence {t['confidence']:.2f}):\\n{t['text']}\\n\"\n",
        "#             )\n",
        "#         evidence = \"\\n\".join(evidence_parts)\n",
        "\n",
        "#         prompt = (\n",
        "#             \"You are an evidence-based medical assistant summarizing clinical trials.\\n\"\n",
        "#             \"You MUST answer based ONLY on the evidence below.\\n\"\n",
        "#             \"If the evidence does not clearly answer the question, explicitly say:\\n\"\n",
        "#             '\"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY.\"\\n\\n'\n",
        "#             \"Rules:\\n\"\n",
        "#             \"- Do NOT diagnose.\\n\"\n",
        "#             \"- Do NOT tell the user to start/stop/change any medication.\\n\"\n",
        "#             \"- Summarize what the trials studied (population, interventions, outcomes).\\n\"\n",
        "#             \"- End with: 'Please discuss these findings with your healthcare provider before making any changes.'\\n\\n\"\n",
        "#             \"PATIENT QUERY (parsed JSON):\\n\"\n",
        "#             f\"{json.dumps(parsed, indent=2)}\\n\\n\"\n",
        "#             \"RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\"\n",
        "#             f\"{evidence}\\n\"\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             res = self.model.generate_content(prompt)\n",
        "#             text = (res.text or \"\").strip()\n",
        "#             if not text:\n",
        "#                 text = \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY.\"\n",
        "#             draft[\"recommendation\"] = text\n",
        "#             draft[\"confidence_veto\"] = False\n",
        "#         except Exception:\n",
        "#             draft[\"recommendation\"] = \"Unable to generate advice at this time.\"\n",
        "#             draft[\"confidence_veto\"] = True\n",
        "\n",
        "#         log = log_provenance_step(\n",
        "#             \"DiagnosisAdvisor\",\n",
        "#             {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "#             draft,\n",
        "#         )\n",
        "#         return draft, log\n",
        "\n",
        "\n",
        "\n",
        "class DiagnosisAdvisor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    # def _handle_general_question(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "    #     \"\"\"Handle general knowledge questions about diabetes\"\"\"\n",
        "    #     trials = retrieved.get(\"trials\", [])\n",
        "    #     user_query = \" \".join(parsed.get(\"symptoms\", []))\n",
        "\n",
        "    #     evidence_parts = []\n",
        "    #     for t in trials[:3]:  # Use top 3 for context\n",
        "    #         evidence_parts.append(\n",
        "    #             f\"Trial {t['nct_id']}: {t['text'][:300]}...\\n\"\n",
        "    #         )\n",
        "    #     evidence = \"\\n\".join(evidence_parts) if evidence_parts else \"No specific trials found.\"\n",
        "\n",
        "    #     prompt = (\n",
        "    #         \"You are a diabetes health educator. Answer the user's question clearly and accurately.\\n\"\n",
        "    #         \"Use your medical knowledge AND the clinical trial evidence provided as validation.\\n\\n\"\n",
        "    #         f\"USER QUESTION: {user_query}\\n\\n\"\n",
        "    #         \"SUPPORTING EVIDENCE FROM CLINICAL TRIALS:\\n\"\n",
        "    #         f\"{evidence}\\n\\n\"\n",
        "    #         \"Instructions:\\n\"\n",
        "    #         \"- Answer the question directly and clearly\\n\"\n",
        "    #         \"- If trials provide relevant context, mention them\\n\"\n",
        "    #         \"- Keep answer concise (3-4 sentences)\\n\"\n",
        "    #         \"- End with: 'For personalized advice, please consult your healthcare provider.'\\n\"\n",
        "    #     )\n",
        "\n",
        "    #     try:\n",
        "    #         res = self.model.generate_content(prompt)\n",
        "    #         text = (res.text or \"\").strip()\n",
        "    #         if not text:\n",
        "    #             text = \"I don't have enough information to answer this question accurately.\"\n",
        "    #         return text\n",
        "    #     except Exception:\n",
        "    #         return \"Unable to generate an answer at this time.\"\n",
        "\n",
        "\n",
        "    def _handle_general_question(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        \"\"\"Handle general knowledge questions about diabetes\"\"\"\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        user_question = parsed.get(\"user_question\") or \" \".join(parsed.get(\"symptoms\", []))\n",
        "\n",
        "        # Build evidence context (top 3 trials)\n",
        "        evidence_parts = []\n",
        "        for t in trials[:3]:\n",
        "            evidence_parts.append(\n",
        "                f\"Trial {t['nct_id']}: {t['text'][:400]}\"\n",
        "            )\n",
        "        evidence = \"\\n\\n\".join(evidence_parts) if evidence_parts else \"No specific trials available.\"\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a diabetes health educator. Answer the user's question clearly using your medical knowledge.\\n\"\n",
        "            \"The clinical trial evidence below provides real-world context - mention it if relevant.\\n\\n\"\n",
        "            f\"USER'S QUESTION: {user_question}\\n\\n\"\n",
        "            \"CLINICAL TRIAL CONTEXT (for reference):\\n\"\n",
        "            f\"{evidence}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"- Answer the question directly in 3-5 sentences\\n\"\n",
        "            \"- Be specific and educational\\n\"\n",
        "            \"- If trials mention relevant findings, cite them briefly\\n\"\n",
        "            \"- End with: 'For personalized advice, please consult your healthcare provider.'\\n\\n\"\n",
        "            \"Example for 'What are symptoms of diabetes?':\\n\"\n",
        "            \"Common symptoms of diabetes include increased thirst, frequent urination, unexplained weight loss, \"\n",
        "            \"fatigue, blurred vision, and slow-healing wounds. Type 1 diabetes symptoms often appear suddenly, \"\n",
        "            \"while type 2 symptoms develop gradually. Some clinical trials (like NCT...) study complications \"\n",
        "            \"such as neuropathy and retinopathy. For personalized advice, please consult your healthcare provider.\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            text = (res.text or \"\").strip()\n",
        "            if not text or len(text) < 50:\n",
        "                text = \"I don't have enough information to answer this question accurately. Please consult your healthcare provider.\"\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            return f\"Unable to generate an answer at this time. Please try rephrasing your question.\"\n",
        "\n",
        "\n",
        "\n",
        "    def _handle_symptom_query(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        \"\"\"Handle symptom-based queries with evidence\"\"\"\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "        symptoms = parsed.get(\"symptoms\", [])\n",
        "        user_input = parsed.get(\"user_question\") or \", \".join(symptoms)\n",
        "\n",
        "        evidence_parts = []\n",
        "        for t in trials:\n",
        "            evidence_parts.append(\n",
        "                f\"Trial {t['nct_id']} (confidence {t['confidence']:.2f}):\\n{t['text']}\"\n",
        "            )\n",
        "        evidence = \"\\n\\n\".join(evidence_parts)\n",
        "\n",
        "        prompt = (\n",
        "            \"You are an evidence-based diabetes health assistant.\\n\"\n",
        "            \"The user has described symptoms. Provide a helpful response based ONLY on the clinical trial evidence.\\n\\n\"\n",
        "            f\"USER'S SYMPTOMS/CONCERN: {user_input}\\n\\n\"\n",
        "            \"RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\"\n",
        "            f\"{evidence}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"- Start with 1-2 sentences acknowledging their symptoms\\n\"\n",
        "            \"- Summarize what the trials found about these symptoms/conditions\\n\"\n",
        "            \"- List 2-3 specific trials with their focus\\n\"\n",
        "            \"- Do NOT diagnose or recommend medication changes\\n\"\n",
        "            \"- End with: 'Please discuss these findings with your healthcare provider before making any changes.'\\n\\n\"\n",
        "            \"Example format:\\n\"\n",
        "            \"Based on your symptoms of high blood sugar and fatigue, several diabetes trials have investigated these concerns. \"\n",
        "            \"Research shows that fatigue is commonly studied alongside glycemic control and quality of life measures. \"\n",
        "            \"Here are relevant trials:\\n\"\n",
        "            \"• NCT... examines fatigue in type 2 diabetes patients\\n\"\n",
        "            \"• NCT... studies the relationship between blood sugar levels and energy\\n\"\n",
        "            \"Please discuss these findings with your healthcare provider before making any changes.\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            text = (res.text or \"\").strip()\n",
        "            if not text or len(text) < 50:\n",
        "                text = \"The retrieved trials may not directly address your specific symptoms. Please consult your healthcare provider.\"\n",
        "            return text\n",
        "        except Exception:\n",
        "            return \"Unable to generate advice at this time. Please consult your healthcare provider.\"\n",
        "\n",
        "\n",
        "\n",
        "    def advise(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "        query_type = parsed.get(\"query_type\", \"symptom_matching\")\n",
        "        is_diabetes_related = parsed.get(\"is_diabetes_related\", True)\n",
        "\n",
        "        draft = {\n",
        "            \"recommendation\": \"\",\n",
        "            \"avg_confidence\": avg_conf,\n",
        "            \"query_type\": query_type,\n",
        "        }\n",
        "\n",
        "        # Handle off-topic queries\n",
        "        if not is_diabetes_related:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"I'm specialized in diabetes-related clinical trials. Your query appears to be \"\n",
        "                \"about symptoms or conditions not directly related to diabetes. \"\n",
        "                \"If you have diabetes-related questions or symptoms (like high blood sugar, \"\n",
        "                \"insulin management, complications, etc.), I'd be happy to help! \"\n",
        "                \"Otherwise, please consult your healthcare provider for your current symptoms.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf}},\n",
        "                draft,\n",
        "                {\"veto\": True, \"reason\": \"off_topic\"},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        # Handle low confidence\n",
        "        if not trials or avg_conf < 0.15:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY based on the \"\n",
        "                \"retrieved clinical trials. Please consult your healthcare provider.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "                draft,\n",
        "                {\"veto\": True, \"reason\": \"low_confidence\"},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        # Route to appropriate handler\n",
        "        if query_type == \"knowledge_seeking\":\n",
        "            draft[\"recommendation\"] = self._handle_general_question(parsed, retrieved)\n",
        "        else:\n",
        "            draft[\"recommendation\"] = self._handle_symptom_query(parsed, retrieved)\n",
        "\n",
        "        draft[\"confidence_veto\"] = False\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"DiagnosisAdvisor\",\n",
        "            {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "            draft,\n",
        "        )\n",
        "        return draft, log\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 5 — ActiveSafetyFilter\n",
        "# ============================================================\n",
        "\n",
        "class ActiveSafetyFilter:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.safety_cfg = {\n",
        "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        }\n",
        "\n",
        "    def verify(self, advice_text: str, trials: List[Dict[str, Any]]):\n",
        "        evidence_text = \"\\n\".join(t[\"text\"] for t in trials)\n",
        "\n",
        "        audit_prompt = (\n",
        "            \"You are a Medical Safety Officer reviewing AI-generated advice.\\n\\n\"\n",
        "            \"ADVICE:\\n\"\n",
        "            f\"{advice_text}\\n\\n\"\n",
        "            \"EVIDENCE FROM CLINICAL TRIALS:\\n\"\n",
        "            f\"{evidence_text[:4000]}\\n\\n\"\n",
        "            \"Check for safety issues:\\n\"\n",
        "            \"- If the advice suggests stopping or changing medication without a doctor → UNSAFE.\\n\"\n",
        "            \"- If it gives a diagnosis → UNSAFE.\\n\"\n",
        "            \"- If it makes claims not supported by the evidence → UNSAFE.\\n\\n\"\n",
        "            'If the advice is acceptable, respond with exactly: SAFE\\n'\n",
        "            'If it is not acceptable, respond starting with: CORRECTED: <safer version>\\n'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(audit_prompt, safety_settings=self.safety_cfg)\n",
        "            txt = (res.text or \"\").strip()\n",
        "            if txt.startswith(\"SAFE\"):\n",
        "                final_text = advice_text\n",
        "                status = \"Pass\"\n",
        "            else:\n",
        "                final_text = f\"⚠️ SAFETY REVISION:\\n{txt}\"\n",
        "                status = \"Revised\"\n",
        "        except Exception:\n",
        "            final_text = \"⚠️ Safety filter triggered. Please consult a doctor.\"\n",
        "            status = \"Revised (API)\"\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"ActiveSafetyFilter\",\n",
        "            {\"advice\": advice_text},\n",
        "            {\"final_text\": final_text, \"status\": status},\n",
        "        )\n",
        "        return final_text, status, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# HEALTHCARE BOT (Orchestrator)\n",
        "# ============================================================\n",
        "\n",
        "class HealthcareBot:\n",
        "    def __init__(self, gemini_model, embed_model, faiss_index, chunk_map, initial_profile=None):\n",
        "        self.parser = SymptomParser(gemini_model)\n",
        "        self.profile_agent = ProfileAgent(initial_profile)\n",
        "        self.retriever = RetrievalAgent(embed_model, faiss_index, chunk_map, self.profile_agent)\n",
        "        self.advisor = DiagnosisAdvisor(gemini_model)\n",
        "        self.safety = ActiveSafetyFilter(gemini_model)\n",
        "\n",
        "        self.history: List[Dict[str, Any]] = []\n",
        "        self.provenance_chain: List[Dict[str, Any]] = []\n",
        "\n",
        "    def _handle_simple_greeting(self, user_input: str):\n",
        "        user_id = self.profile_agent.profile.get(\"user_id\", \"there\")\n",
        "        msg = (\n",
        "            f\"Hello {user_id}! I'm your clinical trial health assistant. \"\n",
        "            \"Tell me your symptoms or a question about diabetes-related trials, \"\n",
        "            \"and I’ll summarize relevant evidence. I cannot diagnose or give direct medical orders.\"\n",
        "        )\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"GreetingAgent\",\n",
        "            user_input,\n",
        "            msg,\n",
        "            {\"type\": \"greeting\"},\n",
        "        )\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": msg,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Non-RAG\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def _handle_off_topic(self, user_input: str, parsed: Dict[str, Any]):\n",
        "        \"\"\"Handle off-topic queries\"\"\"\n",
        "        msg = (\n",
        "            \"I'm specialized in diabetes-related clinical trials and information. \"\n",
        "            \"Your query appears to be about symptoms or conditions not directly related to diabetes. \"\n",
        "            \"If you have diabetes-related questions (blood sugar, insulin, complications, medications, etc.), \"\n",
        "            \"I'd be happy to help! Otherwise, please consult your healthcare provider.\"\n",
        "        )\n",
        "\n",
        "        log = log_provenance_step(\"OffTopicHandler\", user_input, msg, {\"type\": \"off_topic\"})\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": [],\n",
        "            \"safety_status\": \"Off-topic\",\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": msg,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Off-topic\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def _handle_knowledge_question(self, user_input: str, parsed: Dict[str, Any]):\n",
        "        \"\"\"Handle general diabetes knowledge questions using LLM's built-in knowledge\"\"\"\n",
        "\n",
        "        user_question = parsed.get(\"user_question\", user_input)\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a certified diabetes educator. Answer this question clearly and accurately \"\n",
        "            \"using evidence-based medical knowledge.\\n\\n\"\n",
        "            f\"QUESTION: {user_question}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"- Provide a clear, educational answer (4-6 sentences)\\n\"\n",
        "            \"- Use medical accuracy\\n\"\n",
        "            \"- Be specific with examples when relevant\\n\"\n",
        "            \"- Mention that clinical trials are available if they want personalized info\\n\"\n",
        "            \"- End with: 'For personalized guidance based on your specific situation, \"\n",
        "            \"please ask about your symptoms or consult your healthcare provider.'\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.advisor.model.generate_content(prompt)\n",
        "            answer = (res.text or \"\").strip()\n",
        "\n",
        "            if not answer or len(answer) < 50:\n",
        "                answer = (\n",
        "                    \"I can help you find relevant clinical trials for your specific symptoms. \"\n",
        "                    \"For general diabetes information, please consult your healthcare provider \"\n",
        "                    \"or ask me about specific symptoms you're experiencing.\"\n",
        "                )\n",
        "        except Exception:\n",
        "            answer = \"Unable to answer at this time. Please try rephrasing your question.\"\n",
        "\n",
        "        # Log as knowledge-based response\n",
        "        log = log_provenance_step(\n",
        "            \"KnowledgeAgent\",\n",
        "            user_input,\n",
        "            answer,\n",
        "            {\"type\": \"general_knowledge\", \"no_retrieval\": True}\n",
        "        )\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        # Update profile\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": [],\n",
        "            \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": answer,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def process_query(self, user_input: str):\n",
        "        self.provenance_chain = []\n",
        "\n",
        "        # 1. Parse\n",
        "        parsed, parse_log = self.parser.parse(user_input)\n",
        "        self.provenance_chain.append(parse_log)\n",
        "\n",
        "        intent = (parsed.get(\"intent\") or \"symptom_query\").lower()\n",
        "        is_diabetes_related = parsed.get(\"is_diabetes_related\", True)\n",
        "        query_type = parsed.get(\"query_type\", \"symptom_matching\")\n",
        "\n",
        "        # Handle greetings\n",
        "        if intent == \"greeting\":\n",
        "            return self._handle_simple_greeting(user_input)\n",
        "\n",
        "        # Handle off-topic queries (skip retrieval)\n",
        "        if intent == \"off_topic\" or not is_diabetes_related:\n",
        "            msg = (\n",
        "                \"I'm specialized in diabetes-related clinical trials. Your query appears to be \"\n",
        "                \"about symptoms or conditions not directly related to diabetes. \"\n",
        "                \"If you have diabetes-related questions or symptoms (like high blood sugar, \"\n",
        "                \"insulin management, complications, etc.), I'd be happy to help!\"\n",
        "            )\n",
        "\n",
        "            session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "            turn_data = {\n",
        "                \"query\": user_input,\n",
        "                \"parsed\": parsed,\n",
        "                \"nct_ids\": [],\n",
        "                \"safety_status\": \"Off-topic (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "            }\n",
        "            profile_log = self.profile_agent.update_profile(turn_data)\n",
        "            self.provenance_chain.append(profile_log)\n",
        "            self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": msg,\n",
        "                \"cited_trials\": [],\n",
        "                \"safety_status\": \"Off-topic (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "                \"provenance_chain\": self.provenance_chain,\n",
        "            }\n",
        "\n",
        "        # NEW: Handle general knowledge questions (no retrieval needed)\n",
        "        if query_type == \"knowledge_seeking\":\n",
        "            user_question = parsed.get(\"user_question\", user_input)\n",
        "\n",
        "            prompt = (\n",
        "                \"You are a certified diabetes educator. Answer this question clearly and accurately \"\n",
        "                \"using evidence-based medical knowledge.\\n\\n\"\n",
        "                f\"QUESTION: {user_question}\\n\\n\"\n",
        "                \"Instructions:\\n\"\n",
        "                \"- Provide a clear, educational answer (4-6 sentences)\\n\"\n",
        "                \"- Use medical accuracy and be specific\\n\"\n",
        "                \"- Mention common examples when relevant\\n\"\n",
        "                \"- End with: 'For personalized guidance based on your specific situation, \"\n",
        "                \"please describe your symptoms or consult your healthcare provider.'\\n\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                res = self.advisor.model.generate_content(prompt)\n",
        "                answer = (res.text or \"\").strip()\n",
        "\n",
        "                if not answer or len(answer) < 50:\n",
        "                    answer = (\n",
        "                        \"I can help you find relevant clinical trials for your specific symptoms. \"\n",
        "                        \"For general diabetes information, please consult your healthcare provider \"\n",
        "                        \"or ask me about specific symptoms you're experiencing.\"\n",
        "                    )\n",
        "            except Exception:\n",
        "                answer = \"Unable to answer at this time. Please try rephrasing your question.\"\n",
        "\n",
        "            # Log as knowledge-based response\n",
        "            log = log_provenance_step(\n",
        "                \"KnowledgeAgent\",\n",
        "                user_input,\n",
        "                answer,\n",
        "                {\"type\": \"general_knowledge\", \"no_retrieval\": True}\n",
        "            )\n",
        "            self.provenance_chain.append(log)\n",
        "\n",
        "            session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "            turn_data = {\n",
        "                \"query\": user_input,\n",
        "                \"parsed\": parsed,\n",
        "                \"nct_ids\": [],\n",
        "                \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "            }\n",
        "            profile_log = self.profile_agent.update_profile(turn_data)\n",
        "            self.provenance_chain.append(profile_log)\n",
        "            self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": answer,\n",
        "                \"cited_trials\": [],\n",
        "                \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "                \"provenance_chain\": self.provenance_chain,\n",
        "            }\n",
        "\n",
        "        # 2. Retrieve (for symptom queries)\n",
        "        retrieved, retrieve_log = self.retriever.retrieve(parsed)\n",
        "        self.provenance_chain.append(retrieve_log)\n",
        "\n",
        "        # 3. Advisor\n",
        "        draft_advice, advise_log = self.advisor.advise(parsed, retrieved)\n",
        "        self.provenance_chain.append(advise_log)\n",
        "\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        if draft_advice.get(\"confidence_veto\", False) or not trials:\n",
        "            final_text = draft_advice[\"recommendation\"]\n",
        "            safety_status = \"Vetoed (Low Confidence)\"\n",
        "            evidence_list = []\n",
        "        else:\n",
        "            # 4. Safety\n",
        "            final_text, safety_status, safety_log = self.safety.verify(\n",
        "                draft_advice[\"recommendation\"],\n",
        "                trials,\n",
        "            )\n",
        "            self.provenance_chain.append(safety_log)\n",
        "            evidence_list = trials\n",
        "\n",
        "        nct_ids = [t[\"nct_id\"] for t in evidence_list]\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        # 5. Update profile/history\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": nct_ids,\n",
        "            \"safety_status\": safety_status,\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": final_text,\n",
        "            \"cited_trials\": nct_ids,\n",
        "            \"safety_status\": safety_status,\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GLOBAL BOT INSTANCE + ENTRYPOINT\n",
        "# ============================================================\n",
        "\n",
        "default_profile = {\n",
        "    \"user_id\": \"Patient\",\n",
        "    \"conditions\": [\"diabetes\"],\n",
        "}\n",
        "\n",
        "_bot = HealthcareBot(gemini_model, embed_model, faiss_index, chunk_map, initial_profile=default_profile)\n",
        "\n",
        "def run_bot(user_input: str) -> Dict[str, Any]:\n",
        "    return _bot.process_query(user_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLn6JKXmMFog",
        "outputId": "1b8b3c94-3291-4a32-afc8-8e602e202112"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_bot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI frontend application simple web interface\n",
        "\n",
        "https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps"
      ],
      "metadata": {
        "id": "mDxpjMNrCwCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from run_bot import run_bot\n",
        "\n",
        "st.title(\"Clinical Trial Health Advisor 🤖\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "# Get user input\n",
        "if user_input := st.chat_input(\"Describe your symptoms or ask about trials...\"):\n",
        "    # User message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    # Bot response\n",
        "    result = run_bot(user_input)\n",
        "    reply = result[\"recommendation\"]\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(reply)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EqkmxVqOkZ-",
        "outputId": "16de3300-9252-483a-f00f-9d28e01ab5c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 cloudflared\n",
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "u5BcqHNUOklc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AI LLM from HW4 Q3\n",
        "!streamlit run app.py &>/dev/null&\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqn_TJIOkuT",
        "outputId": "b6bf3be3-f61a-469c-a9be-f8583824d70e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-11-24T02:36:11Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-24T02:36:11Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m |  https://tigers-ideal-capacity-kids.trycloudflare.com                                      |\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 32f88ccc-27cc-41a2-8d56-2839be3591e5\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.227\n",
            "2025/11/24 02:36:14 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-24T02:36:14Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m1a7b7a31-1182-4fdb-b1a3-b5736e578705 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.227 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-11-24T02:40:01Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CozZt-clfvhb",
        "outputId": "3c7fe01d-aa40-4792-dae5-de28acb28053"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPkWwZEUiN5s",
        "outputId": "14f6d9e4-fd18-4b07-8f5b-7ec94b2affe4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsrlEzrJiOOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}