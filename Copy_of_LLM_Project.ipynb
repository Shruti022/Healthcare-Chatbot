{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOn3FSxV9KyYPOPQOes4L/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti022/Healthcare-Chatbot/blob/main/Copy_of_LLM_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVc9VWJI9ZI"
      },
      "source": [
        "Project Phase 1: Stepwise API Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUmbb9KsIaRL"
      },
      "source": [
        "Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2IWghs9QZVy",
        "outputId": "ccb2c566-d4f4-4261-e4a2-62f288a45eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, faiss-cpu, pydeck, streamlit\n",
            "Successfully installed faiss-cpu-1.13.0 pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.51.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas streamlit pyngrok faiss-cpu sentence-transformers numpy\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUskaUMVEYsn",
        "outputId": "a68437cd-2452-48a1-ffc0-8be0682e50fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile build_embeddings.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# === REAL PATH (from readlink) ===\n",
        "BASE = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Load Data\n",
        "# ---------------------------------------------\n",
        "df = pd.read_csv(f\"{BASE}/clinical_trials_diabetes_full.csv\")\n",
        "\n",
        "df[\"status\"] = df[\"status\"].astype(str).str.strip().str.title()\n",
        "bad_status = [\"Terminated\", \"Withdrawn\", \"Suspended\", \"No Longer Available\", \"Unknown\"]\n",
        "df_clean = df[~df[\"status\"].isin(bad_status)].copy()\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Chunking\n",
        "# ---------------------------------------------\n",
        "chunks = []\n",
        "chunk_map = []\n",
        "\n",
        "for idx, row in df_clean.iterrows():\n",
        "    title = str(row.get(\"brief_title\", \"\")).strip()\n",
        "    summary = str(row.get(\"brief_summary\", \"\")).strip()\n",
        "\n",
        "    if len(summary) < 20:\n",
        "        continue\n",
        "\n",
        "    text = f\"Title: {title}\\nSummary: {summary}\"\n",
        "    chunks.append(text)\n",
        "\n",
        "    chunk_map.append({\n",
        "        \"nct_id\": row[\"nct_id\"],\n",
        "        \"title\": title,\n",
        "        \"text\": text,\n",
        "        \"status\": row[\"status\"]\n",
        "    })\n",
        "\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Embeddings\n",
        "# ---------------------------------------------\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embed_model.encode(chunks, batch_size=64, show_progress_bar=True)\n",
        "\n",
        "np.save(f\"{BASE}/clinical_trials_diabetes_full_embeddings.npy\", embeddings)\n",
        "print(\"Saved clinical_trials_diabetes_full_embeddings.npy\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Save chunk map\n",
        "# ---------------------------------------------\n",
        "with open(f\"{BASE}/clinical_trials_diabetes_full_chunk_map.json\", \"w\") as f:\n",
        "    json.dump(chunk_map, f)\n",
        "\n",
        "print(\"Saved clinical_trials_diabetes_full_chunk_map.json\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Build & Save FAISS\n",
        "# ---------------------------------------------\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings).astype(\"float32\"))\n",
        "faiss.write_index(index, f\"{BASE}/clinical_trials_diabetes_full_faiss.index\")\n",
        "\n",
        "print(\"Saved clinical_trials_diabetes_full_faiss.index\")\n",
        "print(\"‚úÖ Embedding build COMPLETE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mwEwzAoPHlB",
        "outputId": "e537e01d-a936-43c9-e7d3-0b844fe7fe5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing build_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_embeddings.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32A5yZO0V4VT",
        "outputId": "1910ce43-99ca-4b10-e0c9-ee94d5472cd0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-24 01:18:05.578058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763947085.598073    1206 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763947085.604809    1206 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763947085.619658    1206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763947085.619686    1206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763947085.619690    1206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763947085.619693    1206 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Created 18063 chunks.\n",
            "modules.json: 100% 349/349 [00:00<00:00, 2.45MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 814kB/s]\n",
            "README.md: 10.5kB [00:00, 35.9MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 509kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 6.04MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:00<00:00, 97.4MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 2.88MB/s]\n",
            "vocab.txt: 232kB [00:00, 18.7MB/s]\n",
            "tokenizer.json: 466kB [00:00, 37.7MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.23MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.99MB/s]\n",
            "Batches: 100% 283/283 [00:31<00:00,  8.90it/s]\n",
            "Saved clinical_trials_diabetes_full_embeddings.npy\n",
            "Saved clinical_trials_diabetes_full_chunk_map.json\n",
            "Saved clinical_trials_diabetes_full_faiss.index\n",
            "‚úÖ Embedding build COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Confidence score from distance ---\n",
        "\n",
        "def calculate_confidence_score(distance: float, normalization_factor: float = 1.0) -> float:\n",
        "    \"\"\"Inverse L2 distance score in (0,1]; closer = higher confidence.\"\"\"\n",
        "    return normalization_factor / (normalization_factor + float(distance))\n",
        "\n",
        "\n",
        "# --- Load pre-built index + chunk map ---\n",
        "\n",
        "def load_data_and_index(chunk_map_path: str, faiss_path: str):\n",
        "    \"\"\"Loads pre-built chunks and FAISS index for quick startup.\"\"\"\n",
        "    print(\"‚è≥ Loading pre-built RAG index...\")\n",
        "\n",
        "    with open(chunk_map_path, \"r\") as f:\n",
        "        chunk_map = json.load(f)\n",
        "\n",
        "    index = faiss.read_index(faiss_path)\n",
        "\n",
        "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    print(f\"‚úÖ RAG Index Ready: {index.ntotal} vectors loaded.\")\n",
        "    return embed_model, index, chunk_map\n",
        "\n",
        "\n",
        "# --- Provenance logging ---\n",
        "\n",
        "def log_provenance_step(agent_name: str, input_data, output_data, detail=None):\n",
        "    \"\"\"\n",
        "    Creates a detailed log entry for a single agent step.\n",
        "    \"\"\"\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"agent\": agent_name,\n",
        "        \"input\": input_data,\n",
        "        \"output\": output_data,\n",
        "        \"detail\": detail or {},\n",
        "        \"model_version\": \"gemini-2.0-flash\",\n",
        "    }\n",
        "    return log_entry\n",
        "\n",
        "\n",
        "# --- Reproducibility hash ---\n",
        "\n",
        "def generate_reproducibility_hash(conversation_history, corpus_version: str = \"v1.0\"):\n",
        "    \"\"\"\n",
        "    Generates a deterministic session hash based on the conversation history.\n",
        "    \"\"\"\n",
        "    queries = [turn.get(\"query\", \"\") for turn in conversation_history]\n",
        "    raw = f\"{corpus_version}|{'|'.join(queries)}\"\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PXqvXLkXaX7",
        "outputId": "6aa41b68-92db-4484-f59c-71cef0a42f1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_bot.py\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "from utils import (\n",
        "    load_data_and_index,\n",
        "    log_provenance_step,\n",
        "    generate_reproducibility_hash,\n",
        "    calculate_confidence_score,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG / PATHS\n",
        "# ============================================================\n",
        "# ‚ö†Ô∏è FOR GITHUB: keep this as \"***\" and DO NOT commit your real key.\n",
        "API_KEY = \"***\"  # <-- replace in Colab with your real key before running\n",
        "\n",
        "if API_KEY == \"***\":\n",
        "    print(\"‚ö†Ô∏è WARNING: You must set API_KEY in run_bot.py before running.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
        "\n",
        "#BASE = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data\"\n",
        "#CHUNK_PATH = f\"{BASE}/clinical_trials_diabetes_full_chunk_map.json\"\n",
        "#FAISS_PATH = f\"{BASE}/clinical_trials_diabetes_full_faiss.index\"\n",
        "\n",
        "CHUNK_PATH = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_chunk_map.json\"\n",
        "FAISS_PATH = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data/clinical_trials_diabetes_full_faiss.index\"\n",
        "\n",
        "# Load embedding model, FAISS index, and chunk metadata\n",
        "embed_model, faiss_index, chunk_map = load_data_and_index(CHUNK_PATH, FAISS_PATH)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 1 ‚Äî Symptom Parser\n",
        "# ============================================================\n",
        "\n",
        "class SymptomParser:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          parsed: dict with symptoms, duration, context, intent\n",
        "          log: provenance entry\n",
        "        \"\"\"\n",
        "        prompt = (\n",
        "            \"You are a medical NLP parser.\\n\"\n",
        "            \"Extract structured info and detect whether this is a greeting or a symptom query.\\n\\n\"\n",
        "            f'Input: \"{text}\"\\n\\n'\n",
        "            \"Return ONLY valid JSON with this format:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "            '  \"duration\": \"text or null\",\\n'\n",
        "            '  \"context\": \"extra free-text context\",\\n'\n",
        "            '  \"intent\": \"greeting\" or \"symptom_query\" or \"other\"\\n'\n",
        "            \"}\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            raw = (res.text or \"\").strip()\n",
        "            match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "            if match:\n",
        "                parsed = json.loads(match.group(0))\n",
        "            else:\n",
        "                parsed = json.loads(raw)\n",
        "        except Exception:\n",
        "            # Fallback\n",
        "            parsed = {\n",
        "                \"symptoms\": [text],\n",
        "                \"duration\": None,\n",
        "                \"context\": \"\",\n",
        "                \"intent\": \"symptom_query\",\n",
        "            }\n",
        "\n",
        "        log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "        return parsed, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 2 ‚Äî ProfileAgent (simple memory)\n",
        "# ============================================================\n",
        "\n",
        "class ProfileAgent:\n",
        "    def __init__(self, initial_profile: Dict[str, Any] = None):\n",
        "        if initial_profile is None:\n",
        "            initial_profile = {\n",
        "                \"user_id\": \"Patient\",\n",
        "                \"conditions\": [\"diabetes\"],\n",
        "                \"history\": [],\n",
        "            }\n",
        "        self.profile = initial_profile\n",
        "\n",
        "    def update_profile(self, turn_data: Dict[str, Any]):\n",
        "        self.profile.setdefault(\"history\", []).append(turn_data)\n",
        "        snapshot = {\n",
        "            \"user_id\": self.profile.get(\"user_id\", \"Patient\"),\n",
        "            \"conditions\": self.profile.get(\"conditions\", []),\n",
        "            \"num_turns\": len(self.profile[\"history\"]),\n",
        "        }\n",
        "        log = log_provenance_step(\"ProfileAgent\", turn_data, {\"profile_snapshot\": snapshot})\n",
        "        return log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 3 ‚Äî RetrievalAgent (FAISS + confidence)\n",
        "# ============================================================\n",
        "\n",
        "class RetrievalAgent:\n",
        "    def __init__(self, embed_model, faiss_index, chunk_map, profile_agent: ProfileAgent = None):\n",
        "        self.embed_model = embed_model\n",
        "        self.index = faiss_index\n",
        "        self.chunk_map = chunk_map\n",
        "        self.profile_agent = profile_agent\n",
        "\n",
        "    def retrieve(self, parsed: Dict[str, Any], top_k: int = 5):\n",
        "        symptoms = parsed.get(\"symptoms\") or []\n",
        "        context = parsed.get(\"context\") or \"\"\n",
        "        query = (\" \".join(symptoms) + \" \" + context).strip()\n",
        "\n",
        "        if not query:\n",
        "            retrieval = {\"query\": \"\", \"trials\": [], \"avg_confidence\": 0.0}\n",
        "            log = log_provenance_step(\"RetrievalAgent\", parsed, retrieval, {\"reason\": \"empty_query\"})\n",
        "            return retrieval, log\n",
        "\n",
        "        q_emb = self.embed_model.encode([query])\n",
        "        distances, indices = self.index.search(q_emb.astype(\"float32\"), top_k)\n",
        "\n",
        "        trials = []\n",
        "        confs = []\n",
        "\n",
        "        for rank, idx in enumerate(indices[0]):\n",
        "            item = self.chunk_map[idx]\n",
        "            dist = float(distances[0][rank])\n",
        "            conf = calculate_confidence_score(dist)\n",
        "            confs.append(conf)\n",
        "\n",
        "            trials.append({\n",
        "                \"nct_id\": item[\"nct_id\"],\n",
        "                \"title\": item[\"title\"],\n",
        "                \"text\": item[\"text\"],\n",
        "                \"status\": item[\"status\"],\n",
        "                \"distance\": dist,\n",
        "                \"confidence\": conf,\n",
        "                \"rank\": rank + 1,\n",
        "            })\n",
        "\n",
        "        avg_conf = float(np.mean(confs)) if confs else 0.0\n",
        "\n",
        "        retrieval = {\n",
        "            \"query\": query,\n",
        "            \"trials\": trials,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "        }\n",
        "\n",
        "        detail = {\n",
        "            \"top_k\": top_k,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "            \"num_trials\": len(trials),\n",
        "        }\n",
        "\n",
        "        log = log_provenance_step(\"RetrievalAgent\", parsed, retrieval, detail)\n",
        "        return retrieval, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 4 ‚Äî DiagnosisAdvisor (evidence-only)\n",
        "# ============================================================\n",
        "\n",
        "class DiagnosisAdvisor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def advise(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "\n",
        "        # If retrieval is very low confidence, veto early\n",
        "        draft = {\n",
        "            \"recommendation\": \"\",\n",
        "            \"avg_confidence\": avg_conf,\n",
        "        }\n",
        "\n",
        "        if not trials or avg_conf < 0.15:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY based on the \"\n",
        "                \"retrieved clinical trials. Please consult your healthcare provider.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "                draft,\n",
        "                {\"veto\": True},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        evidence_parts = []\n",
        "        for t in trials:\n",
        "            evidence_parts.append(\n",
        "                f\"Trial {t['nct_id']} (rank {t['rank']}, confidence {t['confidence']:.2f}):\\n{t['text']}\\n\"\n",
        "            )\n",
        "        evidence = \"\\n\".join(evidence_parts)\n",
        "\n",
        "        prompt = (\n",
        "            \"You are an evidence-based medical assistant summarizing clinical trials.\\n\"\n",
        "            \"You MUST answer based ONLY on the evidence below.\\n\"\n",
        "            \"If the evidence does not clearly answer the question, explicitly say:\\n\"\n",
        "            '\"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY.\"\\n\\n'\n",
        "            \"Rules:\\n\"\n",
        "            \"- Do NOT diagnose.\\n\"\n",
        "            \"- Do NOT tell the user to start/stop/change any medication.\\n\"\n",
        "            \"- Summarize what the trials studied (population, interventions, outcomes).\\n\"\n",
        "            \"- End with: 'Please discuss these findings with your healthcare provider before making any changes.'\\n\\n\"\n",
        "            \"PATIENT QUERY (parsed JSON):\\n\"\n",
        "            f\"{json.dumps(parsed, indent=2)}\\n\\n\"\n",
        "            \"RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\"\n",
        "            f\"{evidence}\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            text = (res.text or \"\").strip()\n",
        "            if not text:\n",
        "                text = \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY.\"\n",
        "            draft[\"recommendation\"] = text\n",
        "            draft[\"confidence_veto\"] = False\n",
        "        except Exception:\n",
        "            draft[\"recommendation\"] = \"Unable to generate advice at this time.\"\n",
        "            draft[\"confidence_veto\"] = True\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"DiagnosisAdvisor\",\n",
        "            {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "            draft,\n",
        "        )\n",
        "        return draft, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 5 ‚Äî ActiveSafetyFilter\n",
        "# ============================================================\n",
        "\n",
        "class ActiveSafetyFilter:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.safety_cfg = {\n",
        "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        }\n",
        "\n",
        "    def verify(self, advice_text: str, trials: List[Dict[str, Any]]):\n",
        "        evidence_text = \"\\n\".join(t[\"text\"] for t in trials)\n",
        "\n",
        "        audit_prompt = (\n",
        "            \"You are a Medical Safety Officer reviewing AI-generated advice.\\n\\n\"\n",
        "            \"ADVICE:\\n\"\n",
        "            f\"{advice_text}\\n\\n\"\n",
        "            \"EVIDENCE FROM CLINICAL TRIALS:\\n\"\n",
        "            f\"{evidence_text[:4000]}\\n\\n\"\n",
        "            \"Check for safety issues:\\n\"\n",
        "            \"- If the advice suggests stopping or changing medication without a doctor ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it gives a diagnosis ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it makes claims not supported by the evidence ‚Üí UNSAFE.\\n\\n\"\n",
        "            'If the advice is acceptable, respond with exactly: SAFE\\n'\n",
        "            'If it is not acceptable, respond starting with: CORRECTED: <safer version>\\n'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(audit_prompt, safety_settings=self.safety_cfg)\n",
        "            txt = (res.text or \"\").strip()\n",
        "            if txt.startswith(\"SAFE\"):\n",
        "                final_text = advice_text\n",
        "                status = \"Pass\"\n",
        "            else:\n",
        "                final_text = f\"‚ö†Ô∏è SAFETY REVISION:\\n{txt}\"\n",
        "                status = \"Revised\"\n",
        "        except Exception:\n",
        "            final_text = \"‚ö†Ô∏è Safety filter triggered. Please consult a doctor.\"\n",
        "            status = \"Revised (API)\"\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"ActiveSafetyFilter\",\n",
        "            {\"advice\": advice_text},\n",
        "            {\"final_text\": final_text, \"status\": status},\n",
        "        )\n",
        "        return final_text, status, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# HEALTHCARE BOT (Orchestrator)\n",
        "# ============================================================\n",
        "\n",
        "class HealthcareBot:\n",
        "    def __init__(self, gemini_model, embed_model, faiss_index, chunk_map, initial_profile=None):\n",
        "        self.parser = SymptomParser(gemini_model)\n",
        "        self.profile_agent = ProfileAgent(initial_profile)\n",
        "        self.retriever = RetrievalAgent(embed_model, faiss_index, chunk_map, self.profile_agent)\n",
        "        self.advisor = DiagnosisAdvisor(gemini_model)\n",
        "        self.safety = ActiveSafetyFilter(gemini_model)\n",
        "\n",
        "        self.history: List[Dict[str, Any]] = []\n",
        "        self.provenance_chain: List[Dict[str, Any]] = []\n",
        "\n",
        "    def _handle_simple_greeting(self, user_input: str):\n",
        "        user_id = self.profile_agent.profile.get(\"user_id\", \"there\")\n",
        "        msg = (\n",
        "            f\"Hello {user_id}! I'm your clinical trial health assistant. \"\n",
        "            \"Tell me your symptoms or a question about diabetes-related trials, \"\n",
        "            \"and I‚Äôll summarize relevant evidence. I cannot diagnose or give direct medical orders.\"\n",
        "        )\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"GreetingAgent\",\n",
        "            user_input,\n",
        "            msg,\n",
        "            {\"type\": \"greeting\"},\n",
        "        )\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": msg,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Non-RAG\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "    def process_query(self, user_input: str):\n",
        "        self.provenance_chain = []\n",
        "\n",
        "        # 1. Parse\n",
        "        parsed, parse_log = self.parser.parse(user_input)\n",
        "        self.provenance_chain.append(parse_log)\n",
        "\n",
        "        intent = (parsed.get(\"intent\") or \"symptom_query\").lower()\n",
        "        if intent == \"greeting\" or (not parsed.get(\"symptoms\") and \"?\" not in user_input):\n",
        "            return self._handle_simple_greeting(user_input)\n",
        "\n",
        "        # 2. Retrieve\n",
        "        retrieved, retrieve_log = self.retriever.retrieve(parsed)\n",
        "        self.provenance_chain.append(retrieve_log)\n",
        "\n",
        "        # 3. Advisor\n",
        "        draft_advice, advise_log = self.advisor.advise(parsed, retrieved)\n",
        "        self.provenance_chain.append(advise_log)\n",
        "\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        if draft_advice.get(\"confidence_veto\", False) or not trials:\n",
        "            final_text = draft_advice[\"recommendation\"]\n",
        "            safety_status = \"Vetoed (Low Confidence)\"\n",
        "            evidence_list = []\n",
        "        else:\n",
        "            # 4. Safety\n",
        "            final_text, safety_status, safety_log = self.safety.verify(\n",
        "                draft_advice[\"recommendation\"],\n",
        "                trials,\n",
        "            )\n",
        "            self.provenance_chain.append(safety_log)\n",
        "            evidence_list = trials\n",
        "\n",
        "        nct_ids = [t[\"nct_id\"] for t in evidence_list]\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        # 5. Update profile/history\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": nct_ids,\n",
        "            \"safety_status\": safety_status,\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": final_text,\n",
        "            \"cited_trials\": nct_ids,\n",
        "            \"safety_status\": safety_status,\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GLOBAL BOT INSTANCE + ENTRYPOINT\n",
        "# ============================================================\n",
        "\n",
        "default_profile = {\n",
        "    \"user_id\": \"Patient\",\n",
        "    \"conditions\": [\"diabetes\"],\n",
        "}\n",
        "\n",
        "_bot = HealthcareBot(gemini_model, embed_model, faiss_index, chunk_map, initial_profile=default_profile)\n",
        "\n",
        "def run_bot(user_input: str) -> Dict[str, Any]:\n",
        "    return _bot.process_query(user_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLn6JKXmMFog",
        "outputId": "840de1fd-ba28-499a-b87e-4afd30dcd989"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_bot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI frontend application simple web interface\n",
        "\n",
        "https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps"
      ],
      "metadata": {
        "id": "mDxpjMNrCwCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from run_bot import run_bot\n",
        "\n",
        "st.title(\"Clinical Trial Health Advisor ü§ñ\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "# Get user input\n",
        "if user_input := st.chat_input(\"Describe your symptoms or ask about trials...\"):\n",
        "    # User message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    # Bot response\n",
        "    result = run_bot(user_input)\n",
        "    reply = result[\"recommendation\"]\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(reply)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EqkmxVqOkZ-",
        "outputId": "2a956428-7ca8-400c-96a3-2168481e0b1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 cloudflared\n",
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "u5BcqHNUOklc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AI LLM from HW4 Q3\n",
        "!streamlit run app.py &>/dev/null&\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqn_TJIOkuT",
        "outputId": "cf0976e6-d938-4c93-9e66-5b6126cb06a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-11-24T01:19:00Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-24T01:19:00Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m |  https://outcomes-leader-eggs-expires.trycloudflare.com                                    |\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 866efacc-9700-4f16-99b9-c8be7260bc87\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "2025/11/24 01:19:03 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-24T01:19:03Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mbd87da11-bd25-42b2-9043-60a036c17f4a \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67 \u001b[36mlocation=\u001b[0mlax07 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-11-24T01:27:41Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    }
  ]
}