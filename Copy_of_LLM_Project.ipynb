{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVc9VWJI9ZI"
      },
      "source": [
        "Project Phase 1: Stepwise API Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUmbb9KsIaRL"
      },
      "source": [
        "Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2IWghs9QZVy",
        "outputId": "d7bb7185-b5fb-4a46-b5db-4e74aa561c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q requests pandas streamlit pyngrok faiss-cpu sentence-transformers numpy\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUskaUMVEYsn",
        "outputId": "c26e7325-f7da-476a-c139-38388dcf1301"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure KEY INPUT\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Securely Capture Key\n",
        "# Input will be invisible. Paste key and press Enter.\n",
        "key_input = getpass.getpass(\"üîë Enter Gemini API Key (Invisible Input): \")\n",
        "\n",
        "if not key_input.startswith(\"AIza\"):\n",
        "    print(\"‚ö†Ô∏è Warning: Key might be invalid (usually starts with 'AIza').\")\n",
        "else:\n",
        "    print(\"‚úÖ API Key captured securely in Environment Variable.\")\n",
        "\n",
        "# 2. Set as Environment Variable for the Session\n",
        "os.environ[\"GEMINI_API_KEY\"] = key_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnfATz0x1DYc",
        "outputId": "c43e29d4-c68b-4faf-dac1-bd8099e10648"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Enter Gemini API Key (Invisible Input): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ API Key captured securely in Environment Variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile build_embeddings.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# === REAL PATH (from readlink) ===\n",
        "BASE = \"/content/drive/MyDrive/LLM_Based_GenAI_Sem1/data/\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Load Data\n",
        "# ---------------------------------------------\n",
        "df = pd.read_csv(f\"{BASE}/clinical_trials_diabetes_full.csv\")\n",
        "\n",
        "df[\"status\"] = df[\"status\"].astype(str).str.strip().str.title()\n",
        "bad_status = [\"Terminated\", \"Withdrawn\", \"Suspended\", \"No Longer Available\", \"Unknown\"]\n",
        "df_clean = df[~df[\"status\"].isin(bad_status)].copy()\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Chunking\n",
        "# ---------------------------------------------\n",
        "chunks = []\n",
        "chunk_map = []\n",
        "\n",
        "for idx, row in df_clean.iterrows():\n",
        "    title = str(row.get(\"brief_title\", \"\")).strip()\n",
        "    summary = str(row.get(\"brief_summary\", \"\")).strip()\n",
        "\n",
        "    if len(summary) < 20:\n",
        "        continue\n",
        "\n",
        "    text = f\"Title: {title}\\nSummary: {summary}\"\n",
        "    chunks.append(text)\n",
        "\n",
        "    chunk_map.append({\n",
        "        \"nct_id\": row[\"nct_id\"],\n",
        "        \"title\": title,\n",
        "        \"text\": text,\n",
        "        \"status\": row[\"status\"]\n",
        "    })\n",
        "\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Embeddings\n",
        "# ---------------------------------------------\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embed_model.encode(chunks, batch_size=64, show_progress_bar=True)\n",
        "\n",
        "np.save(f\"{BASE}/clinical_trials_diabetes_full_embeddings.npy\", embeddings)\n",
        "print(\"Saved clinical_trials_diabetes_full_embeddings.npy\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Save chunk map\n",
        "# ---------------------------------------------\n",
        "with open(f\"{BASE}/clinical_trials_diabetes_full_chunk_map.json\", \"w\") as f:\n",
        "    json.dump(chunk_map, f)\n",
        "\n",
        "print(\"Saved clinical_trials_diabetes_full_chunk_map.json\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Build & Save FAISS\n",
        "# ---------------------------------------------\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings).astype(\"float32\"))\n",
        "faiss.write_index(index, f\"{BASE}/clinical_trials_diabetes_full_faiss.index\")\n",
        "\n",
        "print(\"Saved clinical_trials_diabetes_full_faiss.index\")\n",
        "print(\"‚úÖ Embedding build COMPLETE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mwEwzAoPHlB",
        "outputId": "12173081-797e-44e8-9124-528691ea0301"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing build_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_embeddings.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32A5yZO0V4VT",
        "outputId": "c82a21d8-7041-4091-d08e-2ffbf242edc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-24 21:49:45.165905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764020985.185554     823 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764020985.192682     823 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764020985.208825     823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764020985.208852     823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764020985.208857     823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764020985.208861     823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Created 18063 chunks.\n",
            "modules.json: 100% 349/349 [00:00<00:00, 3.21MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 1.02MB/s]\n",
            "README.md: 10.5kB [00:00, 42.3MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 544kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 6.55MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:00<00:00, 108MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 3.77MB/s]\n",
            "vocab.txt: 232kB [00:00, 19.3MB/s]\n",
            "tokenizer.json: 466kB [00:00, 43.8MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.06MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.62MB/s]\n",
            "Batches: 100% 283/283 [00:34<00:00,  8.27it/s]\n",
            "Saved clinical_trials_diabetes_full_embeddings.npy\n",
            "Saved clinical_trials_diabetes_full_chunk_map.json\n",
            "Saved clinical_trials_diabetes_full_faiss.index\n",
            "‚úÖ Embedding build COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Confidence score from distance ---\n",
        "\n",
        "def calculate_confidence_score(distance: float, normalization_factor: float = 1.0) -> float:\n",
        "    \"\"\"Inverse L2 distance score in (0,1]; closer = higher confidence.\"\"\"\n",
        "    return normalization_factor / (normalization_factor + float(distance))\n",
        "\n",
        "\n",
        "# --- Load pre-built index + chunk map ---\n",
        "\n",
        "def load_data_and_index(chunk_map_path: str, faiss_path: str):\n",
        "    \"\"\"Loads pre-built chunks and FAISS index for quick startup.\"\"\"\n",
        "    print(\"‚è≥ Loading pre-built RAG index...\")\n",
        "\n",
        "    with open(chunk_map_path, \"r\") as f:\n",
        "        chunk_map = json.load(f)\n",
        "\n",
        "    index = faiss.read_index(faiss_path)\n",
        "\n",
        "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    print(f\"‚úÖ RAG Index Ready: {index.ntotal} vectors loaded.\")\n",
        "    return embed_model, index, chunk_map\n",
        "\n",
        "\n",
        "# --- Provenance logging ---\n",
        "\n",
        "def log_provenance_step(agent_name: str, input_data, output_data, detail=None):\n",
        "    \"\"\"\n",
        "    Creates a detailed log entry for a single agent step.\n",
        "    \"\"\"\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"agent\": agent_name,\n",
        "        \"input\": input_data,\n",
        "        \"output\": output_data,\n",
        "        \"detail\": detail or {},\n",
        "        \"model_version\": \"gemini-2.0-flash\",\n",
        "    }\n",
        "    return log_entry\n",
        "\n",
        "\n",
        "# --- Reproducibility hash ---\n",
        "\n",
        "def generate_reproducibility_hash(conversation_history, corpus_version: str = \"v1.0\"):\n",
        "    \"\"\"\n",
        "    Generates a deterministic session hash based on the conversation history.\n",
        "    \"\"\"\n",
        "    queries = [turn.get(\"query\", \"\") for turn in conversation_history]\n",
        "    raw = f\"{corpus_version}|{'|'.join(queries)}\"\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PXqvXLkXaX7",
        "outputId": "ea337835-f093-4e57-c6ee-a357b6ec410d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_bot.py\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "from utils import (\n",
        "    load_data_and_index,\n",
        "    log_provenance_step,\n",
        "    generate_reproducibility_hash,\n",
        "    calculate_confidence_score,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG / PATHS\n",
        "# ============================================================\n",
        "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not API_KEY:\n",
        "    # Fallback only if env var is missing (prints warning)\n",
        "    print(\"‚ùå ERROR: API Key not found in environment variables.\")\n",
        "    print(\"   Please run the 'Secure Input' cell defined in the notebook first.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
        "\n",
        "#BASE = \"/content/drive/.shortcut-targets-by-id/1-SiVJhXHTHtDYSrPmW_0VfuP7gSTePcj/data\"\n",
        "#CHUNK_PATH = f\"{BASE}/clinical_trials_diabetes_full_chunk_map.json\"\n",
        "#FAISS_PATH = f\"{BASE}/clinical_trials_diabetes_full_faiss.index\"\n",
        "\n",
        "CHUNK_PATH = \"/content/drive/MyDrive/LLM_Based_GenAI_Sem1/data/clinical_trials_diabetes_full_chunk_map.json\"\n",
        "FAISS_PATH = \"/content/drive/MyDrive/LLM_Based_GenAI_Sem1/data/clinical_trials_diabetes_full_faiss.index\"\n",
        "\n",
        "# Load embedding model, FAISS index, and chunk metadata\n",
        "embed_model, faiss_index, chunk_map = load_data_and_index(CHUNK_PATH, FAISS_PATH)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 1 ‚Äî Symptom Parser\n",
        "# ============================================================\n",
        "\n",
        "# class SymptomParser:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def parse(self, text: str):\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#           parsed: dict with symptoms, duration, context, intent\n",
        "#           log: provenance entry\n",
        "#         \"\"\"\n",
        "#         prompt = (\n",
        "#             \"You are a medical NLP parser.\\n\"\n",
        "#             \"Extract structured info and detect whether this is a greeting or a symptom query.\\n\\n\"\n",
        "#             f'Input: \"{text}\"\\n\\n'\n",
        "#             \"Return ONLY valid JSON with this format:\\n\"\n",
        "#             \"{\\n\"\n",
        "#             '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "#             '  \"duration\": \"text or null\",\\n'\n",
        "#             '  \"context\": \"extra free-text context\",\\n'\n",
        "#             '  \"intent\": \"greeting\" or \"symptom_query\" or \"other\"\\n'\n",
        "#             \"}\\n\"\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             res = self.model.generate_content(prompt)\n",
        "#             raw = (res.text or \"\").strip()\n",
        "#             match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "#             if match:\n",
        "#                 parsed = json.loads(match.group(0))\n",
        "#             else:\n",
        "#                 parsed = json.loads(raw)\n",
        "#         except Exception:\n",
        "#             # Fallback\n",
        "#             parsed = {\n",
        "#                 \"symptoms\": [text],\n",
        "#                 \"duration\": None,\n",
        "#                 \"context\": \"\",\n",
        "#                 \"intent\": \"symptom_query\",\n",
        "#             }\n",
        "\n",
        "#         log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "#         return parsed, log\n",
        "\n",
        "\n",
        "\n",
        "# class SymptomParser:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def parse(self, text: str):\n",
        "#         \"\"\"\n",
        "#         Returns:\n",
        "#           parsed: dict with symptoms, duration, context, intent, relevance_to_diabetes\n",
        "#           log: provenance entry\n",
        "#         \"\"\"\n",
        "#         prompt = (\n",
        "#             \"You are a medical NLP parser for a diabetes clinical trial chatbot.\\n\"\n",
        "#             \"Extract structured info and classify the query type.\\n\\n\"\n",
        "#             f'Input: \"{text}\"\\n\\n'\n",
        "#             \"Return ONLY valid JSON with this format:\\n\"\n",
        "#             \"{\\n\"\n",
        "#             '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "#             '  \"duration\": \"text or null\",\\n'\n",
        "#             '  \"context\": \"extra free-text context\",\\n'\n",
        "#             '  \"intent\": \"greeting\" or \"symptom_query\" or \"general_question\" or \"off_topic\",\\n'\n",
        "#             '  \"is_diabetes_related\": true or false,\\n'\n",
        "#             '  \"query_type\": \"knowledge_seeking\" or \"symptom_matching\" or \"greeting\"\\n'\n",
        "#             \"}\\n\\n\"\n",
        "#             \"Intent rules:\\n\"\n",
        "#             \"- 'greeting': hi, hello, hey, etc.\\n\"\n",
        "#             \"- 'general_question': asking about diabetes info (symptoms, treatment, etc.)\\n\"\n",
        "#             \"- 'symptom_query': describing personal symptoms\\n\"\n",
        "#             \"- 'off_topic': not related to diabetes at all\\n\\n\"\n",
        "#             \"is_diabetes_related:\\n\"\n",
        "#             \"- true if query mentions diabetes, blood sugar, insulin, HbA1c, or diabetes complications\\n\"\n",
        "#             \"- false if symptoms/conditions are unrelated (e.g., headache, stomach upset alone)\\n\"\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             res = self.model.generate_content(prompt)\n",
        "#             raw = (res.text or \"\").strip()\n",
        "#             match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "#             if match:\n",
        "#                 parsed = json.loads(match.group(0))\n",
        "#             else:\n",
        "#                 parsed = json.loads(raw)\n",
        "#         except Exception:\n",
        "#             # Fallback\n",
        "#             parsed = {\n",
        "#                 \"symptoms\": [text],\n",
        "#                 \"duration\": None,\n",
        "#                 \"context\": \"\",\n",
        "#                 \"intent\": \"symptom_query\",\n",
        "#                 \"is_diabetes_related\": True,\n",
        "#                 \"query_type\": \"symptom_matching\",\n",
        "#             }\n",
        "\n",
        "#         log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "#         return parsed, log\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SymptomParser:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          parsed: dict with symptoms, duration, context, intent, relevance_to_diabetes\n",
        "          log: provenance entry\n",
        "        \"\"\"\n",
        "        prompt = (\n",
        "            \"You are a medical NLP parser for a diabetes clinical trial chatbot.\\n\"\n",
        "            \"Extract structured info and classify the query type.\\n\\n\"\n",
        "            f'Input: \"{text}\"\\n\\n'\n",
        "            \"Return ONLY valid JSON with this format:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"symptoms\": [\"list\", \"of\", \"symptoms\"],\\n'\n",
        "            '  \"duration\": \"text or null\",\\n'\n",
        "            '  \"context\": \"extra free-text context\",\\n'\n",
        "            '  \"intent\": \"greeting\" or \"symptom_query\" or \"general_question\" or \"off_topic\",\\n'\n",
        "            '  \"is_diabetes_related\": true or false,\\n'\n",
        "            '  \"query_type\": \"knowledge_seeking\" or \"symptom_matching\" or \"greeting\",\\n'\n",
        "            '  \"user_question\": \"the actual question being asked in plain English\"\\n'\n",
        "            \"}\\n\\n\"\n",
        "            \"Classification rules:\\n\"\n",
        "            \"- intent='greeting' ‚Üí query_type='greeting' (hi, hello, hey)\\n\"\n",
        "            \"- intent='general_question' ‚Üí query_type='knowledge_seeking' (asking ABOUT diabetes, not describing symptoms)\\n\"\n",
        "            \"  Examples: 'What are symptoms of diabetes?', 'How is diabetes treated?', 'What is HbA1c?'\\n\"\n",
        "            \"- intent='symptom_query' ‚Üí query_type='symptom_matching' (user describing THEIR symptoms)\\n\"\n",
        "            \"  Examples: 'I have high blood sugar', 'I feel tired and thirsty'\\n\"\n",
        "            \"- intent='off_topic' ‚Üí not diabetes-related at all\\n\\n\"\n",
        "            \"is_diabetes_related:\\n\"\n",
        "            \"- true if about diabetes, blood sugar, insulin, HbA1c, or diabetes complications\\n\"\n",
        "            \"- false if unrelated (headache alone, cold, flu, etc.)\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            raw = (res.text or \"\").strip()\n",
        "            match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "            if match:\n",
        "                parsed = json.loads(match.group(0))\n",
        "            else:\n",
        "                parsed = json.loads(raw)\n",
        "\n",
        "            # Validation: ensure query_type matches intent\n",
        "            intent = parsed.get(\"intent\", \"symptom_query\")\n",
        "            if intent == \"general_question\":\n",
        "                parsed[\"query_type\"] = \"knowledge_seeking\"\n",
        "            elif intent == \"greeting\":\n",
        "                parsed[\"query_type\"] = \"greeting\"\n",
        "            elif intent == \"symptom_query\":\n",
        "                parsed[\"query_type\"] = \"symptom_matching\"\n",
        "\n",
        "        except Exception:\n",
        "            # Fallback\n",
        "            parsed = {\n",
        "                \"symptoms\": [text],\n",
        "                \"duration\": None,\n",
        "                \"context\": \"\",\n",
        "                \"intent\": \"symptom_query\",\n",
        "                \"is_diabetes_related\": True,\n",
        "                \"query_type\": \"symptom_matching\",\n",
        "                \"user_question\": text,\n",
        "            }\n",
        "\n",
        "        log = log_provenance_step(\"SymptomParser\", text, parsed)\n",
        "        return parsed, log\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 2 ‚Äî ProfileAgent (simple memory)\n",
        "# ============================================================\n",
        "\n",
        "class ProfileAgent:\n",
        "    def __init__(self, initial_profile: Dict[str, Any] = None):\n",
        "        if initial_profile is None:\n",
        "            initial_profile = {\n",
        "                \"user_id\": \"Patient\",\n",
        "                \"conditions\": [\"diabetes\"],\n",
        "                \"history\": [],\n",
        "            }\n",
        "        self.profile = initial_profile\n",
        "\n",
        "    def update_profile(self, turn_data: Dict[str, Any]):\n",
        "        self.profile.setdefault(\"history\", []).append(turn_data)\n",
        "        snapshot = {\n",
        "            \"user_id\": self.profile.get(\"user_id\", \"Patient\"),\n",
        "            \"conditions\": self.profile.get(\"conditions\", []),\n",
        "            \"num_turns\": len(self.profile[\"history\"]),\n",
        "        }\n",
        "        log = log_provenance_step(\"ProfileAgent\", turn_data, {\"profile_snapshot\": snapshot})\n",
        "        return log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 3 ‚Äî RetrievalAgent (FAISS + confidence)\n",
        "# ============================================================\n",
        "\n",
        "class RetrievalAgent:\n",
        "    def __init__(self, embed_model, faiss_index, chunk_map, profile_agent: ProfileAgent = None):\n",
        "        self.embed_model = embed_model\n",
        "        self.index = faiss_index\n",
        "        self.chunk_map = chunk_map\n",
        "        self.profile_agent = profile_agent\n",
        "\n",
        "    def retrieve(self, parsed: Dict[str, Any], top_k: int = 5):\n",
        "        symptoms = parsed.get(\"symptoms\") or []\n",
        "        context = parsed.get(\"context\") or \"\"\n",
        "        query = (\" \".join(symptoms) + \" \" + context).strip()\n",
        "\n",
        "        if not query:\n",
        "            retrieval = {\"query\": \"\", \"trials\": [], \"avg_confidence\": 0.0}\n",
        "            log = log_provenance_step(\"RetrievalAgent\", parsed, retrieval, {\"reason\": \"empty_query\"})\n",
        "            return retrieval, log\n",
        "\n",
        "        q_emb = self.embed_model.encode([query])\n",
        "        distances, indices = self.index.search(q_emb.astype(\"float32\"), top_k)\n",
        "\n",
        "        trials = []\n",
        "        confs = []\n",
        "\n",
        "        for rank, idx in enumerate(indices[0]):\n",
        "            item = self.chunk_map[idx]\n",
        "            dist = float(distances[0][rank])\n",
        "            conf = calculate_confidence_score(dist)\n",
        "            confs.append(conf)\n",
        "\n",
        "            trials.append({\n",
        "                \"nct_id\": item[\"nct_id\"],\n",
        "                \"title\": item[\"title\"],\n",
        "                \"text\": item[\"text\"],\n",
        "                \"status\": item[\"status\"],\n",
        "                \"distance\": dist,\n",
        "                \"confidence\": conf,\n",
        "                \"rank\": rank + 1,\n",
        "            })\n",
        "\n",
        "        avg_conf = float(np.mean(confs)) if confs else 0.0\n",
        "\n",
        "        retrieval = {\n",
        "            \"query\": query,\n",
        "            \"trials\": trials,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "        }\n",
        "\n",
        "        detail = {\n",
        "            \"top_k\": top_k,\n",
        "            \"avg_confidence\": avg_conf,\n",
        "            \"num_trials\": len(trials),\n",
        "        }\n",
        "\n",
        "        log = log_provenance_step(\"RetrievalAgent\", parsed, retrieval, detail)\n",
        "        return retrieval, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 4 ‚Äî DiagnosisAdvisor (evidence-only)\n",
        "# ============================================================\n",
        "\n",
        "# class DiagnosisAdvisor:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def advise(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "#         trials = retrieved.get(\"trials\", [])\n",
        "#         avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "\n",
        "#         # If retrieval is very low confidence, veto early\n",
        "#         draft = {\n",
        "#             \"recommendation\": \"\",\n",
        "#             \"avg_confidence\": avg_conf,\n",
        "#         }\n",
        "\n",
        "#         if not trials or avg_conf < 0.15:\n",
        "#             draft[\"recommendation\"] = (\n",
        "#                 \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY based on the \"\n",
        "#                 \"retrieved clinical trials. Please consult your healthcare provider.\"\n",
        "#             )\n",
        "#             draft[\"confidence_veto\"] = True\n",
        "#             log = log_provenance_step(\n",
        "#                 \"DiagnosisAdvisor\",\n",
        "#                 {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "#                 draft,\n",
        "#                 {\"veto\": True},\n",
        "#             )\n",
        "#             return draft, log\n",
        "\n",
        "#         evidence_parts = []\n",
        "#         for t in trials:\n",
        "#             evidence_parts.append(\n",
        "#                 f\"Trial {t['nct_id']} (rank {t['rank']}, confidence {t['confidence']:.2f}):\\n{t['text']}\\n\"\n",
        "#             )\n",
        "#         evidence = \"\\n\".join(evidence_parts)\n",
        "\n",
        "#         prompt = (\n",
        "#             \"You are an evidence-based medical assistant summarizing clinical trials.\\n\"\n",
        "#             \"You MUST answer based ONLY on the evidence below.\\n\"\n",
        "#             \"If the evidence does not clearly answer the question, explicitly say:\\n\"\n",
        "#             '\"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY.\"\\n\\n'\n",
        "#             \"Rules:\\n\"\n",
        "#             \"- Do NOT diagnose.\\n\"\n",
        "#             \"- Do NOT tell the user to start/stop/change any medication.\\n\"\n",
        "#             \"- Summarize what the trials studied (population, interventions, outcomes).\\n\"\n",
        "#             \"- End with: 'Please discuss these findings with your healthcare provider before making any changes.'\\n\\n\"\n",
        "#             \"PATIENT QUERY (parsed JSON):\\n\"\n",
        "#             f\"{json.dumps(parsed, indent=2)}\\n\\n\"\n",
        "#             \"RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\"\n",
        "#             f\"{evidence}\\n\"\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             res = self.model.generate_content(prompt)\n",
        "#             text = (res.text or \"\").strip()\n",
        "#             if not text:\n",
        "#                 text = \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY.\"\n",
        "#             draft[\"recommendation\"] = text\n",
        "#             draft[\"confidence_veto\"] = False\n",
        "#         except Exception:\n",
        "#             draft[\"recommendation\"] = \"Unable to generate advice at this time.\"\n",
        "#             draft[\"confidence_veto\"] = True\n",
        "\n",
        "#         log = log_provenance_step(\n",
        "#             \"DiagnosisAdvisor\",\n",
        "#             {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "#             draft,\n",
        "#         )\n",
        "#         return draft, log\n",
        "\n",
        "\n",
        "\n",
        "class DiagnosisAdvisor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    # def _handle_general_question(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "    #     \"\"\"Handle general knowledge questions about diabetes\"\"\"\n",
        "    #     trials = retrieved.get(\"trials\", [])\n",
        "    #     user_query = \" \".join(parsed.get(\"symptoms\", []))\n",
        "\n",
        "    #     evidence_parts = []\n",
        "    #     for t in trials[:3]:  # Use top 3 for context\n",
        "    #         evidence_parts.append(\n",
        "    #             f\"Trial {t['nct_id']}: {t['text'][:300]}...\\n\"\n",
        "    #         )\n",
        "    #     evidence = \"\\n\".join(evidence_parts) if evidence_parts else \"No specific trials found.\"\n",
        "\n",
        "    #     prompt = (\n",
        "    #         \"You are a diabetes health educator. Answer the user's question clearly and accurately.\\n\"\n",
        "    #         \"Use your medical knowledge AND the clinical trial evidence provided as validation.\\n\\n\"\n",
        "    #         f\"USER QUESTION: {user_query}\\n\\n\"\n",
        "    #         \"SUPPORTING EVIDENCE FROM CLINICAL TRIALS:\\n\"\n",
        "    #         f\"{evidence}\\n\\n\"\n",
        "    #         \"Instructions:\\n\"\n",
        "    #         \"- Answer the question directly and clearly\\n\"\n",
        "    #         \"- If trials provide relevant context, mention them\\n\"\n",
        "    #         \"- Keep answer concise (3-4 sentences)\\n\"\n",
        "    #         \"- End with: 'For personalized advice, please consult your healthcare provider.'\\n\"\n",
        "    #     )\n",
        "\n",
        "    #     try:\n",
        "    #         res = self.model.generate_content(prompt)\n",
        "    #         text = (res.text or \"\").strip()\n",
        "    #         if not text:\n",
        "    #             text = \"I don't have enough information to answer this question accurately.\"\n",
        "    #         return text\n",
        "    #     except Exception:\n",
        "    #         return \"Unable to generate an answer at this time.\"\n",
        "\n",
        "\n",
        "    def _handle_general_question(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        \"\"\"Handle general knowledge questions about diabetes\"\"\"\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        user_question = parsed.get(\"user_question\") or \" \".join(parsed.get(\"symptoms\", []))\n",
        "\n",
        "        # Build evidence context (top 3 trials)\n",
        "        evidence_parts = []\n",
        "        for t in trials[:3]:\n",
        "            evidence_parts.append(\n",
        "                f\"Trial {t['nct_id']}: {t['text'][:400]}\"\n",
        "            )\n",
        "        evidence = \"\\n\\n\".join(evidence_parts) if evidence_parts else \"No specific trials available.\"\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a diabetes health educator. Answer the user's question clearly using your medical knowledge.\\n\"\n",
        "            \"The clinical trial evidence below provides real-world context - mention it if relevant.\\n\\n\"\n",
        "            f\"USER'S QUESTION: {user_question}\\n\\n\"\n",
        "            \"CLINICAL TRIAL CONTEXT (for reference):\\n\"\n",
        "            f\"{evidence}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"- Answer the question directly in 3-5 sentences\\n\"\n",
        "            \"- Be specific and educational\\n\"\n",
        "            \"- If trials mention relevant findings, cite them briefly\\n\"\n",
        "            \"- End with: 'For personalized advice, please consult your healthcare provider.'\\n\\n\"\n",
        "            \"Example for 'What are symptoms of diabetes?':\\n\"\n",
        "            \"Common symptoms of diabetes include increased thirst, frequent urination, unexplained weight loss, \"\n",
        "            \"fatigue, blurred vision, and slow-healing wounds. Type 1 diabetes symptoms often appear suddenly, \"\n",
        "            \"while type 2 symptoms develop gradually. Some clinical trials (like NCT...) study complications \"\n",
        "            \"such as neuropathy and retinopathy. For personalized advice, please consult your healthcare provider.\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            text = (res.text or \"\").strip()\n",
        "            if not text or len(text) < 50:\n",
        "                text = \"I don't have enough information to answer this question accurately. Please consult your healthcare provider.\"\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            return f\"Unable to generate an answer at this time. Please try rephrasing your question.\"\n",
        "\n",
        "\n",
        "\n",
        "    # def _handle_symptom_query(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "    #     \"\"\"Handle symptom-based queries with evidence\"\"\"\n",
        "    #     trials = retrieved.get(\"trials\", [])\n",
        "    #     avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "    #     symptoms = parsed.get(\"symptoms\", [])\n",
        "    #     user_input = parsed.get(\"user_question\") or \", \".join(symptoms)\n",
        "\n",
        "    #     evidence_parts = []\n",
        "    #     for t in trials:\n",
        "    #         evidence_parts.append(\n",
        "    #             f\"Trial {t['nct_id']} (confidence {t['confidence']:.2f}):\\n{t['text']}\"\n",
        "    #         )\n",
        "    #     evidence = \"\\n\\n\".join(evidence_parts)\n",
        "\n",
        "    #     prompt = (\n",
        "    #         \"You are an evidence-based diabetes health assistant.\\n\"\n",
        "    #         \"The user has described symptoms. Provide a helpful response based ONLY on the clinical trial evidence.\\n\\n\"\n",
        "    #         f\"USER'S SYMPTOMS/CONCERN: {user_input}\\n\\n\"\n",
        "    #         \"RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\"\n",
        "    #         f\"{evidence}\\n\\n\"\n",
        "    #         \"Instructions:\\n\"\n",
        "    #         \"- Start with 1-2 sentences acknowledging their symptoms\\n\"\n",
        "    #         \"- Summarize what the trials found about these symptoms/conditions\\n\"\n",
        "    #         \"- List 2-3 specific trials with their focus\\n\"\n",
        "    #         \"- Do NOT diagnose or recommend medication changes\\n\"\n",
        "    #         \"- End with: 'Please discuss these findings with your healthcare provider before making any changes.'\\n\\n\"\n",
        "    #         \"Example format:\\n\"\n",
        "    #         \"Based on your symptoms of high blood sugar and fatigue, several diabetes trials have investigated these concerns. \"\n",
        "    #         \"Research shows that fatigue is commonly studied alongside glycemic control and quality of life measures. \"\n",
        "    #         \"Here are relevant trials:\\n\"\n",
        "    #         \"‚Ä¢ NCT... examines fatigue in type 2 diabetes patients\\n\"\n",
        "    #         \"‚Ä¢ NCT... studies the relationship between blood sugar levels and energy\\n\"\n",
        "    #         \"Please discuss these findings with your healthcare provider before making any changes.\\n\"\n",
        "    #     )\n",
        "\n",
        "    #     try:\n",
        "    #         res = self.model.generate_content(prompt)\n",
        "    #         text = (res.text or \"\").strip()\n",
        "    #         if not text or len(text) < 50:\n",
        "    #             text = \"The retrieved trials may not directly address your specific symptoms. Please consult your healthcare provider.\"\n",
        "    #         return text\n",
        "    #     except Exception:\n",
        "    #         return \"Unable to generate advice at this time. Please consult your healthcare provider.\"\n",
        "\n",
        "\n",
        "    def _handle_symptom_query(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        \"\"\"Handle symptom-based queries with evidence\"\"\"\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "        symptoms = parsed.get(\"symptoms\", [])\n",
        "        user_input = parsed.get(\"user_question\", \", \".join(symptoms))\n",
        "\n",
        "        evidence_parts = []\n",
        "        for t in trials[:5]:  # Top 5 trials\n",
        "            evidence_parts.append(\n",
        "                f\"‚Ä¢ {t['nct_id']}: {t['text'][:350]}\"\n",
        "            )\n",
        "        evidence = \"\\n\\n\".join(evidence_parts)\n",
        "\n",
        "        prompt = (\n",
        "            \"You are an evidence-based diabetes health assistant.\\n\"\n",
        "            \"The user has described diabetes-related symptoms or concerns. Provide a helpful, empathetic response.\\n\\n\"\n",
        "            f\"USER'S SYMPTOMS/CONCERN: {user_input}\\n\\n\"\n",
        "            \"RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\"\n",
        "            f\"{evidence}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"1. Start with 1-2 sentences acknowledging their concern (brief medical context)\\n\"\n",
        "            \"2. Say 'Several clinical trials have investigated this' or similar transition\\n\"\n",
        "            \"3. List 3-4 specific trials with brief descriptions:\\n\"\n",
        "            \"   ‚Ä¢ NCT... examines [brief focus]\\n\"\n",
        "            \"   ‚Ä¢ NCT... investigates [brief focus]\\n\"\n",
        "            \"4. Do NOT diagnose or recommend medication changes\\n\"\n",
        "            \"5. End with: 'Please discuss these findings with your healthcare provider before making any changes.'\\n\\n\"\n",
        "            \"Keep response concise (5-7 sentences total).\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(prompt)\n",
        "            text = (res.text or \"\").strip()\n",
        "            if not text or len(text) < 50:\n",
        "                text = \"The retrieved trials may not directly address your specific symptoms. Please consult your healthcare provider.\"\n",
        "            return text\n",
        "        except Exception:\n",
        "            return \"Unable to generate advice at this time. Please consult your healthcare provider.\"\n",
        "\n",
        "\n",
        "\n",
        "    def advise(self, parsed: Dict[str, Any], retrieved: Dict[str, Any]):\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        avg_conf = retrieved.get(\"avg_confidence\", 0.0)\n",
        "        query_type = parsed.get(\"query_type\", \"symptom_matching\")\n",
        "        is_diabetes_related = parsed.get(\"is_diabetes_related\", True)\n",
        "\n",
        "        draft = {\n",
        "            \"recommendation\": \"\",\n",
        "            \"avg_confidence\": avg_conf,\n",
        "            \"query_type\": query_type,\n",
        "        }\n",
        "\n",
        "        # Handle off-topic queries\n",
        "        if not is_diabetes_related:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"I'm specialized in diabetes-related clinical trials. Your query appears to be \"\n",
        "                \"about symptoms or conditions not directly related to diabetes. \"\n",
        "                \"If you have diabetes-related questions or symptoms (like high blood sugar, \"\n",
        "                \"insulin management, complications, etc.), I'd be happy to help! \"\n",
        "                \"Otherwise, please consult your healthcare provider for your current symptoms.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf}},\n",
        "                draft,\n",
        "                {\"veto\": True, \"reason\": \"off_topic\"},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        # Handle low confidence\n",
        "        if not trials or avg_conf < 0.15:\n",
        "            draft[\"recommendation\"] = (\n",
        "                \"EVIDENCE IS INSUFFICIENT TO ANSWER THIS QUESTION DIRECTLY based on the \"\n",
        "                \"retrieved clinical trials. Please consult your healthcare provider.\"\n",
        "            )\n",
        "            draft[\"confidence_veto\"] = True\n",
        "            log = log_provenance_step(\n",
        "                \"DiagnosisAdvisor\",\n",
        "                {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "                draft,\n",
        "                {\"veto\": True, \"reason\": \"low_confidence\"},\n",
        "            )\n",
        "            return draft, log\n",
        "\n",
        "        # Route to appropriate handler\n",
        "        if query_type == \"knowledge_seeking\":\n",
        "            draft[\"recommendation\"] = self._handle_general_question(parsed, retrieved)\n",
        "        else:\n",
        "            draft[\"recommendation\"] = self._handle_symptom_query(parsed, retrieved)\n",
        "\n",
        "        draft[\"confidence_veto\"] = False\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"DiagnosisAdvisor\",\n",
        "            {\"parsed\": parsed, \"retrieval_meta\": {\"avg_confidence\": avg_conf, \"num_trials\": len(trials)}},\n",
        "            draft,\n",
        "        )\n",
        "        return draft, log\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# AGENT 5 ‚Äî ActiveSafetyFilter\n",
        "# ============================================================\n",
        "\n",
        "class ActiveSafetyFilter:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.safety_cfg = {\n",
        "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        }\n",
        "\n",
        "    def verify(self, advice_text: str, trials: List[Dict[str, Any]]):\n",
        "        evidence_text = \"\\n\".join(t[\"text\"] for t in trials)\n",
        "\n",
        "        audit_prompt = (\n",
        "            \"You are a Medical Safety Officer reviewing AI-generated advice.\\n\\n\"\n",
        "            \"ADVICE:\\n\"\n",
        "            f\"{advice_text}\\n\\n\"\n",
        "            \"EVIDENCE FROM CLINICAL TRIALS:\\n\"\n",
        "            f\"{evidence_text[:4000]}\\n\\n\"\n",
        "            \"Check for safety issues:\\n\"\n",
        "            \"- If the advice suggests stopping or changing medication without a doctor ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it gives a diagnosis ‚Üí UNSAFE.\\n\"\n",
        "            \"- If it makes claims not supported by the evidence ‚Üí UNSAFE.\\n\\n\"\n",
        "            'If the advice is acceptable, respond with exactly: SAFE\\n'\n",
        "            'If it is not acceptable, respond starting with: CORRECTED: <safer version>\\n'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.model.generate_content(audit_prompt, safety_settings=self.safety_cfg)\n",
        "            txt = (res.text or \"\").strip()\n",
        "            if txt.startswith(\"SAFE\"):\n",
        "                final_text = advice_text\n",
        "                status = \"Pass\"\n",
        "            else:\n",
        "                final_text = f\"‚ö†Ô∏è SAFETY REVISION:\\n{txt}\"\n",
        "                status = \"Revised\"\n",
        "        except Exception:\n",
        "            final_text = \"‚ö†Ô∏è Safety filter triggered. Please consult a doctor.\"\n",
        "            status = \"Revised (API)\"\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"ActiveSafetyFilter\",\n",
        "            {\"advice\": advice_text},\n",
        "            {\"final_text\": final_text, \"status\": status},\n",
        "        )\n",
        "        return final_text, status, log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# HEALTHCARE BOT (Orchestrator)\n",
        "# ============================================================\n",
        "\n",
        "class HealthcareBot:\n",
        "    def __init__(self, gemini_model, embed_model, faiss_index, chunk_map, initial_profile=None):\n",
        "        self.parser = SymptomParser(gemini_model)\n",
        "        self.profile_agent = ProfileAgent(initial_profile)\n",
        "        self.retriever = RetrievalAgent(embed_model, faiss_index, chunk_map, self.profile_agent)\n",
        "        self.advisor = DiagnosisAdvisor(gemini_model)\n",
        "        self.safety = ActiveSafetyFilter(gemini_model)\n",
        "\n",
        "        self.history: List[Dict[str, Any]] = []\n",
        "        self.provenance_chain: List[Dict[str, Any]] = []\n",
        "\n",
        "    def _handle_simple_greeting(self, user_input: str):\n",
        "        user_id = self.profile_agent.profile.get(\"user_id\", \"there\")\n",
        "        msg = (\n",
        "            f\"Hello {user_id}! I'm your clinical trial health assistant. \"\n",
        "            \"Tell me your symptoms or a question about diabetes-related trials, \"\n",
        "            \"and I‚Äôll summarize relevant evidence. I cannot diagnose or give direct medical orders.\"\n",
        "        )\n",
        "\n",
        "        log = log_provenance_step(\n",
        "            \"GreetingAgent\",\n",
        "            user_input,\n",
        "            msg,\n",
        "            {\"type\": \"greeting\"},\n",
        "        )\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": msg,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Non-RAG\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def _handle_off_topic(self, user_input: str, parsed: Dict[str, Any]):\n",
        "        \"\"\"Handle off-topic queries\"\"\"\n",
        "        msg = (\n",
        "            \"I'm specialized in diabetes-related clinical trials and information. \"\n",
        "            \"Your query appears to be about symptoms or conditions not directly related to diabetes. \"\n",
        "            \"If you have diabetes-related questions (blood sugar, insulin, complications, medications, etc.), \"\n",
        "            \"I'd be happy to help! Otherwise, please consult your healthcare provider.\"\n",
        "        )\n",
        "\n",
        "        log = log_provenance_step(\"OffTopicHandler\", user_input, msg, {\"type\": \"off_topic\"})\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": [],\n",
        "            \"safety_status\": \"Off-topic\",\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": msg,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Off-topic\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def _handle_knowledge_question(self, user_input: str, parsed: Dict[str, Any]):\n",
        "        \"\"\"Handle general diabetes knowledge questions using LLM's built-in knowledge\"\"\"\n",
        "\n",
        "        user_question = parsed.get(\"user_question\", user_input)\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a certified diabetes educator. Answer this question clearly and accurately \"\n",
        "            \"using evidence-based medical knowledge.\\n\\n\"\n",
        "            f\"QUESTION: {user_question}\\n\\n\"\n",
        "            \"Instructions:\\n\"\n",
        "            \"- Provide a clear, educational answer (4-6 sentences)\\n\"\n",
        "            \"- Use medical accuracy\\n\"\n",
        "            \"- Be specific with examples when relevant\\n\"\n",
        "            \"- Mention that clinical trials are available if they want personalized info\\n\"\n",
        "            \"- End with: 'For personalized guidance based on your specific situation, \"\n",
        "            \"please ask about your symptoms or consult your healthcare provider.'\\n\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            res = self.advisor.model.generate_content(prompt)\n",
        "            answer = (res.text or \"\").strip()\n",
        "\n",
        "            if not answer or len(answer) < 50:\n",
        "                answer = (\n",
        "                    \"I can help you find relevant clinical trials for your specific symptoms. \"\n",
        "                    \"For general diabetes information, please consult your healthcare provider \"\n",
        "                    \"or ask me about specific symptoms you're experiencing.\"\n",
        "                )\n",
        "        except Exception:\n",
        "            answer = \"Unable to answer at this time. Please try rephrasing your question.\"\n",
        "\n",
        "        # Log as knowledge-based response\n",
        "        log = log_provenance_step(\n",
        "            \"KnowledgeAgent\",\n",
        "            user_input,\n",
        "            answer,\n",
        "            {\"type\": \"general_knowledge\", \"no_retrieval\": True}\n",
        "        )\n",
        "        self.provenance_chain.append(log)\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        # Update profile\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": [],\n",
        "            \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": answer,\n",
        "            \"cited_trials\": [],\n",
        "            \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def process_query(self, user_input: str):\n",
        "        self.provenance_chain = []\n",
        "\n",
        "        # 1. Parse\n",
        "        parsed, parse_log = self.parser.parse(user_input)\n",
        "        self.provenance_chain.append(parse_log)\n",
        "\n",
        "        intent = (parsed.get(\"intent\") or \"symptom_query\").lower()\n",
        "        is_diabetes_related = parsed.get(\"is_diabetes_related\", True)\n",
        "        query_type = parsed.get(\"query_type\", \"symptom_matching\")\n",
        "\n",
        "        # Handle greetings\n",
        "        if intent == \"greeting\":\n",
        "            return self._handle_simple_greeting(user_input)\n",
        "\n",
        "        # Handle off-topic queries (skip retrieval)\n",
        "        if intent == \"off_topic\" or not is_diabetes_related:\n",
        "            msg = (\n",
        "                \"I'm specialized in diabetes-related clinical trials. Your query appears to be \"\n",
        "                \"about symptoms or conditions not directly related to diabetes. \"\n",
        "                \"If you have diabetes-related questions or symptoms (like high blood sugar, \"\n",
        "                \"insulin management, complications, etc.), I'd be happy to help!\"\n",
        "            )\n",
        "\n",
        "            session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "            turn_data = {\n",
        "                \"query\": user_input,\n",
        "                \"parsed\": parsed,\n",
        "                \"nct_ids\": [],\n",
        "                \"safety_status\": \"Off-topic (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "            }\n",
        "            profile_log = self.profile_agent.update_profile(turn_data)\n",
        "            self.provenance_chain.append(profile_log)\n",
        "            self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": msg,\n",
        "                \"cited_trials\": [],\n",
        "                \"safety_status\": \"Off-topic (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "                \"provenance_chain\": self.provenance_chain,\n",
        "            }\n",
        "\n",
        "        # NEW: Handle general knowledge questions (no retrieval needed)\n",
        "        if query_type == \"knowledge_seeking\":\n",
        "            user_question = parsed.get(\"user_question\", user_input)\n",
        "\n",
        "            prompt = (\n",
        "                \"You are a certified diabetes educator. Answer this question clearly and accurately \"\n",
        "                \"using evidence-based medical knowledge.\\n\\n\"\n",
        "                f\"QUESTION: {user_question}\\n\\n\"\n",
        "                \"Instructions:\\n\"\n",
        "                \"- Provide a clear, educational answer (4-6 sentences)\\n\"\n",
        "                \"- Use medical accuracy and be specific\\n\"\n",
        "                \"- Mention common examples when relevant\\n\"\n",
        "                \"- End with: 'For personalized guidance based on your specific situation, \"\n",
        "                \"please describe your symptoms or consult your healthcare provider.'\\n\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                res = self.advisor.model.generate_content(prompt)\n",
        "                answer = (res.text or \"\").strip()\n",
        "\n",
        "                if not answer or len(answer) < 50:\n",
        "                    answer = (\n",
        "                        \"I can help you find relevant clinical trials for your specific symptoms. \"\n",
        "                        \"For general diabetes information, please consult your healthcare provider \"\n",
        "                        \"or ask me about specific symptoms you're experiencing.\"\n",
        "                    )\n",
        "            except Exception:\n",
        "                answer = \"Unable to answer at this time. Please try rephrasing your question.\"\n",
        "\n",
        "            # Log as knowledge-based response\n",
        "            log = log_provenance_step(\n",
        "                \"KnowledgeAgent\",\n",
        "                user_input,\n",
        "                answer,\n",
        "                {\"type\": \"general_knowledge\", \"no_retrieval\": True}\n",
        "            )\n",
        "            self.provenance_chain.append(log)\n",
        "\n",
        "            session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "            turn_data = {\n",
        "                \"query\": user_input,\n",
        "                \"parsed\": parsed,\n",
        "                \"nct_ids\": [],\n",
        "                \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "            }\n",
        "            profile_log = self.profile_agent.update_profile(turn_data)\n",
        "            self.provenance_chain.append(profile_log)\n",
        "            self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": answer,\n",
        "                \"cited_trials\": [],\n",
        "                \"safety_status\": \"Knowledge-Based (No Retrieval)\",\n",
        "                \"session_hash\": session_hash,\n",
        "                \"provenance_chain\": self.provenance_chain,\n",
        "            }\n",
        "\n",
        "        # 2. Retrieve (for symptom queries)\n",
        "        retrieved, retrieve_log = self.retriever.retrieve(parsed)\n",
        "        self.provenance_chain.append(retrieve_log)\n",
        "\n",
        "        # 3. Advisor\n",
        "        draft_advice, advise_log = self.advisor.advise(parsed, retrieved)\n",
        "        self.provenance_chain.append(advise_log)\n",
        "\n",
        "        trials = retrieved.get(\"trials\", [])\n",
        "        if draft_advice.get(\"confidence_veto\", False) or not trials:\n",
        "            final_text = draft_advice[\"recommendation\"]\n",
        "            safety_status = \"Vetoed (Low Confidence)\"\n",
        "            evidence_list = []\n",
        "        else:\n",
        "            # 4. Safety\n",
        "            final_text, safety_status, safety_log = self.safety.verify(\n",
        "                draft_advice[\"recommendation\"],\n",
        "                trials,\n",
        "            )\n",
        "            self.provenance_chain.append(safety_log)\n",
        "            evidence_list = trials\n",
        "\n",
        "        nct_ids = [t[\"nct_id\"] for t in evidence_list]\n",
        "\n",
        "        session_hash = generate_reproducibility_hash(self.history + [{\"query\": user_input}])\n",
        "\n",
        "        # 5. Update profile/history\n",
        "        turn_data = {\n",
        "            \"query\": user_input,\n",
        "            \"parsed\": parsed,\n",
        "            \"nct_ids\": nct_ids,\n",
        "            \"safety_status\": safety_status,\n",
        "            \"session_hash\": session_hash,\n",
        "        }\n",
        "        profile_log = self.profile_agent.update_profile(turn_data)\n",
        "        self.provenance_chain.append(profile_log)\n",
        "        self.history.append({\"query\": user_input, \"response_hash\": session_hash})\n",
        "\n",
        "        return {\n",
        "            \"recommendation\": final_text,\n",
        "            \"cited_trials\": nct_ids,\n",
        "            \"safety_status\": safety_status,\n",
        "            \"session_hash\": session_hash,\n",
        "            \"provenance_chain\": self.provenance_chain,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GLOBAL BOT INSTANCE + ENTRYPOINT\n",
        "# ============================================================\n",
        "\n",
        "default_profile = {\n",
        "    \"user_id\": \"Patient\",\n",
        "    \"conditions\": [\"diabetes\"],\n",
        "}\n",
        "\n",
        "_bot = HealthcareBot(gemini_model, embed_model, faiss_index, chunk_map, initial_profile=default_profile)\n",
        "\n",
        "def run_bot(user_input: str) -> Dict[str, Any]:\n",
        "    return _bot.process_query(user_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLn6JKXmMFog",
        "outputId": "d91cec38-690d-4ad8-fe2c-ccd92aa33f2d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_bot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI frontend application simple web interface\n",
        "\n",
        "https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps"
      ],
      "metadata": {
        "id": "mDxpjMNrCwCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from run_bot import run_bot\n",
        "\n",
        "st.title(\"Clinical Trial Health Advisor ü§ñ\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "# Get user input\n",
        "if user_input := st.chat_input(\"Describe your symptoms or ask about trials...\"):\n",
        "    # User message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    # Bot response\n",
        "    result = run_bot(user_input)\n",
        "    reply = result[\"recommendation\"]\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(reply)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EqkmxVqOkZ-",
        "outputId": "f4f8c5b6-971a-4bcb-f929-476051e7e3c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 cloudflared\n",
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "u5BcqHNUOklc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AI LLM\n",
        "!streamlit run app.py &>/dev/null&\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqn_TJIOkuT",
        "outputId": "acea9f21-142d-43b0-9c39-791ebc5a7d9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-11-24T21:58:18Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-24T21:58:18Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m |  https://bachelor-respondents-jackie-tradition.trycloudflare.com                           |\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: a0d8d681-3c8b-437e-8d60-7f58478ed904\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.53\n",
            "2025/11/24 21:58:21 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-24T21:58:21Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m16eb1929-fb9b-4948-a45f-74b19e1fe206 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.53 \u001b[36mlocation=\u001b[0mlax01 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.53\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.53\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.53\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.53\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-11-24T22:02:59Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    }
  ]
}