{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 1: Install Dependencies for RAG and Streamlit\n",
        "# ============================================================================\n",
        "!pip install -q faiss-cpu sentence-transformers numpy pandas google-genai streamlit pyngrok requests"
      ],
      "metadata": {
        "id": "ybPqhfye9jhX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 2: Google Drive Mounting and Path Configuration (CRITICAL)\n",
        "# ============================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "import sys"
      ],
      "metadata": {
        "id": "B4PLCD1J_bV5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyyVS1yQ9zdp",
        "outputId": "d13ce0ad-5ad0-4685-9367-870f43c727ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 2: Google Drive Mounting & File Copy (THE FIX)\n",
        "# ============================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# PATH TO YOUR DRIVE FOLDER\n",
        "PROJECT_FOLDER_PATH = '/content/drive/MyDrive/Colab Notebooks/LLM_Based_GenAI_Sem1/data/'\n",
        "\n",
        "# Check connection\n",
        "if not os.path.exists(PROJECT_FOLDER_PATH):\n",
        "    print(f\"‚ùå Error: Folder not found at {PROJECT_FOLDER_PATH}\")\n",
        "    raise FileNotFoundError(\"Check your Drive path configuration.\")\n",
        "\n",
        "print(\"‚úÖ Drive Mounted.\")\n",
        "\n",
        "# --- CRITICAL FIX: COPY MODULES TO LOCAL RUNTIME ---\n",
        "files_to_copy = ['utils.py', 'agents.py', 'orchestrator.py']\n",
        "\n",
        "print(\"\\nüîÑ Copying agent modules to local runtime...\")\n",
        "for file_name in files_to_copy:\n",
        "    src = os.path.join(PROJECT_FOLDER_PATH, file_name)\n",
        "    dst = os.path.join('/content', file_name) # Local Colab root\n",
        "\n",
        "    if os.path.exists(src):\n",
        "        shutil.copyfile(src, dst)\n",
        "        print(f\"   - Copied {file_name} -> Local /content/\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå MISSING: {file_name} in Drive folder!\")\n",
        "        raise FileNotFoundError(f\"Missing {file_name}\")\n",
        "\n",
        "print(\"‚úÖ All modules copied. Import paths are now clean.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T5GHZu8-IWo",
        "outputId": "f8dfc597-2ff1-49da-a49f-58e9329783f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Drive Mounted.\n",
            "\n",
            "üîÑ Copying agent modules to local runtime...\n",
            "   - Copied utils.py -> Local /content/\n",
            "   - Copied agents.py -> Local /content/\n",
            "   - Copied orchestrator.py -> Local /content/\n",
            "‚úÖ All modules copied. Import paths are now clean.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 3: Define Data Paths & Load RAG (Verification)\n",
        "# ============================================================================\n",
        "\n",
        "# We import directly now since files are local\n",
        "import utils\n",
        "import agents\n",
        "import orchestrator\n",
        "from utils import load_data_and_index\n",
        "\n",
        "# Define Absolute Data Paths (Data stays in Drive)\n",
        "CHUNK_MAP_PATH = os.path.join(PROJECT_FOLDER_PATH, 'clinical_trials_diabetes_full_chunk_map.json')\n",
        "FAISS_INDEX_PATH = os.path.join(PROJECT_FOLDER_PATH, 'clinical_trials_diabetes_full_faiss.index')\n",
        "\n",
        "# Verify data existence\n",
        "if not os.path.exists(CHUNK_MAP_PATH):\n",
        "    raise FileNotFoundError(f\"Chunk map not found at: {CHUNK_MAP_PATH}\")\n",
        "if not os.path.exists(FAISS_INDEX_PATH):\n",
        "    raise FileNotFoundError(f\"FAISS index not found at: {FAISS_INDEX_PATH}\")\n",
        "\n",
        "print(f\"‚úÖ Data paths verified:\\n  - {CHUNK_MAP_PATH}\\n  - {FAISS_INDEX_PATH}\")\n",
        "\n",
        "# Load RAG components globally for checking\n",
        "print(\"\\n‚è≥ Loading RAG Index (Test Load)...\")\n",
        "embed_model, faiss_index, chunk_map = load_data_and_index(CHUNK_MAP_PATH, FAISS_INDEX_PATH)\n",
        "print(\"‚úÖ RAG Index Verified in Notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHj-lryAHeeH",
        "outputId": "05621690-0b0b-4a55-8cc8-125ba13cd616"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data paths verified:\n",
            "  - /content/drive/MyDrive/Colab Notebooks/LLM_Based_GenAI_Sem1/data/clinical_trials_diabetes_full_chunk_map.json\n",
            "  - /content/drive/MyDrive/Colab Notebooks/LLM_Based_GenAI_Sem1/data/clinical_trials_diabetes_full_faiss.index\n",
            "\n",
            "‚è≥ Loading RAG Index (Test Load)...\n",
            "‚è≥ Loading pre-built RAG index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAG Index Ready: 18063 vectors loaded.\n",
            "‚úÖ RAG Index Verified in Notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 4: Intialize API Key\n",
        "# ============================================================================\n",
        "# Use %run magic with explicit paths and **.py** extensions\n",
        "# print(\"\\n--- Loading Modular Components (.py files) ---\")\n",
        "\n",
        "# CRITICAL FIX: Ensure files are PURE PYTHON code, not notebook JSON.\n",
        "# %run \"{PROJECT_FOLDER_PATH}utils.py\"\n",
        "# %run \"{PROJECT_FOLDER_PATH}agents.py\"\n",
        "# %run \"{PROJECT_FOLDER_PATH}orchestrator.py\"\n",
        "\n",
        "\n",
        "# Now, we use standard Python imports for all modules.\n",
        "# print(\"\\n--- Loading Modular Components (Importing dependencies) ---\")\n",
        "\n",
        "# import utils # Imports the utility functions\n",
        "# import agents # Imports all agent classes\n",
        "# import orchestrator # <-- FIX: Import orchestrator as a module\n",
        "\n",
        "# # Now define the necessary global functions/classes from modules\n",
        "# from utils import load_data_and_index\n",
        "\n",
        "\n",
        "# print(\"--- Components Loaded Successfully ---\")\n",
        "\n",
        "# # --- RAG Data Loading ---\n",
        "# CHUNK_MAP_PATH = os.path.join(PROJECT_FOLDER_PATH, 'clinical_trials_diabetes_full_chunk_map.json')\n",
        "# FAISS_INDEX_PATH = os.path.join(PROJECT_FOLDER_PATH, 'clinical_trials_diabetes_full_faiss.index')\n",
        "\n",
        "# # Load the RAG components (these become global variables)\n",
        "# embed_model, faiss_index, chunk_map = load_data_and_index(CHUNK_MAP_PATH, FAISS_INDEX_PATH)\n",
        "\n",
        "# Initialize Gemini Model\n",
        "API_KEY = \"xxxxx\"\n",
        "# genai.configure(api_key=API_KEY)\n",
        "# gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "\n",
        "# # We define the paths for the script to use\n",
        "# APP_CHUNK_MAP_PATH = os.path.join(PROJECT_FOLDER_PATH, 'clinical_trials_diabetes_full_chunk_map.json')\n",
        "# APP_FAISS_INDEX_PATH = os.path.join(PROJECT_FOLDER_PATH, 'clinical_trials_diabetes_full_faiss.index')\n"
      ],
      "metadata": {
        "id": "D3E83WTt-Jsk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jdr7DZgf9dYZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Step 5: Initialization and Streamlit UI Execution\n",
        "# ============================================================================\n",
        "\n",
        "# # Initialize Gemini Model\n",
        "# API_KEY = \"AIzaSyAdaiU1-sckZlVHlwzf5qcfKqTYzjN1HXg\"\n",
        "# genai.configure(api_key=API_KEY)\n",
        "# gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "\n",
        "# Streamlit UI .py File\n",
        "# We need to temporarily save the contents of streamlit_app.ipynb\n",
        "# to a local .py file for the 'streamlit run' command to work.\n",
        "# The Streamlit content now relies on the modules being found via the sys.path fix.\n",
        "\n",
        "# We inject the ACTUAL PATH STRINGS into the script content\n",
        "STREAMLIT_APP_CONTENT = f\"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# IMPORT YOUR MODULES (Now working natively)\n",
        "import orchestrator\n",
        "from utils import load_data_and_index\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "def initialize_chatbot():\n",
        "    # 1. API Key\n",
        "    if \"GEMINI_API_KEY\" not in os.environ:\n",
        "        genai.configure(api_key=\"{{API_KEY}}\")\n",
        "\n",
        "    # 2. Load Data\n",
        "    # We use the exact paths verified in the notebook\n",
        "    chunk_path = r\"{{CHUNK_MAP_PATH}}\"\n",
        "    faiss_path = r\"{{FAISS_INDEX_PATH}}\"\n",
        "\n",
        "    if 'embed_model' not in st.session_state:\n",
        "        try:\n",
        "            embed_model, faiss_index, chunk_map = load_data_and_index(chunk_path, faiss_path)\n",
        "            st.session_state.embed_model = embed_model\n",
        "            st.session_state.faiss_index = faiss_index\n",
        "            st.session_state.chunk_map = chunk_map\n",
        "        except Exception as e:\n",
        "            st.error(f\"Data Load Error: {{e}}\")\n",
        "            st.stop()\n",
        "\n",
        "    # 3. Initialize Bot\n",
        "    gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "\n",
        "    initial_profile = {{\n",
        "        'user_id': 'Alice',\n",
        "        'age': 55,\n",
        "        'conditions': ['Type 2 Diabetes', 'High Cholesterol'],\n",
        "        'medications': ['Statin']\n",
        "    }}\n",
        "\n",
        "    if 'bot' not in st.session_state:\n",
        "        st.session_state.bot = orchestrator.HealthcareBot(\n",
        "            gemini_model=gemini_model,\n",
        "            embed_model=st.session_state.embed_model,\n",
        "            faiss_index=st.session_state.faiss_index,\n",
        "            chunk_map=st.session_state.chunk_map,\n",
        "            initial_profile=initial_profile\n",
        "        )\n",
        "        st.session_state.messages = []\n",
        "        st.session_state.messages.append({{\"role\": \"assistant\", \"content\": f\"Hello {{initial_profile['user_id']}}! I am your AI Health Assistant. How can I help you?\"}})\n",
        "\n",
        "# --- UI LAYOUT ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Healthcare RAG Chatbot\")\n",
        "st.title(\"ü©∫ Personalized Healthcare Assistant (RAG Agent)\")\n",
        "\n",
        "with st.spinner(\"Initializing System...\"):\n",
        "    initialize_chatbot()\n",
        "\n",
        "bot = st.session_state.bot\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"üë§ User Profile\")\n",
        "    st.json(bot.profile_agent.profile)\n",
        "    st.divider()\n",
        "    st.header(\"üõ†Ô∏è Debug Info\")\n",
        "    if bot.history:\n",
        "        st.caption(f\"Last Hash: {{bot.history[-1].get('response_hash', 'N/A')}}\")\n",
        "\n",
        "# Chat\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Ask about diabetes trials...\"):\n",
        "    st.session_state.messages.append({{\"role\": \"user\", \"content\": prompt}})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.spinner('Analyzing clinical trials...'):\n",
        "        result = bot.process_query(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(result['recommendation'])\n",
        "\n",
        "        # Provenance Display\n",
        "        with st.expander(\"üî¨ Traceability & Evidence Chain\"):\n",
        "            st.info(f\"Safety: {{result['safety_status']}}\")\n",
        "\n",
        "            for step in result['provenance_chain']:\n",
        "                with st.container(border=True):\n",
        "                    st.caption(f\"**{{step['agent']}}**\")\n",
        "\n",
        "                    if step['agent'] == 'RetrievalAgent':\n",
        "                        trials = step['output'].get('trials', [])\n",
        "                        score = trials[0].get('retrieval_score', 0.0) if trials else 0.0\n",
        "                        st.markdown(f\"**Retrieved:** {{len(trials)}} | **Top Score:** `{{score:.2f}}`\")\n",
        "\n",
        "                    elif step['agent'] == 'DiagnosisAdvisor':\n",
        "                        st.markdown(f\"**Veto:** `{{step['output'].get('veto', False)}}`\")\n",
        "\n",
        "                    st.json(step)\n",
        "\n",
        "    st.session_state.messages.append({{\"role\": \"assistant\", \"content\": result['recommendation']}})\n",
        "\"\"\"\n",
        "\n",
        "# Write the file\n",
        "with open('streamlit_app.py', 'w') as f:\n",
        "    f.write(STREAMLIT_APP_CONTENT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 6: Streamlit UI Execution\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n################################################################################\")\n",
        "print(\"üöÄ INSTALLING AND STARTING CLOUDFLARE TUNNEL\")\n",
        "print(\"################################################################################\")\n",
        "\n",
        "# 1. Install Cloudflared\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 cloudflared\n",
        "!chmod +x cloudflared\n",
        "\n",
        "# 2. Start Streamlit on port 8501\n",
        "# Note: Streamlit default port is 8501.\n",
        "!nohup streamlit run streamlit_app_temp.py --server.port 8501 --server.enableCORS false > /dev/null 2>&1 &\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"Streamlit service started on port 8501. Establishing Cloudflare tunnel...\")\n",
        "\n",
        "# 3. Start Cloudflare Tunnel\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTt81kI-_2AB",
        "outputId": "0b5ac34a-a4f8-45e4-ecd7-e4df8ba5c70a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "################################################################################\n",
            "üöÄ INSTALLING AND STARTING CLOUDFLARE TUNNEL\n",
            "################################################################################\n",
            "Streamlit service started on port 8501. Establishing Cloudflare tunnel...\n",
            "\u001b[90m2025-11-23T20:11:05Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-23T20:11:05Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m |  https://cage-fin-plays-producing.trycloudflare.com                                        |\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: c4431c52-3351-4792-9f43-068921ea5978\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43\n",
            "2025/11/23 20:11:08 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-23T20:11:08Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mb88717ae-6f7b-4dda-82ba-e7520e50636e \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43 \u001b[36mlocation=\u001b[0mlax01 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-11-23T20:15:52Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    }
  ]
}