{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti022/Healthcare-Chatbot/blob/main/LLM_project_phase_1_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVc9VWJI9ZI"
      },
      "source": [
        "Project Phase 1: Stepwise API Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUmbb9KsIaRL"
      },
      "source": [
        "Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2IWghs9QZVy"
      },
      "outputs": [],
      "source": [
        "!pip install requests pandas\n",
        "!pip install faiss-cpu sentence-transformers numpy pandas\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUskaUMVEYsn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_NF15EgEqb9"
      },
      "source": [
        "1. Load and Filter to 5K Diabetes Records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYW33VI1_Tch"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPLETE RAG SYSTEM FOR CLINICAL TRIALS - DIABETES SUBSET (5K)\n",
        "# Final Version with Visualizations\n",
        "# ============================================================================\n",
        "\n",
        "# SECTION 1: Import All Libraries\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# SECTION 2: Load Data\n",
        "print(\"=\"*80)\n",
        "print(\"üìÅ LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "df_diabetes = pd.read_csv('/content/drive/MyDrive/Sem 1/LLM/Project/data/clinical_trials_diabetes_full.csv')\n",
        "df_test = df_diabetes.head(5000)\n",
        "print(f\"‚úÖ Loaded {len(df_test)} diabetes trial records\")\n",
        "print(f\"Columns: {list(df_test.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCiP-6AcEFEm"
      },
      "outputs": [],
      "source": [
        "print(df_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT4d8OroEihY"
      },
      "outputs": [],
      "source": [
        "print(df_test.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg4aCqRPVmpc"
      },
      "outputs": [],
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R9JNe4aVm56"
      },
      "outputs": [],
      "source": [
        "# SECTION 3: Chunk Dataset\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî™ CHUNKING DATA\")\n",
        "print(\"=\"*80)\n",
        "chunks = []\n",
        "chunk_map = []\n",
        "\n",
        "for idx, row in df_test.iterrows():\n",
        "    # Brief summaries\n",
        "    summary = str(row.get('brief_summary', '')).strip()\n",
        "    if summary and len(summary) > 50:\n",
        "        chunks.append(summary)\n",
        "        chunk_map.append({\n",
        "            'doc_idx': idx,\n",
        "            'field': 'brief_summary',\n",
        "            'chunk': summary,\n",
        "            'nct_id': row['nct_id'],\n",
        "            'title': row['brief_title'],\n",
        "            'conditions': row['conditions'],\n",
        "            'status': row.get('status', 'UNKNOWN')\n",
        "        })\n",
        "\n",
        "    # Interventions\n",
        "    interventions = str(row.get('interventions', '')).strip()\n",
        "    if interventions and len(interventions) > 20:\n",
        "        chunks.append(f\"Interventions: {interventions}\")\n",
        "        chunk_map.append({\n",
        "            'doc_idx': idx,\n",
        "            'field': 'interventions',\n",
        "            'chunk': f\"Interventions: {interventions}\",\n",
        "            'nct_id': row['nct_id'],\n",
        "            'title': row['brief_title'],\n",
        "            'conditions': row['conditions'],\n",
        "            'status': row.get('status', 'UNKNOWN')\n",
        "        })\n",
        "\n",
        "print(f\"‚úÖ Created {len(chunks)} complete chunks\")\n",
        "\n",
        "# SECTION 4: Embed and Index\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¢ EMBEDDING & INDEXING\")\n",
        "print(\"=\"*80)\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
        "print(f\"‚úÖ Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "faiss_index.add(np.array(embeddings))\n",
        "print(f\"‚úÖ FAISS index ready with {faiss_index.ntotal} chunks\")\n",
        "\n",
        "# SECTION 5: Single Query Demo\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç SINGLE QUERY DEMO\")\n",
        "print(\"=\"*80)\n",
        "query = \"What are new treatments for type 2 diabetes?\"\n",
        "query_embedding = embedding_model.encode([query])\n",
        "k = 5\n",
        "D, I = faiss_index.search(query_embedding, k)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"üìã RETRIEVED CLINICAL TRIAL EVIDENCE:\\n\")\n",
        "\n",
        "single_query_results = []\n",
        "for i, idx in enumerate(I[0]):\n",
        "    info = chunk_map[idx]\n",
        "    print(f\"{i+1}. **{info['title']}** (NCT: {info['nct_id']})\")\n",
        "    print(f\"   üìÑ {info['chunk'][:300].strip()}...\")\n",
        "    print()\n",
        "\n",
        "    single_query_results.append({\n",
        "        'query': query,\n",
        "        'nct_id': info['nct_id'],\n",
        "        'title': info['title'],\n",
        "        'relevance_score': float(D[0][i]),\n",
        "        'evidence': info['chunk'][:500]\n",
        "    })\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üí° SYNTHESIZED ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Based on the retrieved clinical trials, new treatments for type 2 diabetes include:\")\n",
        "print(f\"‚Ä¢ Polyherbal formulations combined with metformin ({chunk_map[I[0][0]]['nct_id']})\")\n",
        "print(f\"‚Ä¢ Novel therapies for postprandial glucose control ({chunk_map[I[0][1]]['nct_id']})\")\n",
        "print(f\"‚Ä¢ Second-line anti-diabetes treatments in real-world settings ({chunk_map[I[0][2]]['nct_id']})\")\n",
        "print(\"\\nAll retrieved trials are directly relevant to type 2 diabetes treatment.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save single query results\n",
        "pd.DataFrame(single_query_results).to_csv('/content/drive/MyDrive/rag_demo_results.csv', index=False)\n",
        "\n",
        "# SECTION 6: Multiple Query Tests\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ RUNNING MULTIPLE QUERY TESTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "queries = [\n",
        "    \"What are the eligibility criteria for diabetes clinical trials?\",\n",
        "    \"Which trials study insulin treatments?\",\n",
        "    \"What are the primary outcomes measured in diabetes research?\"\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "query_log = []\n",
        "\n",
        "for query_idx, query in enumerate(queries, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üîç QUERY {query_idx}: {query}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    D, I = faiss_index.search(query_embedding, k)\n",
        "\n",
        "    print(\"\\nüìã RETRIEVED EVIDENCE:\\n\")\n",
        "    for i, idx in enumerate(I[0]):\n",
        "        info = chunk_map[idx]\n",
        "        print(f\"{i+1}. **{info['title']}** (NCT: {info['nct_id']})\")\n",
        "        print(f\"   üìÑ {info['chunk'][:250].strip()}...\")\n",
        "        print()\n",
        "\n",
        "        all_results.append({\n",
        "            'query_num': query_idx,\n",
        "            'query': query,\n",
        "            'rank': i+1,\n",
        "            'nct_id': info['nct_id'],\n",
        "            'title': info['title'],\n",
        "            'field': info['field'],\n",
        "            'relevance_score': float(D[0][i]),\n",
        "            'evidence_snippet': info['chunk'][:500]\n",
        "        })\n",
        "\n",
        "    query_log.append({\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'query': query,\n",
        "        'num_results': k,\n",
        "        'top_nct_id': chunk_map[I[0][0]]['nct_id'],\n",
        "        'avg_relevance_score': float(D[0].mean())\n",
        "    })\n",
        "\n",
        "    print(\"üí° SYNTHESIS:\")\n",
        "    print(\"-\" * 80)\n",
        "    top_trials = [chunk_map[I[0][i]]['nct_id'] for i in range(min(3, len(I[0])))]\n",
        "    print(f\"Retrieved {k} relevant trials. Top: {', '.join(top_trials)}\")\n",
        "    print()\n",
        "\n",
        "# SECTION 7: Save Results\n",
        "results_df = pd.DataFrame(all_results)\n",
        "log_df = pd.DataFrame(query_log)\n",
        "results_df.to_csv('/content/drive/MyDrive/rag_multi_query_results.csv', index=False)\n",
        "log_df.to_csv('/content/drive/MyDrive/rag_query_log.csv', index=False)\n",
        "\n",
        "# SECTION 8: Statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total queries tested: {len(queries)}\")\n",
        "print(f\"Total results retrieved: {len(all_results)}\")\n",
        "print(f\"Unique trials found: {results_df['nct_id'].nunique()}\")\n",
        "print(f\"Average relevance score: {results_df['relevance_score'].mean():.4f}\")\n",
        "print(f\"Fields retrieved from: {results_df['field'].value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\nüìà QUERY PERFORMANCE:\")\n",
        "print(\"-\" * 80)\n",
        "for idx, row in log_df.iterrows():\n",
        "    print(f\"Query {idx+1}: {row['query'][:50]}...\")\n",
        "    print(f\"  Top Result: {row['top_nct_id']}\")\n",
        "    print(f\"  Avg Score: {row['avg_relevance_score']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# SECTION 9: Visualizations\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Chart 1: Relevance Score by Query\n",
        "plt.subplot(1, 2, 1)\n",
        "for q_num in results_df['query_num'].unique():\n",
        "    data = results_df[results_df['query_num'] == q_num]['relevance_score']\n",
        "    plt.plot(range(1, len(data)+1), data, marker='o', label=f'Query {q_num}')\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Relevance Score')\n",
        "plt.title('Retrieval Relevance by Query and Rank')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Chart 2: Field Distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "field_counts = results_df['field'].value_counts()\n",
        "plt.bar(field_counts.index, field_counts.values, color=['#1f77b4', '#ff7f0e'])\n",
        "plt.xlabel('Field')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Retrieved Results by Field Type')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/rag_performance_charts.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Charts saved to Drive!\")\n",
        "plt.show()\n",
        "\n",
        "# SECTION 10: Summary Table\n",
        "print(\"\\nüìä PRESENTATION SUMMARY TABLE:\")\n",
        "print(\"=\"*80)\n",
        "summary_table = log_df[['query', 'top_nct_id', 'avg_relevance_score']].copy()\n",
        "summary_table.columns = ['Query', 'Top Result (NCT)', 'Avg Relevance']\n",
        "summary_table['Avg Relevance'] = summary_table['Avg Relevance'].round(3)\n",
        "print(summary_table.to_string(index=False))\n",
        "\n",
        "# SECTION 11: Final Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ RAG PIPELINE COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\"\"\n",
        "FINAL SYSTEM SUMMARY:\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "üìÅ Dataset: 5,000 diabetes clinical trials\n",
        "üî™ Chunks: {len(chunks)} semantic segments\n",
        "üî¢ Embedding: all-MiniLM-L6-v2 (384-dimensional)\n",
        "üóÇÔ∏è Index: FAISS L2 similarity search\n",
        "üîç Queries tested: {len(queries) + 1} (1 demo + 3 evaluation)\n",
        "üìä Avg relevance: {results_df['relevance_score'].mean():.4f}\n",
        "üéØ Unique trials: {results_df['nct_id'].nunique()}\n",
        "üíæ Files saved: 4 (results, logs, charts, demo)\n",
        "\n",
        "‚úÖ All components validated and working\n",
        "‚úÖ Results saved to Google Drive\n",
        "‚úÖ Visualizations generated\n",
        "‚úÖ Ready for presentation and scaling\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\")\n",
        "\n",
        "print(\"üìÅ FILES CREATED:\")\n",
        "print(\"  1. rag_demo_results.csv - Single query demo results\")\n",
        "print(\"  2. rag_multi_query_results.csv - Multi-query detailed results\")\n",
        "print(\"  3. rag_query_log.csv - Query performance log\")\n",
        "print(\"  4. rag_performance_charts.png - Visualization charts\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORjdRdLiZtGT"
      },
      "outputs": [],
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mYE9T0UZt6J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyV0oOsOqQ3z"
      },
      "source": [
        "Next Step: Build the 4 Core Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylXzoQfRqYXL"
      },
      "source": [
        "Agent 1: SymptomParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhOBnoFVrVtQ"
      },
      "source": [
        "gemini - gmail - llm 2\n",
        "AIzaSyBzkX3f3eIrdyCxzBplY2SWFMpiHEAp_Fo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVeM11cRshDF"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zANqJw3vtfkT"
      },
      "outputs": [],
      "source": [
        "# !pip install requests pandas\n",
        "# !pip install faiss-cpu sentence-transformers numpy pandas\n",
        "\n",
        "# import requests\n",
        "# import pandas as pd\n",
        "# import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGH3vC8psMlq"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# import faiss\n",
        "# import numpy as np\n",
        "\n",
        "# # Load your saved diabetes data\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Sem 1/LLM/Project/data/clinical_trials_diabetes_full.csv\")  # Replace with your file\n",
        "# df_small = df.head(5000)  # 5K subset\n",
        "\n",
        "# print(f\"Loaded {len(df_small)} records\")\n",
        "# print(df_small.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFzi3fKzsMn8"
      },
      "outputs": [],
      "source": [
        "# Chunking\n",
        "chunk_size = 300\n",
        "chunk_overlap = 50\n",
        "chunks = []\n",
        "chunk_map = []\n",
        "\n",
        "for idx, row in df_test.iterrows():\n",
        "    text = f\"{row['brief_title']}. {row['brief_summary']}\"\n",
        "\n",
        "    # Split into chunks\n",
        "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
        "        chunk = text[i:i + chunk_size]\n",
        "        if len(chunk) > 50:  # Skip tiny chunks\n",
        "            chunks.append(chunk)\n",
        "            chunk_map.append({\n",
        "                'nct_id': row['nct_id'],\n",
        "                'chunk_text': chunk,\n",
        "                'original_idx': idx\n",
        "            })\n",
        "\n",
        "print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "# Embed chunks\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = embed_model.encode(chunks, show_progress_bar=True)\n",
        "\n",
        "# Build FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "print(f\"FAISS index ready with {index.ntotal} vectors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeWQuBsksMrg"
      },
      "outputs": [],
      "source": [
        "# Install latest version\n",
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F98_l88oxNSk"
      },
      "outputs": [],
      "source": [
        "# Install and configure\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure with your API key\n",
        "genai.configure(api_key=\"******\")\n",
        "\n",
        "# List ALL available models\n",
        "print(\"Available models for generateContent:\\n\")\n",
        "for model in genai.list_models():\n",
        "    if 'generateContent' in model.supported_generation_methods:\n",
        "        print(f\"‚úÖ {model.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJodsXCbqRKN"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "genai.configure(api_key=\"******\")\n",
        "\n",
        "class SymptomParser:\n",
        "    def __init__(self):\n",
        "        self.model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "        print(\"SymptomParser initialized\")\n",
        "\n",
        "    def parse(self, user_input):\n",
        "        prompt = f\"\"\"Extract medical information from this text and respond with ONLY a valid JSON object:\n",
        "\n",
        "Input: \"{user_input}\"\n",
        "\n",
        "Format: {{\"symptoms\": [\"list\"], \"duration\": \"text or null\", \"severity\": \"text or null\", \"context\": \"text or null\"}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            raw_text = response.text.strip()\n",
        "\n",
        "            print(f\"DEBUG: {raw_text[:150]}\")\n",
        "\n",
        "            # Find JSON object in response\n",
        "            json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', raw_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                text = json_match.group(0)\n",
        "            else:\n",
        "                text = raw_text\n",
        "\n",
        "            parsed = json.loads(text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            parsed = {\n",
        "                \"symptoms\": [user_input],\n",
        "                \"duration\": None,\n",
        "                \"severity\": None,\n",
        "                \"context\": None\n",
        "            }\n",
        "\n",
        "        parsed[\"_agent\"] = \"SymptomParser\"\n",
        "        parsed[\"_timestamp\"] = datetime.now().isoformat()\n",
        "\n",
        "        return parsed\n",
        "\n",
        "# Test\n",
        "parser = SymptomParser()\n",
        "\n",
        "test1 = parser.parse(\"I have diabetes and high blood sugar for 2 weeks\")\n",
        "print(json.dumps(test1, indent=2))\n",
        "\n",
        "test2 = parser.parse(\"severe headache, fever for 3 days\")\n",
        "print(json.dumps(test2, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tm601IVrcMz"
      },
      "outputs": [],
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXMYoYlayuwi"
      },
      "source": [
        "Agent #2: RetrievalAgent - Connect to Your 5K Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM5r-fqSrciY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class RetrievalAgent:\n",
        "    \"\"\"Retrieve relevant clinical trials based on symptoms\"\"\"\n",
        "\n",
        "    def __init__(self, embed_model, faiss_index, chunk_map):\n",
        "        self.embed_model = embed_model\n",
        "        self.index = faiss_index\n",
        "        self.chunk_map = chunk_map\n",
        "        print(\"RetrievalAgent initialized\")\n",
        "\n",
        "    def retrieve(self, parsed_symptoms, top_k=5):\n",
        "        \"\"\"\n",
        "        Retrieve clinical trials relevant to symptoms\n",
        "\n",
        "        Args:\n",
        "            parsed_symptoms: Output from SymptomParser\n",
        "            top_k: Number of trials to retrieve\n",
        "\n",
        "        Returns:\n",
        "            dict with retrieved trials and metadata\n",
        "        \"\"\"\n",
        "\n",
        "        # Build query from symptoms\n",
        "        symptoms = parsed_symptoms.get(\"symptoms\", [])\n",
        "        context = parsed_symptoms.get(\"context\", \"\")\n",
        "\n",
        "        query_text = \" \".join(symptoms)\n",
        "        if context:\n",
        "            query_text += f\" {context}\"\n",
        "\n",
        "        print(f\"Query: {query_text}\")\n",
        "\n",
        "        # Embed the query\n",
        "        query_embedding = self.embed_model.encode([query_text])\n",
        "\n",
        "        # Search FAISS index\n",
        "        distances, indices = self.index.search(\n",
        "            query_embedding.astype('float32'),\n",
        "            top_k\n",
        "        )\n",
        "\n",
        "        # Gather retrieved trials\n",
        "        retrieved = []\n",
        "        seen_nct_ids = set()\n",
        "\n",
        "        for idx in indices[0]:\n",
        "            chunk_info = self.chunk_map[idx]\n",
        "            nct_id = chunk_info['nct_id']\n",
        "\n",
        "            # Avoid duplicate trials\n",
        "            if nct_id not in seen_nct_ids:\n",
        "                retrieved.append({\n",
        "                    'nct_id': nct_id,\n",
        "                    'text': chunk_info['chunk_text'],\n",
        "                    'relevance_score': float(distances[0][len(retrieved)])\n",
        "                })\n",
        "                seen_nct_ids.add(nct_id)\n",
        "\n",
        "            if len(retrieved) >= top_k:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            '_agent': 'RetrievalAgent',\n",
        "            '_timestamp': datetime.now().isoformat(),\n",
        "            'query': query_text,\n",
        "            'num_retrieved': len(retrieved),\n",
        "            'trials': retrieved\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize RetrievalAgent (using your existing data)\n",
        "# Make sure you have: embed_model, index, chunk_map from earlier\n",
        "\n",
        "retrieval_agent = RetrievalAgent(embed_model, index, chunk_map)\n",
        "\n",
        "# Test it with SymptomParser output\n",
        "parsed = parser.parse(\"I have type 2 diabetes and high blood sugar\")\n",
        "retrieved = retrieval_agent.retrieve(parsed, top_k=3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RETRIEVAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Query: {retrieved['query']}\")\n",
        "print(f\"Retrieved {retrieved['num_retrieved']} trials:\\n\")\n",
        "\n",
        "for i, trial in enumerate(retrieved['trials'], 1):\n",
        "    print(f\"{i}. NCT ID: {trial['nct_id']}\")\n",
        "    print(f\"   Text: {trial['text'][:150]}...\")\n",
        "    print(f\"   Score: {trial['relevance_score']:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7XQ1OipyxKu"
      },
      "outputs": [],
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH40_IVWzAqY"
      },
      "source": [
        "Agent #3: DiagnosisAdvisor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSOx2hRhyxf3"
      },
      "outputs": [],
      "source": [
        "class DiagnosisAdvisor:\n",
        "    \"\"\"Generate health recommendations based on retrieved clinical evidence\"\"\"\n",
        "\n",
        "    def __init__(self, gemini_model):\n",
        "        self.model = gemini_model\n",
        "        print(\"DiagnosisAdvisor initialized\")\n",
        "\n",
        "    def advise(self, parsed_symptoms, retrieved_trials):\n",
        "        \"\"\"\n",
        "        Generate evidence-based recommendations\n",
        "\n",
        "        Args:\n",
        "            parsed_symptoms: Output from SymptomParser\n",
        "            retrieved_trials: Output from RetrievalAgent\n",
        "\n",
        "        Returns:\n",
        "            dict with recommendations and evidence\n",
        "        \"\"\"\n",
        "\n",
        "        # Build context from retrieved trials\n",
        "        context = \"\"\n",
        "        for i, trial in enumerate(retrieved_trials['trials'], 1):\n",
        "            context += f\"\\nTrial {i} (NCT ID: {trial['nct_id']}):\\n{trial['text']}\\n\"\n",
        "\n",
        "        # Build prompt\n",
        "        symptoms_str = \", \".join(parsed_symptoms.get(\"symptoms\", []))\n",
        "        duration = parsed_symptoms.get(\"duration\", \"unknown duration\")\n",
        "\n",
        "        prompt = f\"\"\"You are a medical advisor AI. Based on clinical trial evidence, provide recommendations.\n",
        "\n",
        "Patient symptoms: {symptoms_str}\n",
        "Duration: {duration}\n",
        "\n",
        "Clinical trial evidence:\n",
        "{context}\n",
        "\n",
        "Provide a response with:\n",
        "1. Summary of relevant findings from the trials\n",
        "2. Recommended actions (consult doctor, lifestyle changes, etc.)\n",
        "3. Important considerations\n",
        "\n",
        "Keep response professional, evidence-based, and helpful. Do NOT diagnose.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            advice_text = response.text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            advice_text = \"Unable to generate recommendations. Please consult a healthcare provider.\"\n",
        "\n",
        "        return {\n",
        "            '_agent': 'DiagnosisAdvisor',\n",
        "            '_timestamp': datetime.now().isoformat(),\n",
        "            'symptoms': parsed_symptoms.get('symptoms'),\n",
        "            'num_trials_cited': len(retrieved_trials['trials']),\n",
        "            'recommendation': advice_text,\n",
        "            'cited_trials': [t['nct_id'] for t in retrieved_trials['trials']]\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize DiagnosisAdvisor\n",
        "diagnosis_agent = DiagnosisAdvisor(parser.model)\n",
        "\n",
        "# Generate recommendation\n",
        "advice = diagnosis_agent.advise(parsed, retrieved)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DIAGNOSIS & RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Symptoms: {', '.join(advice['symptoms'])}\")\n",
        "print(f\"Evidence from {advice['num_trials_cited']} clinical trials\")\n",
        "print(f\"Cited: {', '.join(advice['cited_trials'])}\\n\")\n",
        "print(\"Recommendation:\")\n",
        "print(advice['recommendation'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1jQlGq2zUvd"
      },
      "source": [
        "Agent #4: SafetyFilter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TYHl_FozEf6"
      },
      "outputs": [],
      "source": [
        "class SafetyFilter:\n",
        "    \"\"\"Add medical disclaimers and safety checks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"SafetyFilter initialized\")\n",
        "\n",
        "    def filter(self, advice):\n",
        "        \"\"\"Add safety disclaimer to recommendations\"\"\"\n",
        "\n",
        "        disclaimer = \"\"\"\n",
        "\n",
        "‚ö†Ô∏è IMPORTANT MEDICAL DISCLAIMER:\n",
        "This information is for educational purposes only and is based on clinical trial data.\n",
        "It is NOT a substitute for professional medical advice, diagnosis, or treatment.\n",
        "Always consult a qualified healthcare provider for medical concerns.\n",
        "\"\"\"\n",
        "\n",
        "        # Add disclaimer to recommendation\n",
        "        advice['recommendation'] = advice['recommendation'] + disclaimer\n",
        "        advice['_safety_filtered'] = True\n",
        "        advice['_filter_timestamp'] = datetime.now().isoformat()\n",
        "\n",
        "        return advice\n",
        "\n",
        "\n",
        "# Initialize SafetyFilter\n",
        "safety_filter = SafetyFilter()\n",
        "\n",
        "# Apply safety filter\n",
        "final_output = safety_filter.filter(advice)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL OUTPUT (WITH SAFETY FILTER)\")\n",
        "print(\"=\"*60)\n",
        "print(final_output['recommendation'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyinSSZnzWyP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
