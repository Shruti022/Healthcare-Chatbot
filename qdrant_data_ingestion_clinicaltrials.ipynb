{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVc9VWJI9ZI"
      },
      "source": [
        "Qdrant\n",
        "\n",
        "> **Step 1 ‚Äì Set up Qdrant Cloud**  \n",
        "> 1. Create a Qdrant Cloud cluster and note the **cluster URL** and **API key** (from the Qdrant Cloud console) .  \n",
        "> 2. Replace `\"provide your url here\"` (or set `QDRANT_URL`) with your cluster URL.  \n",
        "> 3. When prompted in the notebook, paste your API key (input is hidden).  \n",
        ">  \n",
        "> The code below then:  \n",
        "> - Connects to your Qdrant cluster  \n",
        "> - Creates the `clinical_trials` collection (384‚Äëdim cosine) if needed  \n",
        "> - Runs `update_qdrant_auto.py` to read ClinicalTrials.gov CSVs from Drive, clean them, embed with `all‚ÄëMiniLM‚ÄëL6‚Äëv2`, and upsert into Qdrant .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Qdrant client\n",
        "!pip install qdrant-client -q\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "import getpass\n",
        "\n",
        "# üîë Secure API Key Input (invisible)\n",
        "print(\"üîë Enter your Qdrant API Key (input will be hidden):\")\n",
        "qdrant_api_key = getpass.getpass(\"Qdrant API Key: \")\n",
        "\n",
        "# Verify key format\n",
        "if qdrant_api_key and len(qdrant_api_key) > 10:\n",
        "    print(\"‚úÖ API Key captured securely\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è API Key seems invalid\")\n",
        "\n",
        "# Connect to your cluster\n",
        "client = QdrantClient(\n",
        "    url=\"provide your url here\",\n",
        "    api_key=qdrant_api_key\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "client.create_collection(\n",
        "    collection_name=\"clinical_trials\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Collection 'clinical_trials' created successfully!\")\n",
        "\n",
        "# Verify\n",
        "collections = client.get_collections()\n",
        "print(f\"\\nüìä Collections: {collections}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QncCxftPETth"
      },
      "source": [
        "Load Data and Upload to Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdd96FO0kcDd",
        "outputId": "3246ad11-123b-4bf7-a3cb-b30c0639b028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvziMQpXhY_J",
        "outputId": "e4fc3094-63e6-4cf2-93ef-9be73951e838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting update_qdrant_auto.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile update_qdrant_auto.py\n",
        "\"\"\"\n",
        "Automatically finds ALL CSV files in Drive folder and uploads to Qdrant\n",
        "No manual file listing needed!\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "\n",
        "class QdrantAutoPipeline:\n",
        "    def __init__(self, qdrant_url, qdrant_api_key):\n",
        "        self.client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
        "        self.embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.collection_name = \"clinical_trials\"\n",
        "\n",
        "    def find_all_csv_files(self, drive_folder_path):\n",
        "        \"\"\"Automatically find all CSV files in Drive folder\"\"\"\n",
        "        print(f\"üîç Searching for CSV files in: {drive_folder_path}\")\n",
        "\n",
        "        # Find all CSV files\n",
        "        csv_files = glob.glob(f\"{drive_folder_path}/*.csv\")\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"‚ùå No CSV files found!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"‚úÖ Found {len(csv_files)} CSV files:\")\n",
        "        for csv_file in csv_files:\n",
        "            filename = os.path.basename(csv_file)\n",
        "            size_mb = os.path.getsize(csv_file) / (1024 * 1024)\n",
        "            print(f\"   - {filename} ({size_mb:.1f} MB)\")\n",
        "\n",
        "        return csv_files\n",
        "\n",
        "    def load_and_filter_csvs(self, csv_files):\n",
        "        \"\"\"Load all CSV files and filter\"\"\"\n",
        "        print(\"\\nüìÇ Loading CSV files...\")\n",
        "\n",
        "        dfs = []\n",
        "        for csv_path in csv_files:\n",
        "            filename = os.path.basename(csv_path)\n",
        "            print(f\"   Loading {filename}...\")\n",
        "            try:\n",
        "                df = pd.read_csv(csv_path)\n",
        "                dfs.append(df)\n",
        "                print(f\"      ‚úÖ {len(df)} rows\")\n",
        "            except Exception as e:\n",
        "                print(f\"      ‚ö†Ô∏è Error loading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not dfs:\n",
        "            print(\"‚ùå No data loaded!\")\n",
        "            return None\n",
        "\n",
        "        # Concatenate all\n",
        "        df_all = pd.concat(dfs, ignore_index=True)\n",
        "        print(f\"\\n‚úÖ Total trials loaded: {len(df_all):,}\")\n",
        "\n",
        "        # Remove duplicates by NCT ID\n",
        "        initial_count = len(df_all)\n",
        "        df_all = df_all.drop_duplicates(subset=['nct_id'], keep='first')\n",
        "        duplicates_removed = initial_count - len(df_all)\n",
        "        if duplicates_removed > 0:\n",
        "            print(f\"üóëÔ∏è Removed {duplicates_removed:,} duplicate trials\")\n",
        "\n",
        "        # Filter bad statuses\n",
        "        df_all[\"status\"] = df_all[\"status\"].astype(str).str.strip().str.title()\n",
        "        bad_status = [\"Terminated\", \"Withdrawn\", \"Suspended\", \"No Longer Available\", \"Unknown\"]\n",
        "        df_clean = df_all[~df_all[\"status\"].isin(bad_status)].copy()\n",
        "\n",
        "        filtered_out = len(df_all) - len(df_clean)\n",
        "        print(f\"üóëÔ∏è Filtered out {filtered_out:,} trials with bad status\")\n",
        "        print(f\"‚úÖ Final clean dataset: {len(df_clean):,} trials\")\n",
        "\n",
        "        return df_clean\n",
        "\n",
        "    def create_chunks(self, df_clean):\n",
        "        \"\"\"Create text chunks from DataFrame\"\"\"\n",
        "        print(\"\\nüìù Creating chunks...\")\n",
        "\n",
        "        chunks = []\n",
        "        skipped = 0\n",
        "\n",
        "        for idx, row in tqdm(df_clean.iterrows(), total=len(df_clean), desc=\"Processing\"):\n",
        "            title = str(row.get(\"brief_title\", \"\")).strip()\n",
        "            summary = str(row.get(\"brief_summary\", \"\")).strip()\n",
        "\n",
        "            if len(summary) < 20:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            text = f\"Title: {title}\\nSummary: {summary}\"\n",
        "\n",
        "            chunks.append({\n",
        "                \"nct_id\": row[\"nct_id\"],\n",
        "                \"title\": title,\n",
        "                \"text\": text,\n",
        "                \"status\": row[\"status\"]\n",
        "            })\n",
        "\n",
        "        if skipped > 0:\n",
        "            print(f\"‚ö†Ô∏è Skipped {skipped:,} trials with insufficient summary\")\n",
        "        print(f\"‚úÖ Created {len(chunks):,} chunks\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def generate_embeddings(self, chunks):\n",
        "        \"\"\"Generate embeddings for all chunks\"\"\"\n",
        "        print(\"\\nüß† Generating embeddings...\")\n",
        "        print(\"‚è≥ This may take several minutes for large datasets...\")\n",
        "\n",
        "        texts = [c[\"text\"] for c in chunks]\n",
        "        embeddings = self.embed_model.encode(\n",
        "            texts,\n",
        "            batch_size=64,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Generated {len(embeddings):,} embeddings (shape: {embeddings.shape})\")\n",
        "        return embeddings\n",
        "\n",
        "    def upload_to_qdrant(self, embeddings, chunks, mode=\"refresh\"):\n",
        "        \"\"\"Upload data to Qdrant\"\"\"\n",
        "\n",
        "        if mode == \"refresh\":\n",
        "            print(\"\\nüóëÔ∏è Deleting old collection...\")\n",
        "            try:\n",
        "                self.client.delete_collection(self.collection_name)\n",
        "                print(\"‚úÖ Old collection deleted\")\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è No existing collection to delete\")\n",
        "\n",
        "            print(\"üì¶ Creating fresh collection...\")\n",
        "            self.client.create_collection(\n",
        "                collection_name=self.collection_name,\n",
        "                vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
        "            )\n",
        "            print(\"‚úÖ Collection created\")\n",
        "            start_id = 0\n",
        "        else:  # mode == \"add\"\n",
        "            collection_info = self.client.get_collection(self.collection_name)\n",
        "            start_id = collection_info.points_count\n",
        "            print(f\"\\nüìä Adding to existing data, starting from ID: {start_id:,}\")\n",
        "\n",
        "        print(f\"\\n‚è≥ Uploading {len(embeddings):,} vectors to Qdrant...\")\n",
        "\n",
        "        batch_size = 100\n",
        "        total_batches = (len(embeddings) + batch_size - 1) // batch_size\n",
        "\n",
        "        for i in tqdm(range(0, len(embeddings), batch_size), total=total_batches, desc=\"Uploading\"):\n",
        "            batch_end = min(i + batch_size, len(embeddings))\n",
        "\n",
        "            points = []\n",
        "            for idx in range(i, batch_end):\n",
        "                points.append(PointStruct(\n",
        "                    id=start_id + idx,\n",
        "                    vector=embeddings[idx].tolist(),\n",
        "                    payload=chunks[idx]\n",
        "                ))\n",
        "\n",
        "            self.client.upsert(\n",
        "                collection_name=self.collection_name,\n",
        "                points=points\n",
        "            )\n",
        "\n",
        "        # Verify\n",
        "        final_count = self.client.get_collection(self.collection_name).points_count\n",
        "        print(f\"\\n‚úÖ Upload complete!\")\n",
        "        print(f\"üìä Total vectors in Qdrant: {final_count:,}\")\n",
        "\n",
        "    def run_auto_pipeline(self, drive_folder_path, mode=\"refresh\"):\n",
        "        \"\"\"Complete auto pipeline: Auto-find CSVs ‚Üí Qdrant\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"üöÄ QDRANT AUTO-UPDATE PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: Auto-find all CSV files\n",
        "        csv_files = self.find_all_csv_files(drive_folder_path)\n",
        "        if not csv_files:\n",
        "            print(\"‚ùå No CSV files found. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Step 2: Load and filter CSVs\n",
        "        df_clean = self.load_and_filter_csvs(csv_files)\n",
        "        if df_clean is None or len(df_clean) == 0:\n",
        "            print(\"‚ùå No data to process. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Step 3: Create chunks\n",
        "        chunks = self.create_chunks(df_clean)\n",
        "        if not chunks:\n",
        "            print(\"‚ùå No chunks created. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Step 4: Generate embeddings\n",
        "        embeddings = self.generate_embeddings(chunks)\n",
        "\n",
        "        # Step 5: Upload to Qdrant\n",
        "        self.upload_to_qdrant(embeddings, chunks, mode=mode)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚úÖ PIPELINE COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìä Your app now has access to {len(chunks):,} clinical trials\")\n",
        "        print(\"üîÑ No code changes needed - just reload your app!\")\n",
        "\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    import getpass\n",
        "\n",
        "    # Configuration\n",
        "    DRIVE_FOLDER = \"/content/drive/MyDrive/LLM_Based_GenAI_Sem1/data\"\n",
        "    QDRANT_URL = \"https://215ec69e-fa22-4f38-bcf3-941e73901a68.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "\n",
        "    print(\"üîê Qdrant Configuration\")\n",
        "    qdrant_key = getpass.getpass(\"Enter Qdrant API Key: \")\n",
        "\n",
        "    print(\"\\nüìã Update Mode:\")\n",
        "    print(\"1. refresh - Delete all old data and upload fresh\")\n",
        "    print(\"2. add - Keep existing data and add new data\")\n",
        "    mode_choice = input(\"Choose mode (1 or 2): \").strip()\n",
        "    mode = \"refresh\" if mode_choice == \"1\" else \"add\"\n",
        "\n",
        "    # Run pipeline\n",
        "    pipeline = QdrantAutoPipeline(QDRANT_URL, qdrant_key)\n",
        "    pipeline.run_auto_pipeline(DRIVE_FOLDER, mode=mode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv4FcQnIhZDz",
        "outputId": "98a687ca-31f8-448e-d342-202d57817675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-30 16:26:56.042066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764520016.065483   41049 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764520016.075744   41049 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764520016.097559   41049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764520016.097585   41049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764520016.097591   41049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764520016.097604   41049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "üîê Qdrant Configuration\n",
            "Enter Qdrant API Key: \n",
            "\n",
            "üìã Update Mode:\n",
            "1. refresh - Delete all old data and upload fresh\n",
            "2. add - Keep existing data and add new data\n",
            "Choose mode (1 or 2): 1\n",
            "============================================================\n",
            "üöÄ QDRANT AUTO-UPDATE PIPELINE\n",
            "============================================================\n",
            "üîç Searching for CSV files in: /content/drive/MyDrive/LLM_Based_GenAI_Sem1/data\n",
            "‚úÖ Found 6 CSV files:\n",
            "   - clinical_trials_cardiovascular_full.csv (136.0 MB)\n",
            "   - clinical_trials_diabetes_full.csv (49.9 MB)\n",
            "   - clinical_trials_asthma_full.csv (12.4 MB)\n",
            "   - clinical_trials_alzheimer_full.csv (10.1 MB)\n",
            "   - clinical_trials_master_full.csv (564.1 MB)\n",
            "   - clinical_trials_cancer_full.csv (355.6 MB)\n",
            "\n",
            "üìÇ Loading CSV files...\n",
            "   Loading clinical_trials_cardiovascular_full.csv...\n",
            "      ‚úÖ 63192 rows\n",
            "   Loading clinical_trials_diabetes_full.csv...\n",
            "      ‚úÖ 22868 rows\n",
            "   Loading clinical_trials_asthma_full.csv...\n",
            "      ‚úÖ 5038 rows\n",
            "   Loading clinical_trials_alzheimer_full.csv...\n",
            "      ‚úÖ 3761 rows\n",
            "   Loading clinical_trials_master_full.csv...\n",
            "      ‚úÖ 209693 rows\n",
            "   Loading clinical_trials_cancer_full.csv...\n",
            "      ‚úÖ 114834 rows\n",
            "\n",
            "‚úÖ Total trials loaded: 419,386\n",
            "üóëÔ∏è Removed 220,634 duplicate trials\n",
            "üóëÔ∏è Filtered out 52,156 trials with bad status\n",
            "‚úÖ Final clean dataset: 146,596 trials\n",
            "\n",
            "üìù Creating chunks...\n",
            "Processing: 100% 146596/146596 [00:07<00:00, 19539.39it/s]\n",
            "‚ö†Ô∏è Skipped 3 trials with insufficient summary\n",
            "‚úÖ Created 146,593 chunks\n",
            "\n",
            "üß† Generating embeddings...\n",
            "‚è≥ This may take several minutes for large datasets...\n",
            "Batches: 100% 2291/2291 [04:37<00:00,  8.25it/s]\n",
            "‚úÖ Generated 146,593 embeddings (shape: (146593, 384))\n",
            "\n",
            "üóëÔ∏è Deleting old collection...\n",
            "‚úÖ Old collection deleted\n",
            "üì¶ Creating fresh collection...\n",
            "‚úÖ Collection created\n",
            "\n",
            "‚è≥ Uploading 146,593 vectors to Qdrant...\n",
            "Uploading: 100% 1466/1466 [07:12<00:00,  3.39it/s]\n",
            "\n",
            "‚úÖ Upload complete!\n",
            "üìä Total vectors in Qdrant: 146,593\n",
            "\n",
            "============================================================\n",
            "‚úÖ PIPELINE COMPLETE!\n",
            "============================================================\n",
            "üìä Your app now has access to 146,593 clinical trials\n",
            "üîÑ No code changes needed - just reload your app!\n"
          ]
        }
      ],
      "source": [
        "!python update_qdrant_auto.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
